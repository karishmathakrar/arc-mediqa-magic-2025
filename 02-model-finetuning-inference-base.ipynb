{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd0225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing TRANSFORMERS_CACHE: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/hf_cache\n",
      "HF_HOME: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/.hf_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"TRANSFORMERS_CACHE\" in os.environ:\n",
    "    print(f\"Removing existing TRANSFORMERS_CACHE: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "    del os.environ[\"TRANSFORMERS_CACHE\"]\n",
    "\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "print(f\"HF_HOME: {os.getenv('HF_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da607113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import traceback\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, MllamaForConditionalGeneration, AutoModelForVision2Seq, Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12fab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.10 (main, Apr 15 2024, 11:52:16) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3271e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /storage/scratch1/2/kthakrar3/mediqa-magic-v2\n",
      "Selected model: gemma-3-12b-it\n",
      "Model ID: google/gemma-3-12b-it\n",
      "Is Llama model: False\n",
      "Is Qwen model: False\n",
      "Model output directory: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/finetuned-model/gemma-3-12b-it_20250501_1303\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama-3.2-11b-vision\": \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    \"gemma-3-4b-it\": \"google/gemma-3-4b-it\",\n",
    "    \"gemma-3-12b-it\": \"google/gemma-3-12b-it\",\n",
    "    \"Qwen2-VL-2B-Instruct\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"Qwen2-VL-7B-Instruct\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"Qwen2.5-VL-3B-Instruct\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"Qwen2.5-VL-7B-Instruct\": \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "}\n",
    "\n",
    "SELECTED_MODEL = \"gemma-3-12b-it\"\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"2025_dataset\")\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"valid\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "TRAIN_IMAGES_DIR = os.path.join(TRAIN_DIR, \"images_train\")\n",
    "VAL_IMAGES_DIR = os.path.join(VAL_DIR, \"images_valid\")\n",
    "TEST_IMAGES_DIR = os.path.join(TEST_DIR, \"images_test\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "PROCESSED_TRAIN_DATA_DIR = os.path.join(OUTPUT_DIR, f\"processed_train_data-{SELECTED_MODEL}-V3\")\n",
    "PROCESSED_VAL_DATA_DIR = os.path.join(OUTPUT_DIR, f\"processed_val_data-{SELECTED_MODEL}-V3\")\n",
    "PROCESSED_COMBINED_DATA_DIR = os.path.join(OUTPUT_DIR, f\"processed_combined_data-{SELECTED_MODEL}-V3\")\n",
    "PROCESSED_TEST_DATA_DIR = os.path.join(OUTPUT_DIR, f\"processed_test_data-{SELECTED_MODEL}-V3\")\n",
    "\n",
    "QUESTIONS_PATH = os.path.join(TRAIN_DIR, \"closedquestions_definitions_imageclef2025.json\")\n",
    "\n",
    "TRAIN_JSON_PATH = os.path.join(TRAIN_DIR, \"train.json\")\n",
    "VAL_JSON_PATH = os.path.join(VAL_DIR, \"valid.json\")\n",
    "TEST_JSON_PATH = os.path.join(TEST_DIR, \"test.json\")\n",
    "\n",
    "TRAIN_CVQA_PATH = os.path.join(TRAIN_DIR, \"train_cvqa.json\")\n",
    "VAL_CVQA_PATH = os.path.join(VAL_DIR, \"valid_cvqa.json\")\n",
    "\n",
    "MODEL_ID = AVAILABLE_MODELS[SELECTED_MODEL]\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "IS_LLAMA = \"llama\" in MODEL_ID.lower()\n",
    "IS_QWEN = \"qwen\" in MODEL_ID.lower()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "MODEL_SAVE_DIRECTORY = os.path.join(OUTPUT_DIR, \"finetuned-model\", f\"{MODEL_NAME}_{TIMESTAMP}\")\n",
    "os.makedirs(MODEL_SAVE_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Selected model: {SELECTED_MODEL}\")\n",
    "print(f\"Model ID: {MODEL_ID}\")\n",
    "print(f\"Is Llama model: {IS_LLAMA}\")\n",
    "print(f\"Is Qwen model: {IS_QWEN}\")\n",
    "print(f\"Model output directory: {MODEL_SAVE_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051274f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d47a0fd26040c8bab5d91ddc73f5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default chat template: {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "Special tokens map: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n"
     ]
    }
   ],
   "source": [
    "def get_model_config(torch_dtype=None):\n",
    "    \"\"\"Create standardized model configuration dictionary\"\"\"\n",
    "    if torch_dtype is None:\n",
    "        torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        \n",
    "    model_kwargs = dict(\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "        bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"]\n",
    "    )\n",
    "    \n",
    "    return model_kwargs\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8:\n",
    "    print(\"WARNING: GPU may not fully support bfloat16. Consider using float16 instead.\")\n",
    "\n",
    "model_kwargs = get_model_config(torch_dtype=torch.bfloat16)\n",
    "\n",
    "if IS_LLAMA:\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "    if hasattr(model, \"tie_weights\"):\n",
    "        model.tie_weights()\n",
    "elif IS_QWEN:\n",
    "    if \"2.5\" in MODEL_ID:\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "    else:\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "else:\n",
    "    non_llama_kwargs = model_kwargs.copy()\n",
    "    non_llama_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "    model = AutoModelForImageTextToText.from_pretrained(MODEL_ID, **non_llama_kwargs)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "print(f\"Default chat template: {processor.tokenizer.chat_template}\")\n",
    "print(f\"Special tokens map: {processor.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b447d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_model(checkpoint_path=None, token=None, output_dir=None):\n",
    "    \"\"\"Merge LoRA weights into base model and save to output directory\"\"\"\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_pattern = os.path.join(OUTPUT_DIR, \"finetuned-model\", f\"{MODEL_NAME}_*\", \"checkpoint-*\")\n",
    "        checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "        \n",
    "        if not checkpoint_dirs:\n",
    "            raise FileNotFoundError(f\"No checkpoints found matching pattern {checkpoint_pattern}\")\n",
    "            \n",
    "        checkpoint_dirs = sorted(checkpoint_dirs,\n",
    "                                key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)),\n",
    "                                reverse=True)\n",
    "        checkpoint_path = checkpoint_dirs[0]\n",
    "        print(f\"Using latest checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "    if output_dir is None:\n",
    "        checkpoint_name = os.path.basename(checkpoint_path)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        output_dir = os.path.join(OUTPUT_DIR, \"merged\", f\"{MODEL_NAME}_{checkpoint_name}_{timestamp}\")\n",
    "        \n",
    "    model_kwargs = get_model_config(torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    if IS_LLAMA:\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            token=token\n",
    "        )\n",
    "    elif IS_QWEN:\n",
    "        if \"2.5\" in MODEL_ID:\n",
    "            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                low_cpu_mem_usage=True,\n",
    "                **model_kwargs,\n",
    "                token=token\n",
    "            )\n",
    "        else:\n",
    "            model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                low_cpu_mem_usage=True,\n",
    "                **model_kwargs,\n",
    "                token=token\n",
    "            )\n",
    "    else:\n",
    "        \n",
    "        non_llama_kwargs = model_kwargs.copy()\n",
    "        non_llama_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **non_llama_kwargs,\n",
    "            token=token\n",
    "        )\n",
    "        \n",
    "    print(f\"Applying adapter from {checkpoint_path}...\")\n",
    "    peft_model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "    print(\"Merging weights...\")\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving merged model to {output_dir}...\")\n",
    "    merged_model.save_pretrained(output_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, token=token)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    del model\n",
    "    del peft_model\n",
    "    del merged_model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Merged model saved to {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f0ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BASE MODEL ONLY at google/gemma-3-12b-it (no fine-tuning applied)\n",
      "\n",
      "Model for inference: BASE MODEL (NO FINE-TUNING)\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, use_finetuning=False):\n",
    "        \"\"\"\n",
    "        Initialize inference arguments with options for finetuning.\n",
    "        \n",
    "        Parameters:\n",
    "        - use_finetuning: Whether to use the fine-tuned model (True) or base model (False)\n",
    "        \"\"\"\n",
    "        self.test = False\n",
    "        self.skip_data_prep = False\n",
    "        self.batch_size = 100\n",
    "        self.max_samples = None\n",
    "        self.use_finetuning = use_finetuning\n",
    "        self.use_combined_dataset = False\n",
    "        self.min_data_size = 10\n",
    "        \n",
    "        if self.use_finetuning:\n",
    "            checkpoint_pattern = os.path.join(OUTPUT_DIR, \"finetuned-model\", f\"{MODEL_NAME}_*\", \"checkpoint-*\")\n",
    "            checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "            \n",
    "            if not checkpoint_dirs:\n",
    "                raise FileNotFoundError(f\"No checkpoints found for model {MODEL_NAME}\")\n",
    "            \n",
    "            checkpoint_dirs = sorted(checkpoint_dirs, \n",
    "                                    key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)), \n",
    "                                    reverse=True)\n",
    "            \n",
    "            checkpoint_path = checkpoint_dirs[0]\n",
    "            print(f\"Creating merged model from latest checkpoint: {checkpoint_path}\")\n",
    "            \n",
    "            checkpoint_name = os.path.basename(checkpoint_path)\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "            merged_dir = os.path.join(OUTPUT_DIR, \"merged\", f\"{MODEL_NAME}_{checkpoint_name}_{timestamp}\")\n",
    "            \n",
    "            self.model_path = merge_lora_model(\n",
    "                checkpoint_path=checkpoint_path,\n",
    "                token=HF_TOKEN,\n",
    "                output_dir=merged_dir\n",
    "            )\n",
    "            print(f\"Using merged model at {self.model_path}\")\n",
    "            \n",
    "            self.adapter_path = None\n",
    "        else:\n",
    "            self.model_path = MODEL_ID\n",
    "            self.adapter_path = None\n",
    "            print(f\"Using BASE MODEL ONLY at {self.model_path} (no fine-tuning applied)\")\n",
    "        \n",
    "        print(\"\\nModel for inference:\", \"FINE-TUNED\" if self.use_finetuning else \"BASE MODEL (NO FINE-TUNING)\")\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d8794b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_options(options_str):\n",
    "    \"\"\"\n",
    "    Safely convert a string representation of a list to an actual list.\n",
    "    \"\"\"\n",
    "    if not isinstance(options_str, str):\n",
    "        return options_str\n",
    "        \n",
    "    try:\n",
    "        return ast.literal_eval(options_str)\n",
    "    except (SyntaxError, ValueError):\n",
    "        if options_str.startswith('[') and options_str.endswith(']'):\n",
    "            return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "        elif ',' in options_str:\n",
    "            return [opt.strip() for opt in options_str.split(',')]\n",
    "        else:\n",
    "            return [options_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e076bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Create a dataset for either training or validation data.\n",
    "    \n",
    "    Args:\n",
    "        mode: Either \"train\" or \"val\" to specify which dataset to prepare\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing the processed dataset\n",
    "    \"\"\"\n",
    "    print(f\"Preparing {mode} dataset...\")\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        json_path = TRAIN_JSON_PATH\n",
    "        cvqa_path = TRAIN_CVQA_PATH\n",
    "        images_dir = TRAIN_IMAGES_DIR\n",
    "        output_filename = \"train_dataset_processed.csv\"\n",
    "    elif mode == \"val\":\n",
    "        json_path = VAL_JSON_PATH\n",
    "        cvqa_path = VAL_CVQA_PATH\n",
    "        images_dir = VAL_IMAGES_DIR\n",
    "        output_filename = \"val_dataset.csv\"\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be either 'train' or 'val'\")\n",
    "    \n",
    "    with open(QUESTIONS_PATH, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "        \n",
    "    questions_df = pd.json_normalize(questions)[[\"qid\", \"question_en\", \"options_en\", \"question_type_en\", \"question_category_en\"]]\n",
    "    \n",
    "    input_df = pd.read_json(json_path)\n",
    "    \n",
    "    query_info_df = input_df[[\"encounter_id\", \"image_ids\", \"query_title_en\", \"query_content_en\", \"author_id\"]]\n",
    "    \n",
    "    with open(cvqa_path, 'r') as f:\n",
    "        cvqa_data = json.load(f)\n",
    "    cvqa_df = pd.json_normalize(cvqa_data)\n",
    "    \n",
    "    cvqa_long = cvqa_df.melt(id_vars=[\"encounter_id\"], \n",
    "                             var_name=\"qid\", \n",
    "                             value_name=\"answer_index\")\n",
    "    \n",
    "    cvqa_long = cvqa_long[cvqa_long[\"qid\"] != \"encounter_id\"]\n",
    "    \n",
    "    cvqa_merged = cvqa_long.merge(questions_df, on=\"qid\", how=\"left\")\n",
    "    \n",
    "    def get_answer_text(row):\n",
    "        try:\n",
    "            return row[\"options_en\"][row[\"answer_index\"]]\n",
    "        except (IndexError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    cvqa_merged[\"answer_text\"] = cvqa_merged.apply(get_answer_text, axis=1)\n",
    "    \n",
    "    final_df = cvqa_merged.merge(query_info_df, on=\"encounter_id\", how=\"left\")\n",
    "    \n",
    "    final_df['base_qid'] = final_df['qid'].str.extract(r'(CQID\\d+)')\n",
    "    \n",
    "    grouped_by_family = final_df.groupby(['encounter_id', 'base_qid']).agg({\n",
    "        'qid': list,\n",
    "        'question_en': list,\n",
    "        'answer_text': list,\n",
    "        'answer_index': list,\n",
    "        'image_ids': 'first',\n",
    "        'options_en': 'first',\n",
    "        'question_type_en': 'first',\n",
    "        'question_category_en': 'first',\n",
    "        'query_title_en': 'first',\n",
    "        'query_content_en': 'first',\n",
    "        'author_id': 'first'\n",
    "    })\n",
    "    \n",
    "    grouped_by_family = grouped_by_family.reset_index()\n",
    "    \n",
    "    def get_valid_answers(row):\n",
    "        \"\"\"\n",
    "        Extract all valid answers, with special handling for \"Not mentioned\".\n",
    "        If \"Not mentioned\" is the only answer for all slots, we keep it.\n",
    "        Otherwise, we collect all non-\"Not mentioned\" answers.\n",
    "        \"\"\"\n",
    "        answers = row['answer_text']\n",
    "        answer_indices = row['answer_index']\n",
    "\n",
    "        if all(ans == \"Not mentioned\" for ans in answers):\n",
    "            return [\"Not mentioned\"], [answer_indices[0]]\n",
    "\n",
    "        valid_answers = []\n",
    "        valid_indices = []\n",
    "\n",
    "        for i, ans in enumerate(answers):\n",
    "            if ans != \"Not mentioned\":\n",
    "                if isinstance(ans, str):\n",
    "                    cleaned_ans = ans.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                    if cleaned_ans not in valid_answers:\n",
    "                        valid_answers.append(cleaned_ans)\n",
    "                        valid_indices.append(answer_indices[i])\n",
    "                else:\n",
    "                    str_ans = str(ans).strip(\"'\\\" \")\n",
    "                    if str_ans not in valid_answers:\n",
    "                        valid_answers.append(str_ans)\n",
    "                        valid_indices.append(answer_indices[i])\n",
    "\n",
    "        return valid_answers, valid_indices\n",
    "    \n",
    "    grouped_by_family[['valid_answers', 'valid_indices']] = grouped_by_family.apply(\n",
    "        lambda row: pd.Series(get_valid_answers(row)), axis=1)\n",
    "    \n",
    "    dataset_rows = []\n",
    "    \n",
    "    for _, row in tqdm(grouped_by_family.iterrows(), desc=f\"Creating {mode} dataset\"):\n",
    "        encounter_id = row['encounter_id']\n",
    "        base_qid = row['base_qid']\n",
    "        valid_answers = row['valid_answers']\n",
    "        valid_indices = row['valid_indices']\n",
    "        image_ids = row['image_ids']\n",
    "        question_text = row['question_en'][0]\n",
    "        query_title = row['query_title_en']\n",
    "        query_content = row['query_content_en']\n",
    "        author_id = row['author_id']\n",
    "        options_en = row['options_en']\n",
    "        question_type_en = row['question_type_en']\n",
    "        question_category_en = row['question_category_en']\n",
    "        \n",
    "        for img_id in image_ids:\n",
    "            img_path = os.path.join(images_dir, img_id)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image {img_id} not found at {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            dataset_rows.append({\n",
    "                'encounter_id': encounter_id,\n",
    "                'base_qid': base_qid,\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path,\n",
    "                'valid_answers': valid_answers,\n",
    "                'valid_indices': valid_indices,\n",
    "                'question_text': question_text,\n",
    "                'query_title_en': query_title,\n",
    "                'query_content_en': query_content,\n",
    "                'author_id': author_id,\n",
    "                'options_en': options_en,\n",
    "                'question_type_en': question_type_en, \n",
    "                'question_category_en': question_category_en,\n",
    "                'is_multi_label': len(valid_answers) > 1\n",
    "            })\n",
    "    \n",
    "    dataset = pd.DataFrame(dataset_rows)\n",
    "    \n",
    "    dataset.to_csv(os.path.join(OUTPUT_DIR, output_filename), index=False)\n",
    "    \n",
    "    print(f\"{mode.capitalize()} dataset created with {len(dataset)} entries\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897060c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_process_datasets(args):\n",
    "    \"\"\"\n",
    "    Prepare and process datasets based on arguments.\n",
    "    \n",
    "    Args:\n",
    "        args: Arguments containing test, skip_data_prep, and other parameters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing (train_df, val_df)\n",
    "    \"\"\"\n",
    "    train_df = None\n",
    "    val_df = None\n",
    "    \n",
    "    if args.skip_data_prep:\n",
    "        print(\"Skipping data preparation...\")\n",
    "        \n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, \"train_dataset_processed.csv\")):\n",
    "            train_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"train_dataset_processed.csv\"))\n",
    "            print(f\"Loaded existing training dataset with {len(train_df)} samples\")\n",
    "        \n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, \"val_dataset.csv\")):\n",
    "            val_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"val_dataset.csv\"))\n",
    "            print(f\"Loaded existing validation dataset with {len(val_df)} samples\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if args.use_combined_dataset:\n",
    "            print(\"Creating combined train+val dataset...\")\n",
    "\n",
    "            print(\"Preparing training dataset...\")\n",
    "            train_df = prepare_dataset(mode=\"train\")\n",
    "            \n",
    "            print(\"Preparing validation dataset...\")\n",
    "            val_df = prepare_dataset(mode=\"val\")\n",
    "        \n",
    "            train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "            val_df = None\n",
    "            \n",
    "            combined_file = os.path.join(OUTPUT_DIR, \"combined_train_val_dataset.csv\")\n",
    "            train_df.to_csv(combined_file, index=False)\n",
    "            print(f\"Combined dataset saved to {combined_file} with {len(train_df)} samples\")\n",
    "                        \n",
    "        else: \n",
    "            \n",
    "            print(\"Preparing training dataset...\")\n",
    "            train_df = prepare_dataset(mode=\"train\")\n",
    "            \n",
    "            print(\"Preparing validation dataset...\")\n",
    "            val_df = prepare_dataset(mode=\"val\")\n",
    "            \n",
    "        \n",
    "        if args.test:\n",
    "            print(\"Running in test mode with a small subset of data...\")\n",
    "            \n",
    "            if train_df is not None:\n",
    "                test_size = min(args.min_data_size, len(train_df))\n",
    "                train_df = train_df.head(test_size)\n",
    "                print(f\"Using {len(train_df)} training samples for testing\")\n",
    "            \n",
    "            if val_df is not None:\n",
    "                test_size = min(args.min_data_size, len(val_df))\n",
    "                val_df = val_df.head(test_size)\n",
    "                print(f\"Using {len(val_df)} validation samples for testing\")\n",
    "        \n",
    "        if args.use_combined_dataset:\n",
    "            if os.path.exists(PROCESSED_COMBINED_DATA_DIR):\n",
    "                shutil.rmtree(PROCESSED_COMBINED_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_COMBINED_DATA_DIR, exist_ok=True)\n",
    "        else:\n",
    "            if os.path.exists(PROCESSED_TRAIN_DATA_DIR):\n",
    "                shutil.rmtree(PROCESSED_TRAIN_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_TRAIN_DATA_DIR, exist_ok=True)\n",
    "        \n",
    "            if os.path.exists(PROCESSED_VAL_DATA_DIR):\n",
    "                shutil.rmtree(PROCESSED_VAL_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_VAL_DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = prepare_and_process_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62502b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9725b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_batch(batch_df, batch_idx, save_dir, images_dir, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples and save them as a pickle file.\n",
    "    Works for both training and validation/inference datasets.\n",
    "    \n",
    "    Args:\n",
    "        batch_df: DataFrame containing the batch to process\n",
    "        batch_idx: Index of the batch (for naming the output file)\n",
    "        save_dir: Directory to save the processed batch\n",
    "        images_dir: Directory containing the images\n",
    "        mode: Either \"train\" or \"val\"/\"inference\" to determine processing\n",
    "    \n",
    "    Returns:\n",
    "        Number of successfully processed examples\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    file_prefix = \"batch_\" if mode == \"train\" else \"val_batch_\"\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "                \n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                options = cleaned_options\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                \n",
    "            options_text = \", \".join(options)\n",
    "            \n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            if \"Please specify which affected area for each selection\" in question:\n",
    "                question = question.replace(\" Please specify which affected area for each selection.\", \"\")\n",
    "            \n",
    "            question = re.sub(r'^\\d+\\s+', '', question)\n",
    "            \n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "            \n",
    "            clinical_context = \"\"\n",
    "            if query_title or query_content:\n",
    "                clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "                if query_title:\n",
    "                    clinical_context += f\"{query_title}\\n\"\n",
    "                if query_content:\n",
    "                    clinical_context += f\"{query_content}\\n\"\n",
    "\n",
    "            query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Available Options (choose from these): {options_text}\")\n",
    "            \n",
    "            data_item = {\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            }\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                if 'valid_answers' in row and row['valid_answers']:\n",
    "                    answers = row['valid_answers']\n",
    "                    if isinstance(answers, list):\n",
    "                        cleaned_answers = []\n",
    "                        for ans in answers:\n",
    "                            if isinstance(ans, str):\n",
    "                                cleaned_ans = ans.strip(\"'\\\" \")\n",
    "                                cleaned_ans = cleaned_ans.replace(\" (please specify)\", \"\")\n",
    "                                cleaned_answers.append(cleaned_ans)\n",
    "                            else:\n",
    "                                cleaned_answers.append(str(ans).strip(\"'\\\" \"))\n",
    "                        \n",
    "                        if len(cleaned_answers) > 1:\n",
    "                            answer_text = \", \".join(cleaned_answers)\n",
    "                        elif len(cleaned_answers) == 1:\n",
    "                            answer_text = cleaned_answers[0]\n",
    "                        else:\n",
    "                            answer_text = \"Not mentioned\"\n",
    "                    else:\n",
    "                        if isinstance(answers, str):\n",
    "                            answer_text = answers.strip(\"'\\\" \")\n",
    "                            answer_text = answer_text.replace(\" (please specify)\", \"\")\n",
    "                        else:\n",
    "                            answer_text = str(answers).strip(\"'\\\" \")\n",
    "                            \n",
    "                    if isinstance(answer_text, str) and answer_text.startswith(\"[\") and answer_text.endswith(\"]\"):\n",
    "                        clean_text = answer_text.strip(\"[]'\")\n",
    "                        parts = [part.strip() for part in clean_text.split(\"', '\")]\n",
    "                        answer_text = \", \".join(parts)\n",
    "                \n",
    "                elif 'multi_label' in row:\n",
    "                    answer_text = row['multi_label']\n",
    "                else:\n",
    "                    answer_text = \"Not mentioned\"\n",
    "                \n",
    "                data_item[\"answer_text\"] = answer_text\n",
    "            \n",
    "            batch_data.append(data_item)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"{file_prefix}{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=None, images_dir=None, mode=\"train\", args=None):\n",
    "    \"\"\"\n",
    "    Process an entire dataset in batches.\n",
    "    Works for both training and validation/inference datasets.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the dataset\n",
    "        batch_size: Number of examples per batch\n",
    "        save_dir: Directory to save processed batches\n",
    "        images_dir: Directory containing images\n",
    "        mode: Either \"train\" or \"val\"/\"inference\"\n",
    "        args: Arguments containing use_combined_dataset flag\n",
    "        \n",
    "    Returns:\n",
    "        Total number of processed examples\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    if save_dir is None:\n",
    "        if mode == \"train\" and args is not None and args.use_combined_dataset:\n",
    "            save_dir = PROCESSED_COMBINED_DATA_DIR\n",
    "        elif mode == \"train\":\n",
    "            save_dir = PROCESSED_TRAIN_DATA_DIR\n",
    "        else:\n",
    "            save_dir = PROCESSED_VAL_DATA_DIR\n",
    "    \n",
    "    if images_dir is None:\n",
    "        images_dir = TRAIN_IMAGES_DIR if mode == \"train\" else VAL_IMAGES_DIR\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_dataset_batch(batch_df, batch_idx, save_dir, images_dir, mode=mode)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72862081",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.skip_data_prep:\n",
    "    total_examples = preprocess_dataset(train_df, batch_size=100, mode=\"train\", args=args)\n",
    "    print(f\"Total processed examples: {total_examples}\")\n",
    "else:\n",
    "    print(\"Skipping data preparation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the processed data\n",
    "def check_processed_sample(args):\n",
    "    \"\"\"\n",
    "    Check a sample of processed data and display it\n",
    "    \"\"\"\n",
    "    if args.use_combined_dataset:\n",
    "        check_dir = PROCESSED_COMBINED_DATA_DIR\n",
    "    else:\n",
    "        check_dir = PROCESSED_TRAIN_DATA_DIR\n",
    "    \n",
    "    batch_file = os.path.join(check_dir, \"batch_0.pkl\")\n",
    "    \n",
    "    try:\n",
    "        with open(batch_file, 'rb') as f:\n",
    "            batch_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"\\nSample of processed data from {check_dir}:\")\n",
    "        \n",
    "        print(\"\\nFirst example:\")\n",
    "        sample_data = batch_data[0]\n",
    "        for key, value in sample_data.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        print(\"\\nSecond example:\")\n",
    "        sample_data = batch_data[1]\n",
    "        for key, value in sample_data.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking processed data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9268c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_processed_sample(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_llm_training_inputs(processed_dir=None, args=None, qids=[\"CQID034\", \"CQID012\", \"CQID011\"], \n",
    "                               multi_label_examples=True, single_label_examples=True, \n",
    "                               num_samples_per_qid=1, show_images=True):\n",
    "    \"\"\"\n",
    "    Shows the exact inputs that would be sent to the LLM during training.\n",
    "    \n",
    "    Args:\n",
    "        processed_dir: Directory containing processed batch files\n",
    "        args: Arguments containing use_combined_dataset flag\n",
    "        qids: List of QIDs to look for\n",
    "        multi_label_examples: Whether to include multi-label examples\n",
    "        single_label_examples: Whether to include single-label examples\n",
    "        num_samples_per_qid: How many samples to show for each QID\n",
    "        show_images: Whether to display the images\n",
    "    \"\"\"\n",
    "    if processed_dir is None:\n",
    "        if args is not None and args.use_combined_dataset:\n",
    "            processed_dir = PROCESSED_COMBINED_DATA_DIR\n",
    "        else:\n",
    "            processed_dir = PROCESSED_TRAIN_DATA_DIR\n",
    "    \n",
    "    print(f\"Inspecting LLM training inputs from: {processed_dir}\")\n",
    "    \n",
    "    batch_files = sorted([f for f in os.listdir(processed_dir) \n",
    "                          if f.startswith(\"batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    examples_by_qid = defaultdict(lambda: {\"multi\": [], \"single\": []})\n",
    "    \n",
    "    for batch_file in batch_files:\n",
    "        file_path = os.path.join(processed_dir, batch_file)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            for sample in batch_data:\n",
    "                sample_qid = sample['qid']\n",
    "                if any(sample_qid.startswith(qid) for qid in qids):\n",
    "                    is_multi_label = ',' in sample['answer_text']\n",
    "                    \n",
    "                    if is_multi_label and multi_label_examples:\n",
    "                        examples_by_qid[sample_qid][\"multi\"].append(sample)\n",
    "                    elif not is_multi_label and single_label_examples:\n",
    "                        examples_by_qid[sample_qid][\"single\"].append(sample)\n",
    "            \n",
    "            all_found = True\n",
    "            for qid in qids:\n",
    "                qid_examples = [key for key in examples_by_qid.keys() if key.startswith(qid)]\n",
    "                if not qid_examples:\n",
    "                    all_found = False\n",
    "                    break\n",
    "                    \n",
    "                for q in qid_examples:\n",
    "                    if multi_label_examples and len(examples_by_qid[q][\"multi\"]) < num_samples_per_qid:\n",
    "                        if not examples_by_qid[q][\"multi\"]:\n",
    "                            all_found = False\n",
    "                    if single_label_examples and len(examples_by_qid[q][\"single\"]) < num_samples_per_qid:\n",
    "                        if not examples_by_qid[q][\"single\"]:\n",
    "                            all_found = False\n",
    "            \n",
    "            if all_found:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {batch_file}: {e}\")\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    for qid in qids:\n",
    "        matching_qids = [key for key in examples_by_qid.keys() if key.startswith(qid)]\n",
    "        \n",
    "        if not matching_qids:\n",
    "            print(f\"\\nNo examples found for QID {qid}\")\n",
    "            continue\n",
    "            \n",
    "        for matching_qid in matching_qids:\n",
    "            if multi_label_examples and examples_by_qid[matching_qid][\"multi\"]:\n",
    "                examples = examples_by_qid[matching_qid][\"multi\"][:num_samples_per_qid]\n",
    "                for i, sample in enumerate(examples):\n",
    "                    sample_count += 1\n",
    "                    display_llm_training_input(sample, sample_count, show_images)\n",
    "            \n",
    "            if single_label_examples and examples_by_qid[matching_qid][\"single\"]:\n",
    "                examples = examples_by_qid[matching_qid][\"single\"][:num_samples_per_qid]\n",
    "                for i, sample in enumerate(examples):\n",
    "                    sample_count += 1\n",
    "                    display_llm_training_input(sample, sample_count, show_images)\n",
    "    \n",
    "    if sample_count == 0:\n",
    "        print(\"No matching examples found. Try different QIDs or check your data directory.\")\n",
    "\n",
    "def display_llm_training_input(sample, sample_num, show_images=True):\n",
    "    \"\"\"\n",
    "    Display a single example formatted exactly as it would be sent to the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"EXAMPLE #{sample_num}: {sample['qid']}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"Question ID: {sample['qid']}\")\n",
    "    print(f\"Type: {sample['question_type']}\")\n",
    "    print(f\"Category: {sample['question_category']}\")\n",
    "    print(f\"Image path: {sample['image_path']}\")\n",
    "    \n",
    "    is_multi_label = ',' in sample['answer_text']\n",
    "    print(f\"Multi-label: {is_multi_label}\")\n",
    "    \n",
    "    if show_images and os.path.exists(sample['image_path']):\n",
    "        try:\n",
    "            img = Image.open(sample['image_path'])\n",
    "            width, height = img.size\n",
    "            print(f\"Image dimensions: {width}x{height}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image for {os.path.basename(sample['image_path'])}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image: {e}\")\n",
    "    \n",
    "    system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "    IMPORTANT: \n",
    "    - Respond ONLY with the exact text of the option(s) that apply\n",
    "    - Do not provide any explanations\n",
    "    - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "    - Do not write \"Options:\" or similar prefixes\n",
    "    - Do not write \"Answer:\" or similar prefixes\n",
    "    - Multiple answers should be separated by commas\n",
    "    - If unsure, respond with \"Not mentioned\"\n",
    "    - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "    - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nACTUAL LLM INPUT (as formatted in your MedicalImageDataset):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\"System message:\")\n",
    "    print(system_message)\n",
    "    print(\"\\nUser message:\")\n",
    "    print(sample['query_text'])\n",
    "    print(\"[IMAGE WOULD BE INCLUDED HERE]\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    print(\"\\nEXPECTED LLM OUTPUT:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(sample['answer_text'])\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    if is_multi_label:\n",
    "        answers = [ans.strip() for ans in sample['answer_text'].split(\",\")]\n",
    "#         print(f\"\\nSplit answers:\")\n",
    "#         for ans in answers:\n",
    "#             print(f\"- {ans}\")\n",
    "    \n",
    "    print(\"\\nCHAT TEMPLATE FORMAT (approximate):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\"<bos><begin_of_system>\\nYou are a medical image analysis assistant. Your task is to examine the provided clinical images along with clinical context, and select the option(s) that best describe what you see. \\nIMPORTANT: You must respond ONLY with the exact text of the option(s) that apply. \\n- Do not provide any explanations\\n- Do not include option numbers\\n- Do not write \\\"Options:\\\" or similar prefixes\\n- Do not write \\\"Answer:\\\" or similar prefixes\\n- Multiple answers should be separated by commas\\n- If unsure, respond with \\\"Not mentioned\\\"\\n<end_of_system>\\n\\n<begin_of_user>\\n\" + sample['query_text'] + \"\\n<boi>IMAGE_EMBEDDING_TOKENS<eoi>\\n<end_of_user>\\n\\n<begin_of_assistant>\\n\" + sample['answer_text'] + \"<end_of_assistant>\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_llm_training_inputs(\n",
    "    args=args,\n",
    "    qids=[\"CQID034\", \"CQID012\", \"CQID011\"],      \n",
    "    multi_label_examples=True,                   \n",
    "    single_label_examples=True,                  \n",
    "    num_samples_per_qid=1,                       \n",
    "    show_images=True                             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.skip_data_prep:\n",
    "    total_examples = preprocess_dataset(val_df, batch_size=100, mode=\"val\")\n",
    "    print(f\"Total processed examples: {total_examples}\")\n",
    "else:\n",
    "    print(\"Skipping data preparation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da78e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_inference_inputs(processed_val_dir=None, num_samples=3):\n",
    "    \"\"\"\n",
    "    Inspect the actual inputs being used during inference by examining the\n",
    "    processed validation data files.\n",
    "    \n",
    "    Args:\n",
    "        processed_val_dir: Directory containing processed validation data\n",
    "        num_samples: Number of samples to display\n",
    "    \"\"\"\n",
    "    if processed_val_dir is None:\n",
    "        processed_val_dir = PROCESSED_VAL_DATA_DIR\n",
    "        \n",
    "    batch_files = sorted([f for f in os.listdir(processed_val_dir) \n",
    "                         if f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No batch files found in {processed_val_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(batch_files)} batch files in {processed_val_dir}\")\n",
    "    \n",
    "    with open(os.path.join(processed_val_dir, batch_files[0]), 'rb') as f:\n",
    "        batch_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"Batch contains {len(batch_data)} samples\")\n",
    "    \n",
    "    for i, sample in enumerate(batch_data[:num_samples]):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SAMPLE {i+1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(f\"Question ID: {sample['qid']}\")\n",
    "        \n",
    "        print(\"\\nSYSTEM MESSAGE (Used during inference):\")\n",
    "        print(\"-\" * 80)\n",
    "        system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "        \n",
    "        IMPORTANT: \n",
    "        - Respond ONLY with the exact text of the option(s) that apply\n",
    "        - Do not provide any explanations\n",
    "        - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\"\n",
    "        - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "        - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "        \"\"\"\n",
    "        \n",
    "        print(system_message)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nUSER MESSAGE TEXT (Used during inference):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(sample['query_text'])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nIMAGE (Used during inference):\")\n",
    "        print(f\"Image path: {sample['image_path']}\")\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(sample['image_path'])\n",
    "            print(f\"Image dimensions: {img.size[0]}x{img.size[1]}, Format: {img.format}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open image: {e}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        if i >= num_samples - 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_inference_inputs(num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "        IMPORTANT: \n",
    "        - Respond ONLY with the exact text of the option(s) that apply\n",
    "        - Do not provide any explanations\n",
    "        - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\"\n",
    "        - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "        - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function for batching examples for vision models.\"\"\"\n",
    "    texts = []\n",
    "    all_images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        if IS_QWEN:\n",
    "            from qwen_vl_utils import process_vision_info\n",
    "            image_inputs, _ = process_vision_info(example[\"messages\"])\n",
    "            image_input = image_inputs[0] if image_inputs else None\n",
    "        else:\n",
    "            image_input = None\n",
    "            for msg in example[\"messages\"]:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    for content in msg[\"content\"]:\n",
    "                        if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                            image_input = content[\"image\"]\n",
    "                            break\n",
    "        \n",
    "        if image_input is None:\n",
    "            image_input = Image.new('RGB', (224, 224), color='black')\n",
    "            \n",
    "        texts.append(text.strip())\n",
    "        all_images.append(image_input)\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=all_images,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    if processor.tokenizer.pad_token_id is not None:\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    if IS_LLAMA:\n",
    "        for token_id in processor.tokenizer.all_special_ids:\n",
    "            labels[labels == token_id] = -100\n",
    "        \n",
    "        try:\n",
    "            if hasattr(processor, \"image_token\"):\n",
    "                image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\n",
    "                labels[labels == image_token_id] = -100\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        if hasattr(processor, \"image_token_index\"):\n",
    "            labels[labels == processor.image_token_index] = -100\n",
    "        else:\n",
    "            image_tokens = []\n",
    "            \n",
    "            image_token_names = [\"<boi>\", \"<eoi>\", \"<image>\", \"<|image|>\", \"bos_token\", \"eos_token\"]\n",
    "            for token_name in image_token_names:\n",
    "                try:\n",
    "                    token_id = processor.tokenizer.convert_tokens_to_ids(token_name)\n",
    "                    if token_id != processor.tokenizer.unk_token_id:\n",
    "                        image_tokens.append(token_id)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            for token_id in image_tokens:\n",
    "                labels[labels == token_id] = -100\n",
    "                \n",
    "        special_token_patterns = [\"<|begin_of_\", \"<|end_of_\", \"<|start_header\", \"<|end_header\", \n",
    "                                \"<|eot_id|>\", \"<|begin_of_text|>\"]\n",
    "        for pattern in special_token_patterns:\n",
    "            for token, token_id in processor.tokenizer.get_vocab().items():\n",
    "                if pattern in token:\n",
    "                    labels[labels == token_id] = -100\n",
    "    \n",
    "    elif IS_QWEN:\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "        for token_id in image_tokens:\n",
    "            labels[labels == token_id] = -100\n",
    "        \n",
    "        for token_id in processor.tokenizer.all_special_ids:\n",
    "            labels[labels == token_id] = -100\n",
    "            \n",
    "    else:\n",
    "        try:\n",
    "            for special_token in [\"boi_token\", \"eoi_token\"]:\n",
    "                if special_token in processor.tokenizer.special_tokens_map:\n",
    "                    token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "                        processor.tokenizer.special_tokens_map[special_token]\n",
    "                    )\n",
    "                    labels[labels == token_id] = -100\n",
    "            \n",
    "            labels[labels == 262144] = -100\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not mask tokens: {e}\")\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_combined_dataset:\n",
    "    dataset = MedicalImageDataset(PROCESSED_COMBINED_DATA_DIR, processor)\n",
    "else:\n",
    "    dataset = MedicalImageDataset(PROCESSED_TRAIN_DATA_DIR, processor)\n",
    "    \n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        example = dataset[i]\n",
    "        print(f\"\\nExample {i+1} message structure:\")\n",
    "        print(example)\n",
    "    \n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"\\nCollated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c64995",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32722ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LLAMA or IS_QWEN:\n",
    "    target_modules = [\"q_proj\", \"v_proj\"]\n",
    "else:\n",
    "    target_modules = \"all-linear\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=None if IS_LLAMA else [\"lm_head\", \"embed_tokens\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c35280",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=MODEL_SAVE_DIRECTORY,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5229d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(training_dir=None, window_size=10):\n",
    "    if training_dir is None:\n",
    "        training_dir = find_latest_training_dir()\n",
    "    \n",
    "    dir_name = os.path.basename(training_dir)\n",
    "    \n",
    "    log_dir = os.path.join(training_dir, \"runs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise FileNotFoundError(f\"TensorBoard logs not found in {log_dir}\")\n",
    "    \n",
    "    event_files = []\n",
    "    for root, dirs, files in os.walk(log_dir):\n",
    "        for file in files:\n",
    "            if file.startswith(\"events.out.tfevents\"):\n",
    "                event_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(\"No TensorBoard event files found\")\n",
    "    \n",
    "    latest_event = max(event_files, key=os.path.getmtime)\n",
    "    \n",
    "    ea = event_accumulator.EventAccumulator(os.path.dirname(latest_event))\n",
    "    ea.Reload()\n",
    "    \n",
    "    tags = ea.Tags()\n",
    "    loss_tag = None\n",
    "    for tag in tags['scalars']:\n",
    "        if 'loss' in tag.lower():\n",
    "            loss_tag = tag\n",
    "            break\n",
    "    \n",
    "    if not loss_tag:\n",
    "        raise ValueError(\"No loss data found in TensorBoard logs\")\n",
    "    \n",
    "    train_loss_steps = []\n",
    "    train_loss_values = []\n",
    "    for event in ea.Scalars(loss_tag):\n",
    "        train_loss_steps.append(event.step)\n",
    "        train_loss_values.append(event.value)\n",
    "    \n",
    "    loss_df = pd.DataFrame({\n",
    "        'Step': train_loss_steps,\n",
    "        'Training Loss': train_loss_values\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_df['Step'], loss_df['Training Loss'], label='Training Loss')\n",
    "    plt.title(f'Training Loss Curve for {dir_name}')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    if len(loss_df) > window_size:\n",
    "        loss_df['Moving Avg'] = loss_df['Training Loss'].rolling(window=window_size).mean()\n",
    "        plt.plot(loss_df['Step'], loss_df['Moving Avg'], 'r-', \n",
    "                 label=f'Moving Average (window={window_size})')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = os.path.join(OUTPUT_DIR, f\"training_loss_{dir_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training loss plot saved to {plot_path}\")\n",
    "    \n",
    "    step_interval = max(1, len(loss_df) // 20)\n",
    "    print(\"\\nTraining Loss Summary:\")\n",
    "    print(\"Step\\tTraining Loss\\tMoving Avg\")\n",
    "    for i in range(0, len(loss_df), step_interval):\n",
    "        if 'Moving Avg' in loss_df.columns:\n",
    "            print(f\"{loss_df.iloc[i]['Step']:.0f}\\t{loss_df.iloc[i]['Training Loss']:.4f}\\t{loss_df.iloc[i]['Moving Avg']:.5f}\")\n",
    "        else:\n",
    "            print(f\"{loss_df.iloc[i]['Step']:.0f}\\t{loss_df.iloc[i]['Training Loss']:.4f}\\tNaN\")\n",
    "    \n",
    "    return loss_df\n",
    "\n",
    "def plot_training_loss_from_args(args, window_size=10):\n",
    "    \"\"\"Plot training loss using the checkpoint information from args\"\"\"\n",
    "    if not args.use_finetuning:\n",
    "        print(\"Not plotting training loss because base model is being used.\")\n",
    "        return None\n",
    "        \n",
    "    if hasattr(args, 'latest_checkpoint'):\n",
    "        training_dir = os.path.dirname(args.latest_checkpoint)\n",
    "        \n",
    "        loss_df = plot_training_loss(training_dir=training_dir, window_size=window_size)\n",
    "        \n",
    "        return loss_df\n",
    "    else:\n",
    "        print(\"No fine-tuned model checkpoint available in args\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652aa712",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss_from_args(args, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bd764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea82a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_dataset():\n",
    "    \"\"\"\n",
    "    Create a dataset for test data without ground truth answers.\n",
    "    Similar structure to prepare_dataset but adapted for test set.\n",
    "    \"\"\"\n",
    "    print(\"Preparing test dataset...\")\n",
    "    \n",
    "    json_path = TEST_JSON_PATH\n",
    "    images_dir = TEST_IMAGES_DIR\n",
    "    output_filename = \"test_dataset.csv\"\n",
    "    \n",
    "    # Load questions from the same source as train/val\n",
    "    with open(QUESTIONS_PATH, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "        \n",
    "    questions_df = pd.json_normalize(questions)[[\"qid\", \"question_en\", \"options_en\", \"question_type_en\", \"question_category_en\"]]\n",
    "    \n",
    "    # Load test data\n",
    "    input_df = pd.read_json(json_path)\n",
    "    \n",
    "    query_info_df = input_df[[\"encounter_id\", \"image_ids\", \"query_title_en\", \"query_content_en\", \"author_id\"]]\n",
    "    \n",
    "    # Get unique base QIDs to use as our question set\n",
    "    base_qids = sorted(list(set([q.split('-')[0] if '-' in q else q for q in questions_df['qid']])))\n",
    "    \n",
    "    # Get a representative question for each base QID\n",
    "    question_representatives = {}\n",
    "    for base_qid in base_qids:\n",
    "        matching_questions = questions_df[questions_df['qid'].str.startswith(base_qid)]\n",
    "        if not matching_questions.empty:\n",
    "            question_representatives[base_qid] = matching_questions.iloc[0]\n",
    "    \n",
    "    dataset_rows = []\n",
    "    \n",
    "    # For each encounter in test set, create entries for all questions and images\n",
    "    for _, row in tqdm(query_info_df.iterrows(), desc=\"Creating test dataset\"):\n",
    "        encounter_id = row['encounter_id']\n",
    "        image_ids = row['image_ids']\n",
    "        query_title = row['query_title_en']\n",
    "        query_content = row['query_content_en']\n",
    "        author_id = row['author_id']\n",
    "        \n",
    "        for base_qid in base_qids:\n",
    "            if base_qid in question_representatives:\n",
    "                question_info = question_representatives[base_qid]\n",
    "                question_text = question_info['question_en']\n",
    "                \n",
    "                # Clean options consistently with train/val\n",
    "                options_en = question_info['options_en']\n",
    "                if isinstance(options_en, str):\n",
    "                    options_en = safe_convert_options(options_en)\n",
    "                \n",
    "                # Clean options - remove \"(please specify)\" as done in the training process\n",
    "                cleaned_options = []\n",
    "                for opt in options_en:\n",
    "                    if isinstance(opt, str):\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                \n",
    "                # Use the cleaned options\n",
    "                options_en = cleaned_options\n",
    "                \n",
    "                question_type_en = question_info['question_type_en']\n",
    "                question_category_en = question_info['question_category_en']\n",
    "                \n",
    "                for img_id in image_ids:\n",
    "                    img_path = os.path.join(images_dir, img_id)\n",
    "                    \n",
    "                    if not os.path.exists(img_path):\n",
    "                        print(f\"Warning: Image {img_id} not found at {img_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    dataset_rows.append({\n",
    "                        'encounter_id': encounter_id,\n",
    "                        'base_qid': base_qid,\n",
    "                        'image_id': img_id,\n",
    "                        'image_path': img_path,\n",
    "                        'question_text': question_text,\n",
    "                        'query_title_en': query_title,\n",
    "                        'query_content_en': query_content,\n",
    "                        'author_id': author_id,\n",
    "                        'options_en': options_en,\n",
    "                        'question_type_en': question_type_en, \n",
    "                        'question_category_en': question_category_en\n",
    "                    })\n",
    "    \n",
    "    dataset = pd.DataFrame(dataset_rows)\n",
    "    \n",
    "    dataset.to_csv(os.path.join(OUTPUT_DIR, output_filename), index=False)\n",
    "    \n",
    "    print(f\"Test dataset created with {len(dataset)} entries\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def process_test_dataset_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of test data samples - similar to process_dataset_batch\n",
    "    but adapted for test data (no ground truth answers)\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get the same fields as in training, using same format\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Use the same option processing as in training\n",
    "            if 'options_en' in row:\n",
    "                options = row['options_en']\n",
    "                if isinstance(options, str):\n",
    "                    options = safe_convert_options(options)\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                \n",
    "            options_text = \", \".join(options)\n",
    "            \n",
    "            # Format metadata consistently with training\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Apply the same cleaning steps as in training\n",
    "            if \"Please specify which affected area for each selection\" in question:\n",
    "                question = question.replace(\" Please specify which affected area for each selection.\", \"\")\n",
    "            \n",
    "            question = re.sub(r'^\\d+\\s+', '', question)\n",
    "            \n",
    "            # Format context consistently with training\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "            \n",
    "            clinical_context = \"\"\n",
    "            if query_title or query_content:\n",
    "                clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "                if query_title:\n",
    "                    clinical_context += f\"{query_title}\\n\"\n",
    "                if query_content:\n",
    "                    clinical_context += f\"{query_content}\\n\"\n",
    "\n",
    "            # Format query text in the same format as training\n",
    "            query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Available Options (choose from these): {options_text}\")\n",
    "            \n",
    "            # Use the same field names as in training to avoid issues in inference\n",
    "            data_item = {\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"image_id\": os.path.basename(image_path),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            }\n",
    "            \n",
    "            batch_data.append(data_item)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Use consistent prefix for test batches\n",
    "    batch_file = os.path.join(save_dir, f\"test_batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_test_dataset(df, batch_size=50, save_dir=None, images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the test dataset in batches.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the dataset\n",
    "        batch_size: Number of examples per batch\n",
    "        save_dir: Directory to save processed batches\n",
    "        images_dir: Directory containing images\n",
    "        \n",
    "    Returns:\n",
    "        Total number of processed examples\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = PROCESSED_TEST_DATA_DIR\n",
    "    \n",
    "    if images_dir is None:\n",
    "        images_dir = TEST_IMAGES_DIR\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_test_dataset_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00c7bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf1d31b8cea40de9b52306a22e6117b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test dataset: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset created with 2826 entries\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(PROCESSED_TEST_DATA_DIR, exist_ok=True)\n",
    "test_df = prepare_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae575cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>question_text</th>\n",
       "      <th>query_title_en</th>\n",
       "      <th>query_content_en</th>\n",
       "      <th>author_id</th>\n",
       "      <th>options_en</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00908</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00908_00001.jpg</td>\n",
       "      <td>/storage/scratch1/2/kthakrar3/mediqa-magic-v2/...</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>Take a look. Is this a skin disease?</td>\n",
       "      <td>Picture 1:  On the outside of the thigh, there...</td>\n",
       "      <td>U13429</td>\n",
       "      <td>[single spot, limited area, widespread, Not me...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00908</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00908_00002.jpg</td>\n",
       "      <td>/storage/scratch1/2/kthakrar3/mediqa-magic-v2/...</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>Take a look. Is this a skin disease?</td>\n",
       "      <td>Picture 1:  On the outside of the thigh, there...</td>\n",
       "      <td>U13429</td>\n",
       "      <td>[single spot, limited area, widespread, Not me...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00908</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00908_00001.jpg</td>\n",
       "      <td>/storage/scratch1/2/kthakrar3/mediqa-magic-v2/...</td>\n",
       "      <td>1 Where is the affected area?</td>\n",
       "      <td>Take a look. Is this a skin disease?</td>\n",
       "      <td>Picture 1:  On the outside of the thigh, there...</td>\n",
       "      <td>U13429</td>\n",
       "      <td>[head, neck, upper extremities, lower extremit...</td>\n",
       "      <td>Site Location</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00908</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00908_00002.jpg</td>\n",
       "      <td>/storage/scratch1/2/kthakrar3/mediqa-magic-v2/...</td>\n",
       "      <td>1 Where is the affected area?</td>\n",
       "      <td>Take a look. Is this a skin disease?</td>\n",
       "      <td>Picture 1:  On the outside of the thigh, there...</td>\n",
       "      <td>U13429</td>\n",
       "      <td>[head, neck, upper extremities, lower extremit...</td>\n",
       "      <td>Site Location</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00908</td>\n",
       "      <td>CQID012</td>\n",
       "      <td>IMG_ENC00908_00001.jpg</td>\n",
       "      <td>/storage/scratch1/2/kthakrar3/mediqa-magic-v2/...</td>\n",
       "      <td>1 How large are the affected areas? Please spe...</td>\n",
       "      <td>Take a look. Is this a skin disease?</td>\n",
       "      <td>Picture 1:  On the outside of the thigh, there...</td>\n",
       "      <td>U13429</td>\n",
       "      <td>[size of thumb nail, size of palm, larger area...</td>\n",
       "      <td>Size</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid                image_id  \\\n",
       "0     ENC00908  CQID010  IMG_ENC00908_00001.jpg   \n",
       "1     ENC00908  CQID010  IMG_ENC00908_00002.jpg   \n",
       "2     ENC00908  CQID011  IMG_ENC00908_00001.jpg   \n",
       "3     ENC00908  CQID011  IMG_ENC00908_00002.jpg   \n",
       "4     ENC00908  CQID012  IMG_ENC00908_00001.jpg   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /storage/scratch1/2/kthakrar3/mediqa-magic-v2/...   \n",
       "1  /storage/scratch1/2/kthakrar3/mediqa-magic-v2/...   \n",
       "2  /storage/scratch1/2/kthakrar3/mediqa-magic-v2/...   \n",
       "3  /storage/scratch1/2/kthakrar3/mediqa-magic-v2/...   \n",
       "4  /storage/scratch1/2/kthakrar3/mediqa-magic-v2/...   \n",
       "\n",
       "                                       question_text  \\\n",
       "0                  How much of the body is affected?   \n",
       "1                  How much of the body is affected?   \n",
       "2                      1 Where is the affected area?   \n",
       "3                      1 Where is the affected area?   \n",
       "4  1 How large are the affected areas? Please spe...   \n",
       "\n",
       "                         query_title_en  \\\n",
       "0  Take a look. Is this a skin disease?   \n",
       "1  Take a look. Is this a skin disease?   \n",
       "2  Take a look. Is this a skin disease?   \n",
       "3  Take a look. Is this a skin disease?   \n",
       "4  Take a look. Is this a skin disease?   \n",
       "\n",
       "                                    query_content_en author_id  \\\n",
       "0  Picture 1:  On the outside of the thigh, there...    U13429   \n",
       "1  Picture 1:  On the outside of the thigh, there...    U13429   \n",
       "2  Picture 1:  On the outside of the thigh, there...    U13429   \n",
       "3  Picture 1:  On the outside of the thigh, there...    U13429   \n",
       "4  Picture 1:  On the outside of the thigh, there...    U13429   \n",
       "\n",
       "                                          options_en question_type_en  \\\n",
       "0  [single spot, limited area, widespread, Not me...             Site   \n",
       "1  [single spot, limited area, widespread, Not me...             Site   \n",
       "2  [head, neck, upper extremities, lower extremit...    Site Location   \n",
       "3  [head, neck, upper extremities, lower extremit...    Site Location   \n",
       "4  [size of thumb nail, size of palm, larger area...             Size   \n",
       "\n",
       "  question_category_en  \n",
       "0              General  \n",
       "1              General  \n",
       "2              General  \n",
       "3              General  \n",
       "4              General  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbb0bf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['single', 'multiple', 'Not mentioned']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['options_en'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cae9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f889d81c4360421985a7da96446b4f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples so far\n",
      "Processing batch 2/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041cfb305c004a3dab1d60403bf37e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 examples so far\n",
      "Processing batch 3/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f626fb0c7f4b07b890cab1a5523cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300 examples so far\n",
      "Processing batch 4/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb7279165f47c091644cdecace1c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 examples so far\n",
      "Processing batch 5/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab64e42d4db5455fa080692f29624eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 examples so far\n",
      "Processing batch 6/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174e3cc03e5a4dcba796b39c7469f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600 examples so far\n",
      "Processing batch 7/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed56d9363b34c10a67b690fe53bf4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 6:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 700 examples so far\n",
      "Processing batch 8/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3a7be8fc8a41e0b4c133078d06e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 7:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 800 examples so far\n",
      "Processing batch 9/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98de086c8348de84fb52e5b1fa2fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 8:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 900 examples so far\n",
      "Processing batch 10/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d332e6d1384a259389bc6b4a133765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 9:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 examples so far\n",
      "Processing batch 11/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3c7c8cf63340bcaa2b2c78742a877d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 10:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1100 examples so far\n",
      "Processing batch 12/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe007a5dffd5457faa18c368230904db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 11:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1200 examples so far\n",
      "Processing batch 13/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef29cbff384e4ded98241b0378545d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 12:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1300 examples so far\n",
      "Processing batch 14/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0969fb524bc4b60a8017222e43ff05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 13:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400 examples so far\n",
      "Processing batch 15/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e2c043f968414a85175e276e35d8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 14:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1500 examples so far\n",
      "Processing batch 16/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c76e1146554a9192f519626f09d489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 15:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1600 examples so far\n",
      "Processing batch 17/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3174a0e3aa34ed78f110686417aca9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 16:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1700 examples so far\n",
      "Processing batch 18/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ffd44e11764686830593dc64794eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 17:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800 examples so far\n",
      "Processing batch 19/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3d1a8c3b284a70957b579b4fc19042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 18:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1900 examples so far\n",
      "Processing batch 20/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa97d4c156b494aa49a6f77e72ff934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 19:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 examples so far\n",
      "Processing batch 21/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d46ecc08bae43f1b8829e5435adbc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 20:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2100 examples so far\n",
      "Processing batch 22/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d5b2731ffe4413b12b285b895307f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 21:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2200 examples so far\n",
      "Processing batch 23/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acddb064f0e406fb9b026d8f00b4488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 22:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2300 examples so far\n",
      "Processing batch 24/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a318d4a6c2f346678672a233927d37c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 23:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2400 examples so far\n",
      "Processing batch 25/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9476f1a06e754a0f8ab64791cb27bde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 24:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 examples so far\n",
      "Processing batch 26/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da41358e3b414d80ab5dad3f6e75fa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 25:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2600 examples so far\n",
      "Processing batch 27/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d508a01484c47d692dcd8364aef230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 26:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2700 examples so far\n",
      "Processing batch 28/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb10314a865475d9ef7ea70aa30fc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 27:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2800 examples so far\n",
      "Processing batch 29/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb7cdd9c41b41a3a9e86e0f0c0969e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 28:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2826 examples so far\n",
      "Total processed test examples: 2826\n"
     ]
    }
   ],
   "source": [
    "total_examples = preprocess_test_dataset(test_df, batch_size=100)\n",
    "print(f\"Total processed test examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18d86e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 100 examples in /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/processed_test_data-gemma-3-12b-it-V3/test_batch_0.pkl\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE #1\n",
      "================================================================================\n",
      "id: ENC00908\n",
      "qid: CQID010\n",
      "image_id: IMG_ENC00908_00001.jpg\n",
      "query_text (truncated): MAIN QUESTION TO ANSWER: How much of the body is affected?\n",
      "Question Metadata: Type: Site, Category: ...\n",
      "image_path: IMG_ENC00908_00001.jpg\n",
      "question_type: Site\n",
      "question_category: General\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE #2\n",
      "================================================================================\n",
      "id: ENC00908\n",
      "qid: CQID010\n",
      "image_id: IMG_ENC00908_00002.jpg\n",
      "query_text (truncated): MAIN QUESTION TO ANSWER: How much of the body is affected?\n",
      "Question Metadata: Type: Site, Category: ...\n",
      "image_path: IMG_ENC00908_00002.jpg\n",
      "question_type: Site\n",
      "question_category: General\n"
     ]
    }
   ],
   "source": [
    "def inspect_test_processed_examples():\n",
    "    \"\"\"\n",
    "    Inspect the first two processed test examples to see their structure.\n",
    "    \"\"\"\n",
    "    batch_file = os.path.join(PROCESSED_TEST_DATA_DIR, \"test_batch_0.pkl\")\n",
    "    \n",
    "    try:\n",
    "        with open(batch_file, 'rb') as f:\n",
    "            batch_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"\\nFound {len(batch_data)} examples in {batch_file}\")\n",
    "        \n",
    "        for i, example in enumerate(batch_data[:2]):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"EXAMPLE #{i+1}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            for key, value in example.items():\n",
    "                if key == 'image_path':\n",
    "                    print(f\"{key}: {os.path.basename(value)}\")\n",
    "                elif key == 'query_text':\n",
    "                    print(f\"{key} (truncated): {value[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        \n",
    "        return batch_data[:2]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting processed test data: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "test_examples = inspect_test_processed_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "305c0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageInference:\n",
    "    def __init__(self, model_path, token=None, adapter_path=None, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the inference class for medical image analysis.\n",
    "        Parameters:\n",
    "        - model_path: Path to the model (base model or merged model)\n",
    "        - token: HF token for downloading models\n",
    "        - adapter_path: Path to adapter weights (only used if not using merged model)\n",
    "        - device: Computing device (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        print(f\"Loading processor from {model_path}...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path, token=token)\n",
    "\n",
    "        base_kwargs = dict(\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            low_cpu_mem_usage=True,\n",
    "            token=token\n",
    "        )\n",
    "\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        if IS_LLAMA and adapter_path:\n",
    "            self.model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                model_path,\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                token=token\n",
    "            )\n",
    "            print(f\"Loading adapter from {adapter_path}...\")\n",
    "            from peft import PeftModel\n",
    "            self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
    "        elif IS_QWEN and adapter_path:\n",
    "\n",
    "            base_kwargs[\"attn_implementation\"] = \"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    "            if \"Qwen2.5-VL\" in model_path:\n",
    "                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                    model_path,\n",
    "                    **base_kwargs\n",
    "                )\n",
    "            else:\n",
    "                self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                    model_path,\n",
    "                    **base_kwargs\n",
    "                )\n",
    "            print(f\"Loading adapter from {adapter_path}...\")\n",
    "            from peft import PeftModel\n",
    "            self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
    "        else:\n",
    "\n",
    "            if IS_LLAMA:\n",
    "                self.model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                    model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    device_map=\"auto\",\n",
    "                    token=token\n",
    "                )\n",
    "            elif IS_QWEN:\n",
    "                base_kwargs[\"attn_implementation\"] = \"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    "                if \"Qwen2.5-VL\" in model_path:\n",
    "                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                        model_path,\n",
    "                        **base_kwargs\n",
    "                    )\n",
    "                else:\n",
    "                    self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                        model_path,\n",
    "                        **base_kwargs\n",
    "                    )\n",
    "            else:\n",
    "                non_llama_kwargs = base_kwargs.copy()\n",
    "                non_llama_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "                self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                    model_path,\n",
    "                    **non_llama_kwargs\n",
    "                )\n",
    "        self.model.eval()\n",
    "        self.IS_QWEN = IS_QWEN\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "    def predict(self, query_text, image_path, max_new_tokens=100):\n",
    "        try:\n",
    "\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "            IMPORTANT: \n",
    "            - Respond ONLY with the exact text of the option(s) that apply\n",
    "            - Do not provide any explanations\n",
    "            - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "            - Do not write \"Options:\" or similar prefixes\n",
    "            - Do not write \"Answer:\" or similar prefixes\n",
    "            - Multiple answers should be separated by commas\n",
    "            - If unsure, respond with \"Not mentioned\"\n",
    "            - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "            - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "            \"\"\"\n",
    "\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query_text},\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            if self.IS_QWEN:\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                image_inputs, video_inputs = process_vision_info(messages)\n",
    "                inputs = self.processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "            else:\n",
    "                inputs = self.processor(\n",
    "                    text=self.processor.apply_chat_template(messages, tokenize=False),\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "            inputs = inputs.to(self.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                generation_params = {\n",
    "                    \"max_new_tokens\": max_new_tokens,\n",
    "                    \"do_sample\": True,\n",
    "                    \"temperature\": 0.9,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"top_k\": 64\n",
    "                }\n",
    "\n",
    "                if not (self.IS_QWEN or IS_LLAMA):\n",
    "                    generation_params[\"disable_compile\"] = True\n",
    "\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    **generation_params\n",
    "                )\n",
    "\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            prediction = self.processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "\n",
    "            prediction = prediction.strip()\n",
    "\n",
    "            if prediction.startswith(\"assistant\\n\\n\"):\n",
    "                prediction = prediction[len(\"assistant\\n\\n\"):]\n",
    "\n",
    "            if prediction.startswith(\"assistant\\n\"):\n",
    "                prediction = prediction[len(\"assistant\\n\"):]\n",
    "\n",
    "            if prediction.startswith(\"system\\n\"):\n",
    "                prediction = prediction[len(\"system\\n\"):]\n",
    "\n",
    "            if prediction.startswith(\"model\\n\"):\n",
    "                prediction = prediction[len(\"model\\n\"):]\n",
    "\n",
    "            prediction = re.sub(r'^\\*+\\s*', '', prediction)\n",
    "            prediction = re.sub(r'\\n\\*+\\s*', ' ', prediction)\n",
    "            prediction = re.sub(r'\\*\\s*', '', prediction)\n",
    "\n",
    "            if prediction.startswith(\"Note:\") or prediction.startswith(\"Disclaimer:\") or prediction.startswith(\"*Note:\"):\n",
    "                if \"\\n\" in prediction:\n",
    "                    prediction = prediction.split(\"\\n\", 1)[1].strip()\n",
    "\n",
    "            if \"Answer:\" in prediction:\n",
    "                parts = prediction.split(\"Answer:\")\n",
    "                if len(parts) > 1:\n",
    "                    prediction = parts[1].strip()\n",
    "\n",
    "            if prediction.endswith(\".\"):\n",
    "                prediction = prediction[:-1]\n",
    "\n",
    "            if prediction.startswith(\"<start_of_turn>model\") or prediction.startswith(\"<start_of_turn>assistant\"):\n",
    "                prediction = prediction.split(\"\\n\", 1)[1] if \"\\n\" in prediction else \"\"\n",
    "            if prediction.endswith(\"<end_of_turn>\"):\n",
    "                prediction = prediction[:-len(\"<end_of_turn>\")]\n",
    "\n",
    "            return prediction.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {image_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return \"Not mentioned\"\n",
    "    \n",
    "    def batch_predict(self, processed_data_dir=None, output_file=None, max_samples=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of preprocessed data\n",
    "        \"\"\"\n",
    "        if processed_data_dir is None:\n",
    "            processed_data_dir = PROCESSED_VAL_DATA_DIR\n",
    "\n",
    "        if output_file is None:\n",
    "            output_file = os.path.join(OUTPUT_DIR, \"predictions.csv\")\n",
    "\n",
    "        results = []\n",
    "        sample_count = 0\n",
    "\n",
    "        # Determine the batch file prefix based on the directory\n",
    "        if \"test\" in processed_data_dir.lower():\n",
    "            batch_prefix = \"test_batch_\"\n",
    "        else:\n",
    "            batch_prefix = \"val_batch_\"\n",
    "\n",
    "        batch_files = sorted([f for f in os.listdir(processed_data_dir) \n",
    "                              if f.startswith(batch_prefix) and f.endswith(\".pkl\")])\n",
    "\n",
    "        if not batch_files:\n",
    "            print(f\"Warning: No batch files found in {processed_data_dir} with prefix {batch_prefix}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        for batch_file in tqdm(batch_files, desc=\"Processing batches\"):\n",
    "            with open(os.path.join(processed_data_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "\n",
    "            for sample in tqdm(batch_data, desc=f\"Predicting {batch_file}\", leave=False):\n",
    "                prediction = self.predict(sample[\"query_text\"], sample[\"image_path\"])\n",
    "\n",
    "                results.append({\n",
    "                    \"encounter_id\": sample.get(\"encounter_id\", sample.get(\"id\", \"\")),\n",
    "                    \"base_qid\": sample.get(\"base_qid\", sample.get(\"qid\", \"\")),\n",
    "                    \"image_id\": os.path.basename(sample[\"image_path\"]),\n",
    "                    \"prediction\": prediction\n",
    "                })\n",
    "\n",
    "                sample_count += 1\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "            if max_samples and sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "        print(f\"Processed {sample_count} samples for prediction\")\n",
    "\n",
    "        if not results:\n",
    "            print(\"Warning: No prediction results generated\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    def aggregate_predictions(self, predictions_df, validation_df=None, test_df=None):\n",
    "        \"\"\"\n",
    "        Aggregate predictions for each encounter and question ID.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions_df: DataFrame with prediction results\n",
    "        - validation_df: Optional DataFrame containing validation data with options_en\n",
    "        - test_df: Optional DataFrame containing test data with options_en\n",
    "        \"\"\"\n",
    "        max_answers = {\n",
    "            'CQID010': 1,\n",
    "            'CQID011': 6,\n",
    "            'CQID012': 6,\n",
    "            'CQID015': 1,\n",
    "            'CQID020': 9,\n",
    "            'CQID025': 1,\n",
    "            'CQID034': 1,\n",
    "            'CQID035': 1,\n",
    "            'CQID036': 1\n",
    "        }\n",
    "\n",
    "        default_max_answers = 1\n",
    "\n",
    "        grouped = predictions_df.groupby(['encounter_id', 'base_qid'])\n",
    "\n",
    "        aggregated_results = []\n",
    "\n",
    "        for (encounter_id, base_qid), group in tqdm(grouped, desc=\"Aggregating predictions\"):\n",
    "            predictions = group['prediction'].tolist()\n",
    "            image_ids = group['image_id'].tolist()\n",
    "\n",
    "            cleaned_predictions = []\n",
    "            for pred in predictions:\n",
    "                if isinstance(pred, str):\n",
    "                    pred = pred.replace(\" (please specify)\", \"\")\n",
    "\n",
    "                    if pred.startswith('[') and pred.endswith(']'):\n",
    "                        try:\n",
    "                            pred_list = safe_convert_options(pred)\n",
    "                            if isinstance(pred_list, list):\n",
    "                                pred_list = [p.replace(\" (please specify)\", \"\") if isinstance(p, str) else p for p in pred_list]\n",
    "                                cleaned_predictions.extend(pred_list)\n",
    "                                continue\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    if ',' in pred:\n",
    "                        items = [p.strip().replace(\" (please specify)\", \"\") for p in pred.split(',')]\n",
    "                        cleaned_predictions.extend(items)\n",
    "                    else:\n",
    "                        cleaned_predictions.append(pred.strip())\n",
    "                else:\n",
    "                    cleaned_predictions.append(str(pred).strip())\n",
    "\n",
    "            all_cleaned_predictions = cleaned_predictions.copy()\n",
    "\n",
    "            cleaned_predictions = [p.lower() if isinstance(p, str) else str(p).lower() for p in cleaned_predictions]\n",
    "            prediction_counts = Counter(cleaned_predictions)\n",
    "\n",
    "            question_type = base_qid.split('-')[0] if '-' in base_qid else base_qid\n",
    "\n",
    "            allowed_max = max_answers.get(question_type, default_max_answers)\n",
    "\n",
    "            sorted_predictions = sorted(prediction_counts.items(), \n",
    "                                       key=lambda x: x[1], \n",
    "                                       reverse=True)\n",
    "\n",
    "            all_sorted_predictions = sorted_predictions.copy()\n",
    "\n",
    "            top_predictions = [p[0] for p in sorted_predictions[:allowed_max]]\n",
    "\n",
    "            if len(sorted_predictions) > allowed_max:\n",
    "                cutoff_count = sorted_predictions[allowed_max-1][1]\n",
    "                tied_predictions = [p[0] for p in sorted_predictions if p[1] == cutoff_count]\n",
    "\n",
    "                if len(tied_predictions) > 1 and len(top_predictions) > allowed_max - len(tied_predictions):\n",
    "                    top_predictions = [p for p in top_predictions if p not in tied_predictions]\n",
    "\n",
    "                    random.seed(42)\n",
    "                    slots_remaining = allowed_max - len(top_predictions)\n",
    "                    selected_tied = random.sample(tied_predictions, slots_remaining)\n",
    "\n",
    "                    top_predictions.extend(selected_tied)\n",
    "\n",
    "            if len(top_predictions) > 1 and \"not mentioned\" in top_predictions:\n",
    "                top_predictions.remove(\"not mentioned\")\n",
    "\n",
    "            combined_prediction = \", \".join(top_predictions)\n",
    "\n",
    "            options_en = None\n",
    "\n",
    "            # Try to get options from validation_df first\n",
    "            if validation_df is not None:\n",
    "                matching_rows = validation_df[(validation_df['encounter_id'] == encounter_id) & \n",
    "                                             (validation_df['base_qid'] == base_qid)]\n",
    "                if not matching_rows.empty:\n",
    "                    options_en = matching_rows.iloc[0].get('options_en')\n",
    "\n",
    "            # If not found and test_df is provided, try there\n",
    "            if options_en is None and test_df is not None:\n",
    "                matching_rows = test_df[(test_df['encounter_id'] == encounter_id) & \n",
    "                                        (test_df['base_qid'] == base_qid)]\n",
    "                if not matching_rows.empty:\n",
    "                    options_en = matching_rows.iloc[0].get('options_en')\n",
    "\n",
    "            result_dict = {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"base_qid\": base_qid,\n",
    "                \"image_ids\": image_ids,\n",
    "                \"unique_predictions\": top_predictions,\n",
    "                \"combined_prediction\": combined_prediction,\n",
    "                \"all_raw_predictions\": all_cleaned_predictions,\n",
    "                \"all_sorted_predictions\": all_sorted_predictions\n",
    "            }\n",
    "\n",
    "            if options_en is not None:\n",
    "                result_dict[\"options_en\"] = options_en\n",
    "\n",
    "            aggregated_results.append(result_dict)\n",
    "\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6407fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_inference():\n",
    "    \"\"\"\n",
    "    Run inference on the test dataset.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_type = \"finetuned\" if args.use_finetuning else \"base\"\n",
    "    inference_id = f\"{MODEL_NAME}_{model_type}_{timestamp}\"\n",
    "    \n",
    "    inference = MedicalImageInference(\n",
    "        model_path=args.model_path,\n",
    "        token=HF_TOKEN,\n",
    "        adapter_path=args.adapter_path\n",
    "    )\n",
    "    \n",
    "    predictions_file = os.path.join(OUTPUT_DIR, \n",
    "                              f\"test_predictions_{inference_id}_{timestamp}{'_test' if args.test else ''}.csv\")\n",
    "    \n",
    "    print(f\"Running inference on test data (max_samples={args.max_samples if args.max_samples else 'all'})...\")\n",
    "    \n",
    "    predictions_df = inference.batch_predict(\n",
    "        processed_data_dir=PROCESSED_TEST_DATA_DIR,\n",
    "        output_file=predictions_file,\n",
    "        max_samples=args.max_samples\n",
    "    )\n",
    "    \n",
    "    return predictions_df, inference, inference_id, timestamp, predictions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49e41a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_for_official_eval_with_display(aggregated_df, output_file):\n",
    "    \"\"\"\n",
    "    Format predictions as expected by the official evaluation script,\n",
    "    mapping text answers to indices and distributing multiple answers\n",
    "    across question variants when appropriate.\n",
    "    Also displays the text values alongside their indices for verification.\n",
    "    \"\"\"\n",
    "    QIDS = [\n",
    "        \"CQID010-001\",\n",
    "        \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",\n",
    "        \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",\n",
    "        \"CQID015-001\",\n",
    "        \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "        \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",\n",
    "        \"CQID025-001\",\n",
    "        \"CQID034-001\",\n",
    "        \"CQID035-001\",\n",
    "        \"CQID036-001\",\n",
    "    ]\n",
    "    \n",
    "    qid_variants = {}\n",
    "    for qid in QIDS:\n",
    "        base_qid, variant = qid.split('-')\n",
    "        if base_qid not in qid_variants:\n",
    "            qid_variants[base_qid] = []\n",
    "        qid_variants[base_qid].append(qid)\n",
    "    \n",
    "    required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "    \n",
    "    formatted_predictions = []\n",
    "    display_info = []\n",
    "    \n",
    "    for encounter_id, group in aggregated_df.groupby('encounter_id'):\n",
    "        encounter_base_qids = set(group['base_qid'].unique())\n",
    "        \n",
    "        if not required_base_qids.issubset(encounter_base_qids):\n",
    "            print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "            continue\n",
    "        \n",
    "        pred_entry = {'encounter_id': encounter_id}\n",
    "        encounter_display = {'encounter_id': encounter_id, 'questions': []}\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            base_qid = row['base_qid']\n",
    "            \n",
    "            if base_qid not in qid_variants:\n",
    "                continue\n",
    "            \n",
    "            options = safe_convert_options(row['options_en'])\n",
    "            \n",
    "            not_mentioned_index = None\n",
    "            for i, opt in enumerate(options):\n",
    "                if opt == \"Not mentioned\":\n",
    "                    not_mentioned_index = i\n",
    "                    break\n",
    "            \n",
    "            if not_mentioned_index is None:\n",
    "                not_mentioned_index = len(options) - 1\n",
    "            \n",
    "            if isinstance(row['unique_predictions'], list):\n",
    "                predictions = row['unique_predictions']\n",
    "            else:\n",
    "                try:\n",
    "                    predictions = eval(row['unique_predictions'])\n",
    "                except:\n",
    "                    predictions = [row['unique_predictions']]\n",
    "            \n",
    "            prediction_indices = []\n",
    "            prediction_texts = []\n",
    "            \n",
    "            for pred in predictions:\n",
    "                pred_text = str(pred).strip()\n",
    "                prediction_texts.append(pred_text)\n",
    "                        \n",
    "                found = False\n",
    "                for i, option in enumerate(options):\n",
    "                    clean_option = option.replace(\" (please specify)\", \"\").lower()\n",
    "\n",
    "                    if pred_text.lower() == clean_option:\n",
    "                        prediction_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found:\n",
    "                    prediction_indices.append(100)\n",
    "            \n",
    "            unique_indices = []\n",
    "            unique_texts = []\n",
    "            for idx, text in zip(prediction_indices, prediction_texts):\n",
    "                if idx not in unique_indices:\n",
    "                    unique_indices.append(idx)\n",
    "                    unique_texts.append(text)\n",
    "            \n",
    "            if len(unique_indices) > 1 and 100 in unique_indices:\n",
    "                idx_to_remove = unique_indices.index(100)\n",
    "                unique_indices.remove(100)\n",
    "                unique_texts.pop(idx_to_remove)\n",
    "            \n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            question_display = {\n",
    "                'base_qid': base_qid,\n",
    "                'predicted_texts': unique_texts,\n",
    "                'predicted_indices': unique_indices,\n",
    "                'options': options,\n",
    "                'not_mentioned_index': not_mentioned_index,\n",
    "                'variant_assignments': {}\n",
    "            }\n",
    "            \n",
    "            if len(available_variants) == 1:\n",
    "                if unique_indices:\n",
    "                    pred_entry[available_variants[0]] = unique_indices[0]\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': unique_indices[0],\n",
    "                        'text': unique_texts[0] if unique_texts else \"None\"\n",
    "                    }\n",
    "                else:\n",
    "                    pred_entry[available_variants[0]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            else:\n",
    "                for i, idx in enumerate(unique_indices):\n",
    "                    if i < len(available_variants):\n",
    "                        pred_entry[available_variants[i]] = idx\n",
    "                        question_display['variant_assignments'][available_variants[i]] = {\n",
    "                            'index': idx,\n",
    "                            'text': unique_texts[i] if i < len(unique_texts) else \"None\"\n",
    "                        }\n",
    "                \n",
    "                for i in range(len(unique_indices), len(available_variants)):\n",
    "                    pred_entry[available_variants[i]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[i]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            encounter_display['questions'].append(question_display)\n",
    "        \n",
    "        formatted_predictions.append(pred_entry)\n",
    "        display_info.append(encounter_display)\n",
    "    \n",
    "    if not formatted_predictions:\n",
    "        print(\"Warning: No complete encounters found in the data!\")\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_predictions, f, indent=2)\n",
    "    \n",
    "    for encounter in display_info:\n",
    "        print(f\"\\nEncounter: {encounter['encounter_id']}\")\n",
    "        for question in encounter['questions']:\n",
    "            print(f\"  Question: {question['base_qid']}\")\n",
    "            print(f\"  Predicted texts: {question['predicted_texts']}\")\n",
    "            print(f\"  Predicted indices: {question['predicted_indices']}\")\n",
    "            print(f\"  'Not mentioned' index: {question['not_mentioned_index']}\")\n",
    "            print(\"  Variant assignments:\")\n",
    "            for variant, assignment in question['variant_assignments'].items():\n",
    "                print(f\"    {variant}: index={assignment['index']} ({assignment['text']})\")\n",
    "            print(f\"  Available options: {question['options']}\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea681a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor from google/gemma-3-12b-it...\n",
      "Loading model from google/gemma-3-12b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616b6293f8274ebcacfee876dfe95f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Running inference on test data (max_samples=all)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f705d2678fc440db9e741c202010b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d394e4c479da4395afa8c20bafab290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting test_batch_0.pkl:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32441b649a6497ca451d8f1f5018e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting test_batch_1.pkl:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1d48b3ac944b4897f1389b9d68c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting test_batch_10.pkl:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1202b370b34224b931e73976d1ae13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting test_batch_11.pkl:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions_df, inference, TEST_INFERENCE_MODEL_ID, TEST_INFERENCE_TIMESTAMP, test_predictions_file = run_test_inference()\n",
    "print(\"Aggregating test predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_df = inference.aggregate_predictions(test_predictions_df)\n",
    "aggregated_df = inference.aggregate_predictions(test_predictions_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf05e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_file = os.path.join(OUTPUT_DIR, \n",
    "                             f\"aggregated_test_predictions_{TEST_INFERENCE_MODEL_ID}_{TEST_INFERENCE_TIMESTAMP}{'_test' if args.test else ''}.csv\")\n",
    "aggregated_df.to_csv(aggregated_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ee213",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_json = os.path.join(OUTPUT_DIR, \n",
    "                                  f\"test_data_cvqa_sys_{TEST_INFERENCE_MODEL_ID}_{TEST_INFERENCE_TIMESTAMP}{'_test' if args.test else ''}.json\")\n",
    "format_predictions_for_official_eval_with_display(aggregated_df, test_predictions_json)\n",
    "print(f\"Formatted test predictions saved to {test_predictions_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ffccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_dir = os.path.join(OUTPUT_DIR, f\"test_submission_{TEST_INFERENCE_MODEL_ID}_{SUBMISSION_TIMESTAMP}\")\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "dest_json = os.path.join(submission_dir, \"data_cvqa_sys.json\")\n",
    "shutil.copy2(test_predictions_json, dest_json)\n",
    "\n",
    "submission_masks_dir = os.path.join(submission_dir, \"masks_preds\")\n",
    "os.makedirs(submission_masks_dir, exist_ok=True)\n",
    "\n",
    "zip_path = os.path.join(OUTPUT_DIR, f\"test_submission_{TEST_INFERENCE_MODEL_ID}_{SUBMISSION_TIMESTAMP}.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    zipf.write(dest_json, arcname=\"data_cvqa_sys.json\")\n",
    "    zipf.write(submission_masks_dir, arcname=\"masks_preds\")\n",
    "\n",
    "print(f\"Test submission package created at: {zip_path}\")\n",
    "print(f\"Files included:\")\n",
    "print(f\" - data_cvqa_sys.json (copied from {test_predictions_json})\")\n",
    "print(f\" - masks_preds/ (empty directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MODEL_ID = TEST_INFERENCE_MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_TIMESTAMP = TEST_INFERENCE_TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34051d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUFFIX = '_test' if args.test else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f302ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_type = \"finetuned\" if args.use_finetuning else \"base\"\n",
    "    inference_id = f\"{MODEL_NAME}_{model_type}_{timestamp}\"\n",
    "    \n",
    "    inference = MedicalImageInference(\n",
    "        model_path=args.model_path,\n",
    "        token=HF_TOKEN,\n",
    "        adapter_path=args.adapter_path\n",
    "    )\n",
    "    \n",
    "    predictions_file = os.path.join(OUTPUT_DIR, \n",
    "                              f\"val_predictions_{inference_id}_{timestamp}{'_test' if args.test else ''}.csv\")\n",
    "    \n",
    "    print(f\"Running inference (max_samples={args.max_samples if args.max_samples else 'all'})...\")\n",
    "    predictions_df = inference.batch_predict(PROCESSED_VAL_DATA_DIR, predictions_file, max_samples=args.max_samples)\n",
    "    return predictions_df, inference, inference_id, timestamp, predictions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd849093",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df, inference, INFERENCE_MODEL_ID, INFERENCE_TIMESTAMP, predictions_file = run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregating predictions...\")\n",
    "\n",
    "val_dataset_path = os.path.join(OUTPUT_DIR, \"val_dataset.csv\")\n",
    "val_dataset = pd.read_csv(val_dataset_path)\n",
    "\n",
    "aggregated_df = inference.aggregate_predictions(predictions_df, validation_df=val_dataset)\n",
    "\n",
    "aggregated_file = os.path.join(OUTPUT_DIR, \n",
    "                             f\"aggregated_predictions_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{'_test' if args.test else ''}.csv\")\n",
    "aggregated_df.to_csv(aggregated_file, index=False)\n",
    "\n",
    "print(f\"Inference complete. Results saved to {predictions_file} and {aggregated_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271687d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of raw predictions:\")\n",
    "predictions_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39598c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUFFIX = '_test' if args.test else ''\n",
    "\n",
    "prediction_pattern = os.path.join(OUTPUT_DIR, f\"val_predictions_*{TEST_SUFFIX}.csv\")\n",
    "prediction_files = sorted(glob.glob(prediction_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not prediction_files:\n",
    "    raise FileNotFoundError(f\"No prediction files found matching pattern {prediction_pattern}\")\n",
    "LATEST_PREDICTION_FILE = prediction_files[0]\n",
    "print(f\"Loading latest prediction file: {LATEST_PREDICTION_FILE}\")\n",
    "predictions = pd.read_csv(LATEST_PREDICTION_FILE)\n",
    "\n",
    "aggregated_pattern = os.path.join(OUTPUT_DIR, f\"aggregated_predictions_*{TEST_SUFFIX}.csv\")\n",
    "aggregated_files = sorted(glob.glob(aggregated_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not aggregated_files:\n",
    "    raise FileNotFoundError(f\"No aggregated files found matching pattern {aggregated_pattern}\")\n",
    "LATEST_AGGREGATED_FILE = aggregated_files[0]\n",
    "print(f\"Loading latest aggregated file: {LATEST_AGGREGATED_FILE}\")\n",
    "aggregated = pd.read_csv(LATEST_AGGREGATED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07978e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_answer_preds = predictions[predictions['prediction'].str.contains(',', na=False)]\n",
    "\n",
    "print(\"Sample of predictions with multiple answers:\")\n",
    "multi_answer_preds[[\"encounter_id\", \"base_qid\", \"image_id\", \"prediction\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTotal multi-answer predictions: {len(multi_answer_preds)}\")\n",
    "\n",
    "print(\"\\nMulti-answer predictions by question type:\")\n",
    "multi_answer_preds['base_qid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932db705",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_result = aggregated[\n",
    "    (aggregated['encounter_id'] == 'ENC00852') & \n",
    "    (aggregated['base_qid'] == 'CQID034')\n",
    "]\n",
    "agg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_counts = predictions[\"prediction\"].value_counts().head(10)\n",
    "print(\"\\nMost common predictions:\")\n",
    "print(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_json = os.path.join(OUTPUT_DIR, \n",
    "                              f\"data_cvqa_sys_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.json\")\n",
    "format_predictions_for_official_eval_with_display(aggregated, predictions_json)\n",
    "print(f\"Formatted predictions saved to {predictions_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_question = val_dataset[(val_dataset['encounter_id'] == 'ENC00853') & \n",
    "                               (val_dataset['base_qid'] == 'CQID012')]\n",
    "\n",
    "print(\"Question Information:\")\n",
    "print(f\"Question text: {specific_question['question_text'].values[0]}\")\n",
    "print(f\"Question type: {specific_question['question_type_en'].values[0]}\")\n",
    "print(f\"Question category: {specific_question['question_category_en'].values[0]}\")\n",
    "print(f\"Options: {specific_question['options_en'].values[0]}\")\n",
    "print(f\"Multi-label: {specific_question['is_multi_label'].values[0]}\")\n",
    "print(\"\\nClinical Context:\")\n",
    "print(f\"Query title: {specific_question['query_title_en'].values[0]}\")\n",
    "print(f\"Query content: {specific_question['query_content_en'].values[0]}\")\n",
    "print(\"\\nImage information:\")\n",
    "print(f\"Image ID: {specific_question['image_id'].values[0]}\")\n",
    "print(f\"Image path: {specific_question['image_path'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539ba32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482db9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.json\")\n",
    "# json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "# if not json_files:\n",
    "#     raise FileNotFoundError(f\"No JSON prediction files found matching pattern {json_pattern}\")\n",
    "    \n",
    "# predictions_file_path = json_files[0]\n",
    "# print(f\"Using most recent prediction JSON: {predictions_file_path}\")\n",
    "\n",
    "json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.json\")\n",
    "json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# If regular pattern fails, try with test_ prefix\n",
    "if not json_files:\n",
    "    test_json_pattern = os.path.join(OUTPUT_DIR, f\"test_data_cvqa_sys_{TEST_INFERENCE_MODEL_ID}_{TEST_INFERENCE_TIMESTAMP}{TEST_SUFFIX}.json\")\n",
    "    json_files = sorted(glob.glob(test_json_pattern), key=os.path.getmtime, reverse=True)\n",
    "    \n",
    "    if not json_files:\n",
    "        # Try a more flexible pattern as last resort\n",
    "        fallback_pattern = os.path.join(OUTPUT_DIR, f\"*data_cvqa_sys_*{TEST_SUFFIX}.json\")\n",
    "        json_files = sorted(glob.glob(fallback_pattern), key=os.path.getmtime, reverse=True)\n",
    "        \n",
    "        if not json_files:\n",
    "            raise FileNotFoundError(f\"No JSON prediction files found with any pattern\")\n",
    "\n",
    "predictions_file_path = json_files[0]\n",
    "print(f\"Using prediction JSON: {predictions_file_path}\")\n",
    "\n",
    "with open(predictions_file_path, 'r') as f:\n",
    "    formatted_preds = json.load(f)\n",
    "    \n",
    "print(\"First 3 prediction entries:\")\n",
    "for i in range(min(3, len(formatted_preds))):\n",
    "    print(f\"\\nPrediction {i+1}:\")\n",
    "    pprint(formatted_preds[i])\n",
    "\n",
    "print(\"\\nLooking for predictions with index 100 (not in options):\")\n",
    "found = False\n",
    "for entry in formatted_preds:\n",
    "    for key, value in entry.items():\n",
    "        if key != 'encounter_id':\n",
    "            if (isinstance(value, list) and 100 in value) or value == 100:\n",
    "                print(f\"\\nFound prediction not in options:\")\n",
    "                print(f\"Encounter: {entry['encounter_id']}\")\n",
    "                print(f\"Question: {key}\")\n",
    "                print(f\"Prediction indices: {value}\")\n",
    "                \n",
    "                aggregated_file_path = os.path.join(OUTPUT_DIR, \n",
    "                                                  f\"aggregated_predictions_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.csv\")\n",
    "                \n",
    "                if not os.path.exists(aggregated_file_path):\n",
    "                    print(f\"Warning: Could not find aggregated predictions file at {aggregated_file_path}\")\n",
    "                    agg_pattern = os.path.join(OUTPUT_DIR, f\"aggregated_predictions_*{TEST_SUFFIX}.csv\")\n",
    "                    agg_files = sorted(glob.glob(agg_pattern), key=os.path.getmtime, reverse=True)\n",
    "                    if agg_files:\n",
    "                        aggregated_file_path = agg_files[0]\n",
    "                        print(f\"Using most recent aggregated file instead: {aggregated_file_path}\")\n",
    "                    else:\n",
    "                        print(\"No aggregated prediction files found. Skipping detailed analysis.\")\n",
    "                        continue\n",
    "                \n",
    "                agg_df = pd.read_csv(aggregated_file_path)\n",
    "                base_qid = key.split('-')[0]\n",
    "                encounter = entry['encounter_id']\n",
    "                match = agg_df[(agg_df['encounter_id'] == encounter) & (agg_df['base_qid'] == base_qid)]\n",
    "                if not match.empty:\n",
    "                    print(f\"Original prediction text: {match['combined_prediction'].values[0]}\")\n",
    "                    print(f\"Available options: {match['options_en'].values[0]}\")\n",
    "                found = True\n",
    "                break\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print(\"No predictions with index 100 found in the first few entries.\")\n",
    "\n",
    "question_counts = {}\n",
    "for entry in formatted_preds:\n",
    "    qid_count = len(entry) - 1\n",
    "    if qid_count in question_counts:\n",
    "        question_counts[qid_count] += 1\n",
    "    else:\n",
    "        question_counts[qid_count] = 1\n",
    "\n",
    "print(\"\\nNumber of questions per encounter:\")\n",
    "for count, num_entries in sorted(question_counts.items()):\n",
    "    print(f\"{count} questions: {num_entries} encounters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea12e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Try first with the regular pattern\n",
    "json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{INFERENCE_MODEL_ID}*{TEST_SUFFIX}.json\")\n",
    "json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# If regular pattern fails, try with test_ prefix\n",
    "if not json_files:\n",
    "    test_json_pattern = os.path.join(OUTPUT_DIR, f\"test_data_cvqa_sys_{TEST_INFERENCE_MODEL_ID}*{TEST_SUFFIX}.json\")\n",
    "    json_files = sorted(glob.glob(test_json_pattern), key=os.path.getmtime, reverse=True)\n",
    "    \n",
    "    if not json_files:\n",
    "        # Try a more flexible pattern as last resort\n",
    "        fallback_pattern = os.path.join(OUTPUT_DIR, f\"*data_cvqa_sys_*{TEST_SUFFIX}.json\")\n",
    "        json_files = sorted(glob.glob(fallback_pattern), key=os.path.getmtime, reverse=True)\n",
    "        \n",
    "        if not json_files:\n",
    "            raise FileNotFoundError(f\"No JSON prediction files found with any pattern\")\n",
    "\n",
    "most_recent_json = json_files[0]\n",
    "print(f\"Using most recent prediction JSON: {most_recent_json}\")\n",
    "\n",
    "# Create submission directory and prepare files\n",
    "masks_preds_dir = os.path.join(OUTPUT_DIR, \"masks_preds\")\n",
    "os.makedirs(masks_preds_dir, exist_ok=True)\n",
    "\n",
    "# Use TEST_INFERENCE_MODEL_ID if it exists, otherwise use INFERENCE_MODEL_ID\n",
    "model_id_for_submission = TEST_INFERENCE_MODEL_ID if 'TEST_INFERENCE_MODEL_ID' in globals() else INFERENCE_MODEL_ID\n",
    "submission_dir = os.path.join(OUTPUT_DIR, f\"submission_{model_id_for_submission}_{SUBMISSION_TIMESTAMP}\")\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "dest_json = os.path.join(submission_dir, \"data_cvqa_sys.json\")\n",
    "shutil.copy2(most_recent_json, dest_json)\n",
    "\n",
    "submission_masks_dir = os.path.join(submission_dir, \"masks_preds\")\n",
    "os.makedirs(submission_masks_dir, exist_ok=True)\n",
    "\n",
    "zip_path = os.path.join(OUTPUT_DIR, f\"mysubmission_{model_id_for_submission}_{SUBMISSION_TIMESTAMP}.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    zipf.write(dest_json, arcname=\"data_cvqa_sys.json\")\n",
    "    zipf.write(submission_masks_dir, arcname=\"masks_preds\")\n",
    "\n",
    "print(f\"Submission package created at: {zip_path}\")\n",
    "print(f\"Files included:\")\n",
    "print(f\" - data_cvqa_sys.json (copied from {most_recent_json})\")\n",
    "print(f\" - masks_preds/ (empty directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35843cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMISSION_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{INFERENCE_MODEL_ID}*{TEST_SUFFIX}.json\")\n",
    "# json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "# if not json_files:\n",
    "#     raise FileNotFoundError(f\"No JSON prediction files found matching pattern {json_pattern}\")\n",
    "# most_recent_json = json_files[0]\n",
    "# print(f\"Using most recent prediction JSON: {most_recent_json}\")\n",
    "\n",
    "# masks_preds_dir = os.path.join(OUTPUT_DIR, \"masks_preds\")\n",
    "# os.makedirs(masks_preds_dir, exist_ok=True)\n",
    "\n",
    "# submission_dir = os.path.join(OUTPUT_DIR, f\"submission_{INFERENCE_MODEL_ID}_{SUBMISSION_TIMESTAMP}\")\n",
    "# os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# dest_json = os.path.join(submission_dir, \"data_cvqa_sys.json\")\n",
    "# shutil.copy2(most_recent_json, dest_json)\n",
    "\n",
    "# submission_masks_dir = os.path.join(submission_dir, \"masks_preds\")\n",
    "# os.makedirs(submission_masks_dir, exist_ok=True)\n",
    "\n",
    "# zip_path = os.path.join(OUTPUT_DIR, f\"mysubmission_{INFERENCE_MODEL_ID}_{SUBMISSION_TIMESTAMP}.zip\")\n",
    "# with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "#     zipf.write(dest_json, arcname=\"data_cvqa_sys.json\")\n",
    "    \n",
    "#     zipf.write(submission_masks_dir, arcname=\"masks_preds\")\n",
    "\n",
    "# print(f\"Submission package created at: {zip_path}\")\n",
    "# print(f\"Files included:\")\n",
    "# print(f\" - data_cvqa_sys.json (copied from {most_recent_json})\")\n",
    "# print(f\" - masks_preds/ (empty directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f4bfd",
   "metadata": {},
   "source": [
    "Measuring input context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f882d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_tokens(dataset_dir=None, processor=None, num_samples=None):\n",
    "    \"\"\"\n",
    "    Analyze token counts in the dataset without running training or inference\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to the processed dataset directory\n",
    "        processor: The processor from the model\n",
    "        num_samples: Optional limit on number of samples to process\n",
    "    \"\"\"\n",
    "    if processor is None:\n",
    "        raise ValueError(\"Processor must be provided for tokenization\")\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Directory not found: {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    token_stats = {\n",
    "        \"samples\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    batch_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"batch_\") and f.endswith(\".pkl\") or\n",
    "                          f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No batch files found in {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    total_tokens = 0\n",
    "    max_tokens = 0\n",
    "    min_tokens = float('inf')\n",
    "    sample_count = 0\n",
    "    all_token_counts = []\n",
    "    \n",
    "    for batch_file in tqdm(batch_files, desc=\"Analyzing batches\"):\n",
    "        try:\n",
    "            with open(os.path.join(dataset_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            for sample in tqdm(batch_data, desc=f\"Analyzing {batch_file}\", leave=False):\n",
    "                if not isinstance(sample, dict) or \"query_text\" not in sample:\n",
    "                    print(f\"Warning: Unexpected sample format in {batch_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "                IMPORTANT: \n",
    "                - Respond ONLY with the exact text of the option(s) that apply\n",
    "                - Do not provide any explanations\n",
    "                - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "                - Do not write \"Options:\" or similar prefixes\n",
    "                - Do not write \"Answer:\" or similar prefixes\n",
    "                - Multiple answers should be separated by commas\n",
    "                - If unsure, respond with \"Not mentioned\"\n",
    "                - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "                - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "                \"\"\"\n",
    "                \n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": sample[\"query_text\"]},\n",
    "                            {\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                text = processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "                \n",
    "                tokens = processor.tokenizer.encode(text)\n",
    "                token_count = len(tokens)\n",
    "                all_token_counts.append(token_count)\n",
    "                \n",
    "                total_tokens += token_count\n",
    "                max_tokens = max(max_tokens, token_count)\n",
    "                min_tokens = min(min_tokens, token_count)\n",
    "                \n",
    "                token_stats[\"samples\"].append({\n",
    "                    \"id\": sample.get(\"id\", \"unknown\"),\n",
    "                    \"qid\": sample.get(\"qid\", \"unknown\"),\n",
    "                    \"image\": os.path.basename(sample.get(\"image_path\", \"unknown\")),\n",
    "                    \"token_count\": token_count,\n",
    "                    \"text_length\": len(sample[\"query_text\"])\n",
    "                })\n",
    "                \n",
    "                sample_count += 1\n",
    "                if num_samples and sample_count >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if num_samples and sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if sample_count == 0:\n",
    "        print(\"No samples were successfully processed\")\n",
    "        return None\n",
    "    \n",
    "    token_stats[\"summary\"] = {\n",
    "        \"total_samples\": sample_count,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_tokens_per_sample\": total_tokens / sample_count,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"min_tokens\": min_tokens,\n",
    "        \"median_tokens\": np.median(all_token_counts),\n",
    "        \"percentile_90\": np.percentile(all_token_counts, 90),\n",
    "        \"percentile_99\": np.percentile(all_token_counts, 99)\n",
    "    }\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(dataset_dir)}_token_analysis.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(token_stats, f, indent=2)\n",
    "    \n",
    "    print(\"\\nToken Usage Analysis:\")\n",
    "    print(f\"Total samples analyzed: {sample_count}\")\n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Average tokens per sample: {total_tokens/sample_count:.2f}\")\n",
    "    print(f\"Median tokens per sample: {np.median(all_token_counts):.2f}\")\n",
    "    print(f\"90th percentile: {np.percentile(all_token_counts, 90):.2f}\")\n",
    "    print(f\"99th percentile: {np.percentile(all_token_counts, 99):.2f}\")\n",
    "    print(f\"Max tokens in a sample: {max_tokens}\")\n",
    "    print(f\"Min tokens in a sample: {min_tokens}\")\n",
    "    print(f\"Percentage of 128K context window used (max): {(max_tokens/128000)*100:.2f}%\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    bin_count = min(50, len(set(all_token_counts)))\n",
    "    \n",
    "    n, bins, patches = plt.hist(all_token_counts, bins=bin_count, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    plt.axvline(x=np.mean(all_token_counts), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_token_counts):.1f}')\n",
    "    \n",
    "    plt.axvline(x=np.median(all_token_counts), color='green', linestyle='-', linewidth=2, label=f'Median: {np.median(all_token_counts):.1f}')\n",
    "    \n",
    "    plt.axvline(x=np.percentile(all_token_counts, 90), color='orange', linestyle='-.', linewidth=2, label=f'90th percentile: {np.percentile(all_token_counts, 90):.1f}')\n",
    "    \n",
    "    plt.title(f\"Distribution of Token Counts (n={sample_count})\", fontsize=16)\n",
    "    plt.xlabel(\"Token Count\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Samples\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    stats_text = (\n",
    "        f\"Min: {min_tokens}\\n\"\n",
    "        f\"Max: {max_tokens}\\n\"\n",
    "        f\"Mean: {np.mean(all_token_counts):.1f}\\n\"\n",
    "        f\"Median: {np.median(all_token_counts):.1f}\\n\"\n",
    "        f\"Std Dev: {np.std(all_token_counts):.1f}\\n\"\n",
    "        f\"90th %ile: {np.percentile(all_token_counts, 90):.1f}\\n\"\n",
    "        f\"% of 128K used: {(max_tokens/128000)*100:.2f}%\"\n",
    "    )\n",
    "    \n",
    "    plt.text(0.95, 0.95, stats_text, \n",
    "             transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', \n",
    "             horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(dataset_dir)}_token_distribution.png\")\n",
    "    plt.savefig(plt_path, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return token_stats\n",
    "\n",
    "try:\n",
    "    print(\"Analyzing training data tokens...\")\n",
    "    train_token_stats = analyze_dataset_tokens(PROCESSED_TRAIN_DATA_DIR, processor)\n",
    "    \n",
    "    print(\"\\nAnalyzing validation data tokens...\")\n",
    "    val_token_stats = analyze_dataset_tokens(PROCESSED_VAL_DATA_DIR, processor)\n",
    "    \n",
    "    print(\"\\nToken analysis complete! Files saved to the outputs directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running token analysis: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_other_please_specify(df, column_name='valid_answers'):\n",
    "    \"\"\"\n",
    "    Check if \"other (please specify)\" appears as a ground truth answer\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check (train or validation)\n",
    "        column_name: Column containing the answers (default: 'valid_answers')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics about occurrences\n",
    "    \"\"\"\n",
    "    if df[column_name].dtype == 'object':\n",
    "        \n",
    "        def safe_eval(x):\n",
    "            try:\n",
    "                if isinstance(x, list):\n",
    "                    return x\n",
    "                if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                    return ast.literal_eval(x)\n",
    "                return [x]\n",
    "            except:\n",
    "                return [str(x)]\n",
    "        \n",
    "        valid_answers_lists = df[column_name].apply(safe_eval)\n",
    "    else:\n",
    "        valid_answers_lists = df[column_name]\n",
    "    \n",
    "    matches = []\n",
    "    for idx, answers in enumerate(valid_answers_lists):\n",
    "        for answer in answers:\n",
    "            if isinstance(answer, str) and \"other (please specify)\" in answer.lower():\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"encounter_id\": df.iloc[idx].get('encounter_id', 'unknown'),\n",
    "                    \"base_qid\": df.iloc[idx].get('base_qid', 'unknown'),\n",
    "                    \"answer\": answer,\n",
    "                    \"all_answers\": answers\n",
    "                })\n",
    "    \n",
    "    results = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"match_count\": len(matches),\n",
    "        \"percentage\": (len(matches) / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    \n",
    "    print(f\"Results for {column_name} in dataset with {len(df)} rows:\")\n",
    "    print(f\"Found {len(matches)} occurrences of 'other (please specify)' ({results['percentage']:.2f}%)\")\n",
    "    \n",
    "    if matches:\n",
    "        print(\"\\nSample matches:\")\n",
    "        for i, match in enumerate(matches[:5]):\n",
    "            print(f\"{i+1}. Index {match['index']}, Encounter: {match['encounter_id']}, QID: {match['base_qid']}\")\n",
    "            print(f\"   Answer: {match['answer']}\")\n",
    "            print(f\"   All answers: {match['all_answers']}\")\n",
    "    \n",
    "    return results\n",
    "def check_for_other_please_specify(df, column_name='valid_answers'):\n",
    "    \"\"\"\n",
    "    Check if \"other (please specify)\" appears as a ground truth answer\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check (train or validation)\n",
    "        column_name: Column containing the answers (default: 'valid_answers')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics about occurrences\n",
    "    \"\"\"\n",
    "    if df[column_name].dtype == 'object':\n",
    "        \n",
    "        def safe_eval(x):\n",
    "            try:\n",
    "                if isinstance(x, list):\n",
    "                    return x\n",
    "                if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                    return ast.literal_eval(x)\n",
    "                return [x]\n",
    "            except:\n",
    "                return [str(x)]\n",
    "        \n",
    "        valid_answers_lists = df[column_name].apply(safe_eval)\n",
    "    else:\n",
    "        valid_answers_lists = df[column_name]\n",
    "    \n",
    "    matches = []\n",
    "    for idx, answers in enumerate(valid_answers_lists):\n",
    "        for answer in answers:\n",
    "            if isinstance(answer, str) and \"other (please specify)\" in answer.lower():\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"encounter_id\": df.iloc[idx].get('encounter_id', 'unknown'),\n",
    "                    \"base_qid\": df.iloc[idx].get('base_qid', 'unknown'),\n",
    "                    \"answer\": answer,\n",
    "                    \"all_answers\": answers\n",
    "                })\n",
    "    \n",
    "    results = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"match_count\": len(matches),\n",
    "        \"percentage\": (len(matches) / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    \n",
    "    print(f\"Results for {column_name} in dataset with {len(df)} rows:\")\n",
    "    print(f\"Found {len(matches)} occurrences of 'other (please specify)' ({results['percentage']:.2f}%)\")\n",
    "    \n",
    "    if matches:\n",
    "        print(\"\\nSample matches:\")\n",
    "        for i, match in enumerate(matches[:5]):\n",
    "            print(f\"{i+1}. Index {match['index']}, Encounter: {match['encounter_id']}, QID: {match['base_qid']}\")\n",
    "            print(f\"   Answer: {match['answer']}\")\n",
    "            print(f\"   All answers: {match['all_answers']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"CHECKING TRAINING DATA:\")\n",
    "train_results = check_for_other_please_specify(train_df)\n",
    "\n",
    "print(\"\\nCHECKING VALIDATION DATA:\")\n",
    "val_results = check_for_other_please_specify(val_df)\n",
    "\n",
    "print(\"\\nCHECKING CQID011 IN TRAINING DATA:\")\n",
    "train_cqid011 = train_df[train_df['base_qid'] == 'CQID011']\n",
    "train_cqid011_results = check_for_other_please_specify(train_cqid011)\n",
    "\n",
    "print(\"\\nCHECKING CQID011 IN VALIDATION DATA:\")\n",
    "val_cqid011 = val_df[val_df['base_qid'] == 'CQID011']\n",
    "val_cqid011_results = check_for_other_please_specify(val_cqid011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e98618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCHECKING CQID013 IN VALIDATION DATA:\")\n",
    "val_cqid013 = val_df[val_df['base_qid'] == 'CQID013']\n",
    "val_cqid013_results = check_for_other_please_specify(val_cqid013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20972fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
