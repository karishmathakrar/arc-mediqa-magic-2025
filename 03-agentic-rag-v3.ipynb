{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39808731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import numpy as np\n",
    "import lancedb\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcbb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download NLTK resources\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "#     nltk.data.find('corpora/stopwords')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt')\n",
    "#     nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca5a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /storage/scratch1/2/kthakrar3/mediqa-\n",
      "[nltk_data]     magic-v2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /storage/scratch1/2/kthakrar3/mediqa-\n",
      "[nltk_data]     magic-v2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# nltk_data_dir = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "# os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "# nltk.data.path.append(nltk_data_dir)\n",
    "# nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "# nltk.download('stopwords', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e16aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MODEL_PREDICTIONS_DIR = os.path.join(os.getcwd(), \"outputs\", \"outputs-akshay-2\", \"outputs\")\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n",
    "    VAL_DATASET_PATH = os.path.join(OUTPUT_DIR, \"val_dataset.csv\")\n",
    "    IMAGES_DIR = os.path.join(os.getcwd(), \"2025_dataset\", \"valid\", \"images_valid\")\n",
    "    GEMINI_MODEL = \"gemini-2.5-flash-preview-04-17\"\n",
    "    MAX_REFLECTION_CYCLES = 2\n",
    "    CONFIDENCE_THRESHOLD = 0.8  # Threshold for accepting an answer without reflection\n",
    "    \n",
    "    # RAG-specific configurations\n",
    "    KNOWLEDGE_DB_PATH = os.path.join(os.getcwd(), \"knowledge_db\")\n",
    "    EMBEDDING_MODEL = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "    CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    VECTOR_DIMENSION = 768\n",
    "    TOP_K_SEMANTIC = 7\n",
    "    TOP_K_KEYWORD = 7\n",
    "    TOP_K_HYBRID = 10\n",
    "    TOP_K_RERANK = 5\n",
    "    \n",
    "    # Dataset\n",
    "    DATASET_NAME = \"brucewayne0459/Skin_diseases_and_care\"\n",
    "    \n",
    "    # Question type configurations for RAG\n",
    "    QUESTION_TYPE_RETRIEVAL_CONFIG = {\n",
    "        \"Site Location\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "        \"Lesion Color\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "        \"Size\": {\"use_rag\": False, \"weight\": 0.1},\n",
    "        \"Skin Description\": {\"use_rag\": True, \"weight\": 0.3},\n",
    "        \"Onset\": {\"use_rag\": True, \"weight\": 0.4},\n",
    "        \"Itch\": {\"use_rag\": True, \"weight\": 0.4},\n",
    "        \"Extent\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "        \"Treatment\": {\"use_rag\": True, \"weight\": 0.7},\n",
    "        \"Lesion Evolution\": {\"use_rag\": True, \"weight\": 0.5},\n",
    "        \"Texture\": {\"use_rag\": True, \"weight\": 0.3},\n",
    "        \"Lesion Count\": {\"use_rag\": False, \"weight\": 0.1},\n",
    "        \"Differential\": {\"use_rag\": True, \"weight\": 0.8},\n",
    "        \"Specific Diagnosis\": {\"use_rag\": True, \"weight\": 0.8},\n",
    "    }\n",
    "    \n",
    "    # Default for question types not explicitly listed\n",
    "    DEFAULT_RAG_CONFIG = {\"use_rag\": True, \"weight\": 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1584e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def get_latest_aggregated_files(model_predictions_dir):\n",
    "        \"\"\"Get the latest aggregated prediction files for each model.\"\"\"\n",
    "        pattern = os.path.join(model_predictions_dir, \"aggregated_predictions_*.csv\")\n",
    "        \n",
    "        agg_files = glob.glob(pattern)\n",
    "        \n",
    "        if len(agg_files) == 0:\n",
    "            return []\n",
    "        \n",
    "        latest_files = {}\n",
    "        \n",
    "        for file_path in agg_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(\"_base_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            model_part = parts[0].replace(\"aggregated_predictions_\", \"\")\n",
    "            model_name = model_part\n",
    "            \n",
    "            timestamps = re.findall(r'(\\d+)', parts[1])\n",
    "            if len(timestamps) < 2:\n",
    "                print(f\"Warning: Could not find timestamps in {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            timestamp = int(timestamps[1])\n",
    "            \n",
    "            if model_name not in latest_files or timestamp > latest_files[model_name]['timestamp']:\n",
    "                latest_files[model_name] = {\n",
    "                    'file_path': file_path,\n",
    "                    'timestamp': timestamp\n",
    "                }\n",
    "        \n",
    "        return [info['file_path'] for _, info in latest_files.items()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_model_predictions(model_predictions_dir):\n",
    "        \"\"\"Load all model predictions from aggregated files.\"\"\"\n",
    "        latest_files = DataLoader.get_latest_aggregated_files(model_predictions_dir)\n",
    "        \n",
    "        if not latest_files:\n",
    "            print(\"No aggregated prediction files found. Cannot proceed.\")\n",
    "            return {}\n",
    "        \n",
    "        model_predictions = {}\n",
    "        \n",
    "        for file_path in latest_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(\"_base_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            model_name = parts[0].replace(\"aggregated_predictions_\", \"\")\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                model_predictions[model_name] = df\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        return model_predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def load_validation_dataset(val_dataset_path):\n",
    "        \"\"\"Load the validation dataset.\"\"\"\n",
    "        val_df = pd.read_csv(val_dataset_path)\n",
    "        \n",
    "        val_df = DataLoader.process_validation_dataset(val_df)\n",
    "        \n",
    "        encounter_question_data = defaultdict(lambda: {\n",
    "            'images': [],\n",
    "            'data': None\n",
    "        })\n",
    "        \n",
    "        for _, row in val_df.iterrows():\n",
    "            encounter_id = row['encounter_id']\n",
    "            base_qid = row['base_qid']\n",
    "            key = (encounter_id, base_qid)\n",
    "            \n",
    "            if 'image_path' in row and row['image_path']:\n",
    "                encounter_question_data[key]['images'].append(row['image_path'])\n",
    "            elif 'image_id' in row and row['image_id']:\n",
    "                image_path = os.path.join(Config.IMAGES_DIR, row['image_id'])\n",
    "                encounter_question_data[key]['images'].append(image_path)\n",
    "            \n",
    "            if encounter_question_data[key]['data'] is None:\n",
    "                encounter_question_data[key]['data'] = row.to_dict()\n",
    "        \n",
    "        grouped_data = []\n",
    "        for (encounter_id, base_qid), data in encounter_question_data.items():\n",
    "            entry = data['data'].copy()\n",
    "            entry['all_images'] = data['images']\n",
    "            entry['encounter_id'] = encounter_id\n",
    "            entry['base_qid'] = base_qid\n",
    "            grouped_data.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(grouped_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_convert_options(options_str):\n",
    "        \"\"\"Safely convert a string representation of a list to an actual list.\"\"\"\n",
    "        if not isinstance(options_str, str):\n",
    "            return options_str\n",
    "            \n",
    "        try:\n",
    "            return ast.literal_eval(options_str)\n",
    "        except (SyntaxError, ValueError):\n",
    "            if options_str.startswith('[') and options_str.endswith(']'):\n",
    "                return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "            elif ',' in options_str:\n",
    "                return [opt.strip() for opt in options_str.split(',')]\n",
    "            else:\n",
    "                return [options_str]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_validation_dataset(val_df):\n",
    "        \"\"\"Process and clean the validation dataset.\"\"\"\n",
    "        if 'options_en' in val_df.columns:\n",
    "            val_df['options_en'] = val_df['options_en'].apply(DataLoader.safe_convert_options)\n",
    "            \n",
    "            def clean_options(options):\n",
    "                if not isinstance(options, list):\n",
    "                    return options\n",
    "                    \n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                return cleaned_options\n",
    "                \n",
    "            val_df['options_en_cleaned'] = val_df['options_en'].apply(clean_options)\n",
    "        \n",
    "        if 'question_text' in val_df.columns:\n",
    "            val_df['question_text_cleaned'] = val_df['question_text'].apply(\n",
    "                lambda q: q.replace(\" Please specify which affected area for each selection.\", \"\") \n",
    "                          if isinstance(q, str) and \"Please specify which affected area for each selection\" in q \n",
    "                          else q\n",
    "            )\n",
    "            \n",
    "            val_df['question_text_cleaned'] = val_df['question_text_cleaned'].apply(\n",
    "                lambda q: re.sub(r'^\\d+\\s+', '', q) if isinstance(q, str) else q\n",
    "            )\n",
    "        \n",
    "        if 'base_qid' not in val_df.columns and 'qid' in val_df.columns:\n",
    "            val_df['base_qid'] = val_df['qid'].apply(\n",
    "                lambda q: q.split('-')[0] if isinstance(q, str) and '-' in q else q\n",
    "            )\n",
    "        \n",
    "        return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f144174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def create_query_context(row):\n",
    "        \"\"\"Create query context from validation data similar to the inference process.\"\"\"\n",
    "        question = row.get('question_text_cleaned', row.get('question_text', 'What do you see in this image?'))\n",
    "        \n",
    "        metadata = \"\"\n",
    "        if 'question_type_en' in row:\n",
    "            metadata += f\"Type: {row['question_type_en']}\"\n",
    "            \n",
    "        if 'question_category_en' in row:\n",
    "            metadata += f\", Category: {row['question_category_en']}\"\n",
    "        \n",
    "        query_title = row.get('query_title_en', '')\n",
    "        query_content = row.get('query_content_en', '')\n",
    "        \n",
    "        clinical_context = \"\"\n",
    "        if query_title or query_content:\n",
    "            clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "            if query_title:\n",
    "                clinical_context += f\"{query_title}\\n\"\n",
    "            if query_content:\n",
    "                clinical_context += f\"{query_content}\\n\"\n",
    "        \n",
    "        options = row.get('options_en_cleaned', row.get('options_en', ['Yes', 'No', 'Not mentioned']))\n",
    "        if isinstance(options, list):\n",
    "            options_text = \", \".join(options)\n",
    "        else:\n",
    "            options_text = str(options)\n",
    "        \n",
    "        query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                     f\"Question Metadata: {metadata}\\n\"\n",
    "                     f\"{clinical_context}\"\n",
    "                     f\"Available Options (choose from these): {options_text}\")\n",
    "        \n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f7d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGData:\n",
    "    def __init__(self, all_models_df, validation_df):\n",
    "        self.all_models_df = all_models_df\n",
    "        self.validation_df = validation_df\n",
    "        \n",
    "        self.model_predictions = {}\n",
    "        for (encounter_id, base_qid), group in all_models_df.groupby(['encounter_id', 'base_qid']):\n",
    "            self.model_predictions[(encounter_id, base_qid)] = group\n",
    "        \n",
    "        self.validation_data = {}\n",
    "        for _, row in validation_df.iterrows():\n",
    "            self.validation_data[(row['encounter_id'], row['base_qid'])] = row\n",
    "    \n",
    "    def get_combined_data(self, encounter_id, base_qid):\n",
    "        \"\"\"Retrieve combined data for a specific encounter and question.\"\"\"\n",
    "        model_preds = self.model_predictions.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        val_data = self.validation_data.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        if model_preds is None:\n",
    "            print(f\"No model predictions found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "            \n",
    "        if val_data is None:\n",
    "            print(f\"No validation data found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "        \n",
    "        if 'query_context' not in val_data:\n",
    "            val_data['query_context'] = DataProcessor.create_query_context(val_data)\n",
    "        \n",
    "        model_predictions_dict = {}\n",
    "        for _, row in model_preds.iterrows():\n",
    "            model_name = row['model_name']\n",
    "            \n",
    "            model_predictions_dict[model_name] = self._process_model_predictions(row)\n",
    "        \n",
    "        return {\n",
    "            'encounter_id': encounter_id,\n",
    "            'base_qid': base_qid,\n",
    "            'query_context': val_data['query_context'],\n",
    "            'images': val_data.get('all_images', []),\n",
    "            'options': val_data.get('options_en_cleaned', val_data.get('options_en', [])),\n",
    "            'question_type': val_data.get('question_type_en', ''),\n",
    "            'question_category': val_data.get('question_category_en', ''),\n",
    "            'model_predictions': model_predictions_dict\n",
    "        }\n",
    "    \n",
    "    def _process_model_predictions(self, row):\n",
    "        \"\"\"Process model predictions from row data.\"\"\"\n",
    "        return {\n",
    "            'model_prediction': row.get('combined_prediction', '')\n",
    "        }\n",
    "    \n",
    "    def get_all_encounter_question_pairs(self):\n",
    "        \"\"\"Return a list of all unique encounter_id, base_qid pairs.\"\"\"\n",
    "        return list(self.validation_data.keys())\n",
    "    \n",
    "    def get_sample_data(self, n=5):\n",
    "        \"\"\"Get a sample of combined data for n random encounter-question pairs.\"\"\"\n",
    "        import random\n",
    "        \n",
    "        all_pairs = self.get_all_encounter_question_pairs()\n",
    "        sample_pairs = random.sample(all_pairs, min(n, len(all_pairs)))\n",
    "        \n",
    "        return [self.get_combined_data(encounter_id, base_qid) for encounter_id, base_qid in sample_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(text):\n",
    "    \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "    cleaned_text = text\n",
    "    if \"```json\" in cleaned_text:\n",
    "        cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "    if \"```\" in cleaned_text:\n",
    "        cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not parse as JSON\")\n",
    "        return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227ce186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBaseManager:\n",
    "    \"\"\"Manages the dermatology knowledge base for RAG.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the knowledge base manager.\"\"\"\n",
    "        # Ensure NLTK resources are downloaded\n",
    "        for resource in ['punkt', 'stopwords']:\n",
    "            try:\n",
    "                nltk.data.find(f'tokenizers/{resource}')\n",
    "            except LookupError:\n",
    "                nltk.download(resource)\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)\n",
    "    #     Rest of your initialization code...\n",
    "    #     def __init__(self):\n",
    "    #         \"\"\"Initialize the knowledge base manager.\"\"\"\n",
    "    #         self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)\n",
    "        self.cross_encoder = CrossEncoder(Config.CROSS_ENCODER_MODEL)\n",
    "\n",
    "        # Initialize LanceDB\n",
    "        self.db_path = Config.KNOWLEDGE_DB_PATH\n",
    "        os.makedirs(self.db_path, exist_ok=True)\n",
    "        self.db = lancedb.connect(self.db_path)\n",
    "\n",
    "        # Check if the table exists, create it if not\n",
    "        self.table_name = \"dermatology_knowledge\"\n",
    "\n",
    "        if self.table_name not in self.db.table_names():\n",
    "            print(f\"Knowledge base not found. Creating new knowledge base at {self.db_path}\")\n",
    "            self._initialize_knowledge_base()\n",
    "        else:\n",
    "            print(f\"Using existing knowledge base at {self.db_path}\")\n",
    "            self.table = self.db.open_table(self.table_name)\n",
    "\n",
    "        # BM25 index for keyword search\n",
    "        self.tokenized_corpus = []\n",
    "        self.doc_ids = []\n",
    "        self._initialize_bm25_index()\n",
    "    \n",
    "    def _initialize_knowledge_base(self):\n",
    "        \"\"\"Initialize the knowledge base with the skin diseases dataset.\"\"\"\n",
    "        print(\"Loading dermatology dataset...\")\n",
    "        dataset = load_dataset(Config.DATASET_NAME)\n",
    "\n",
    "        # Prepare data for LanceDB\n",
    "        data = []\n",
    "\n",
    "        print(\"Processing dataset and creating embeddings...\")\n",
    "        for i, item in enumerate(dataset['train']):\n",
    "            topic = item['Topic']\n",
    "            information = item['Information']\n",
    "\n",
    "            # Create document\n",
    "            combined_text = f\"Topic: {topic}\\n\\nInformation: {information}\"\n",
    "\n",
    "            # Create embedding\n",
    "            embedding = self.embedding_model.encode(combined_text)\n",
    "\n",
    "            # Add to data\n",
    "            data.append({\n",
    "                \"id\": i,\n",
    "                \"topic\": topic,\n",
    "                \"information\": information,\n",
    "                \"combined_text\": combined_text,\n",
    "                \"vector\": embedding.tolist()\n",
    "            })\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1} documents\")\n",
    "\n",
    "        # Convert the data to a pandas DataFrame\n",
    "        import pandas as pd\n",
    "        data_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create LanceDB table\n",
    "        print(\"Creating vector database...\")\n",
    "        self.table = self.db.create_table(\n",
    "            self.table_name,\n",
    "            data=data_df  # Use pandas DataFrame instead of list of dicts\n",
    "        )\n",
    "        print(\"Knowledge base initialization complete.\")\n",
    "    \n",
    "#     def _initialize_bm25_index(self):\n",
    "#         \"\"\"Initialize the BM25 index for keyword search.\"\"\"\n",
    "#         print(\"Initializing BM25 index...\")\n",
    "        \n",
    "#         # Query all documents from LanceDB\n",
    "#         results = self.table.search().limit(10000).to_pandas()\n",
    "        \n",
    "#         # Tokenize documents for BM25\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "#         for idx, row in results.iterrows():\n",
    "#             doc_text = row['combined_text']\n",
    "#             self.doc_ids.append(row['id'])\n",
    "            \n",
    "#             # Tokenize and remove stopwords\n",
    "#             tokens = word_tokenize(doc_text.lower())\n",
    "#             tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "            \n",
    "#             self.tokenized_corpus.append(tokens)\n",
    "        \n",
    "#         # Create BM25 index\n",
    "#         self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "#         print(\"BM25 index initialization complete.\")\n",
    "\n",
    "#     def _initialize_bm25_index(self):\n",
    "#         \"\"\"Initialize the BM25 index for keyword search.\"\"\"\n",
    "#         print(\"Initializing BM25 index...\")\n",
    "\n",
    "#         # Query all documents from LanceDB\n",
    "#         results = self.table.search().limit(10000).to_pandas()\n",
    "\n",
    "#         # Instead of using NLTK's word_tokenize, use a simple split function\n",
    "#         stop_words = set(stopwords.words('english')) if 'stopwords' in nltk.data.path else set()\n",
    "\n",
    "#         for idx, row in results.iterrows():\n",
    "#             doc_text = row['combined_text']\n",
    "#             self.doc_ids.append(row['id'])\n",
    "\n",
    "#             # Simple tokenization\n",
    "#             tokens = doc_text.lower().split()\n",
    "#             # Remove stopwords and keep only alphanumeric tokens\n",
    "#             tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "#             self.tokenized_corpus.append(tokens)\n",
    "\n",
    "#         # Create BM25 index\n",
    "#         self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "#         print(\"BM25 index initialization complete.\")\n",
    "        \n",
    "    def _initialize_bm25_index(self):\n",
    "        \"\"\"Initialize the BM25 index for keyword search without NLTK dependencies.\"\"\"\n",
    "        print(\"Initializing BM25 index...\")\n",
    "\n",
    "        # Query all documents from LanceDB\n",
    "        results = self.table.search().limit(10000).to_pandas()\n",
    "\n",
    "        # Common English stopwords - hardcoded to avoid NLTK dependency\n",
    "        common_stopwords = {\n",
    "            \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \n",
    "            \"by\", \"about\", \"from\", \"as\", \"of\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "            \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"can\", \"could\", \"will\",\n",
    "            \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\", \"this\", \"that\", \"these\",\n",
    "            \"those\", \"it\", \"its\", \"they\", \"them\", \"their\", \"he\", \"him\", \"his\", \"she\", \"her\"\n",
    "        }\n",
    "\n",
    "        for idx, row in results.iterrows():\n",
    "            doc_text = row['combined_text']\n",
    "            self.doc_ids.append(row['id'])\n",
    "\n",
    "            # Simple tokenization without NLTK\n",
    "            # Split by whitespace and remove punctuation\n",
    "            tokens = []\n",
    "            for token in doc_text.lower().split():\n",
    "                # Remove punctuation\n",
    "                token = ''.join(c for c in token if c.isalnum())\n",
    "                if token and token not in common_stopwords:\n",
    "                    tokens.append(token)\n",
    "\n",
    "            self.tokenized_corpus.append(tokens)\n",
    "\n",
    "        # Create BM25 index\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        print(\"BM25 index initialization complete.\")\n",
    "    \n",
    "    def semantic_search(self, query, top_k=None):\n",
    "        \"\"\"Perform semantic search using embeddings.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = Config.TOP_K_SEMANTIC\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        \n",
    "        # Search LanceDB\n",
    "        results = self.table.search(query_embedding.tolist()).limit(top_k).to_pandas()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def keyword_search(self, query, top_k=None):\n",
    "        \"\"\"Perform keyword search using BM25.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = Config.TOP_K_KEYWORD\n",
    "\n",
    "        # Simple tokenization without NLTK\n",
    "        # Split by whitespace and remove punctuation\n",
    "        common_stopwords = {\n",
    "            \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \n",
    "            \"by\", \"about\", \"from\", \"as\", \"of\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "            \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"can\", \"could\", \"will\",\n",
    "            \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\", \"this\", \"that\", \"these\",\n",
    "            \"those\", \"it\", \"its\", \"they\", \"them\", \"their\", \"he\", \"him\", \"his\", \"she\", \"her\"\n",
    "        }\n",
    "\n",
    "        query_tokens = []\n",
    "        for token in query.lower().split():\n",
    "            # Remove punctuation\n",
    "            token = ''.join(c for c in token if c.isalnum())\n",
    "            if token and token not in common_stopwords:\n",
    "                query_tokens.append(token)\n",
    "\n",
    "        # Get BM25 scores\n",
    "        doc_scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(doc_scores)[::-1][:top_k]\n",
    "        \n",
    "        # Convert to document IDs and scores\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if doc_scores[idx] > 0:  # Only include if score is positive\n",
    "                doc_id = self.doc_ids[idx]\n",
    "                score = doc_scores[idx]\n",
    "                \n",
    "                # Get document from LanceDB\n",
    "                doc = self.table.search().where(f\"id = {doc_id}\").limit(1).to_pandas()\n",
    "                \n",
    "                if not doc.empty:\n",
    "                    results.append({\n",
    "                        \"id\": doc_id,\n",
    "                        \"topic\": doc['topic'].iloc[0],\n",
    "                        \"information\": doc['information'].iloc[0],\n",
    "                        \"combined_text\": doc['combined_text'].iloc[0],\n",
    "                        \"_distance\": 1.0 - min(score / 10.0, 1.0)  # Convert to distance metric (0 to 1)\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def hybrid_search(self, query, top_k=None):\n",
    "        \"\"\"Perform hybrid search combining semantic and keyword search.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = Config.TOP_K_HYBRID\n",
    "        \n",
    "        # Perform both search types\n",
    "        semantic_results = self.semantic_search(query, top_k=top_k)\n",
    "        keyword_results = self.keyword_search(query, top_k=top_k)\n",
    "        \n",
    "        # Merge results and remove duplicates\n",
    "        combined_results = pd.concat([semantic_results, keyword_results])\n",
    "        combined_results = combined_results.drop_duplicates(subset=['id'])\n",
    "        \n",
    "        # Rerank the top results\n",
    "        if len(combined_results) > 0:\n",
    "            return self.rerank_results(combined_results, query, top_k=min(top_k, len(combined_results)))\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def rerank_results(self, results, query, top_k=None):\n",
    "        \"\"\"Rerank search results using a cross-encoder.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = Config.TOP_K_RERANK\n",
    "        \n",
    "        if len(results) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Prepare input for cross-encoder\n",
    "        pairs = [(query, doc) for doc in results['combined_text'].tolist()]\n",
    "        \n",
    "        # Get scores from cross-encoder\n",
    "        cross_scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Add scores to results\n",
    "        results = results.copy()\n",
    "        results['cross_score'] = cross_scores\n",
    "        \n",
    "        # Sort by cross-encoder score\n",
    "        results = results.sort_values(by='cross_score', ascending=False).head(top_k)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2009dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator:\n",
    "    \"\"\"Generates search queries from evidence.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        \"\"\"Initialize the query generator.\"\"\"\n",
    "        self.client = client\n",
    "    \n",
    "    def generate_queries(self, question_text, question_type, options, integrated_evidence, num_queries=3):\n",
    "        \"\"\"\n",
    "        Generate search queries based on integrated evidence.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: Type of question being asked\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            num_queries: Number of queries to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of search queries\n",
    "        \"\"\"\n",
    "        prompt = self._create_query_generation_prompt(\n",
    "            question_text, \n",
    "            question_type, \n",
    "            options, \n",
    "            integrated_evidence,\n",
    "            num_queries\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            response_text = response.text\n",
    "            parsed_response = parse_json_response(response_text)\n",
    "            \n",
    "            if \"queries\" in parsed_response:\n",
    "                return parsed_response[\"queries\"]\n",
    "            else:\n",
    "                # Fallback: extract queries using text processing if JSON parsing fails\n",
    "                queries = self._extract_queries_from_text(response_text)\n",
    "                if queries:\n",
    "                    return queries\n",
    "                \n",
    "                # Second fallback: generate simple queries\n",
    "                return self._generate_fallback_queries(question_text, question_type, integrated_evidence)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating queries: {str(e)}\")\n",
    "            return self._generate_fallback_queries(question_text, question_type, integrated_evidence)\n",
    "    \n",
    "    def _create_query_generation_prompt(self, question_text, question_type, options, integrated_evidence, num_queries):\n",
    "        \"\"\"Create prompt for generating search queries.\"\"\"\n",
    "        return f\"\"\"As a dermatology specialist, generate specific search queries to retrieve relevant medical knowledge for this dermatological question.\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "Your task is to:\n",
    "1. Analyze the evidence from visual findings and clinical context\n",
    "2. Identify key dermatological features, terms, and potential conditions\n",
    "3. Create {num_queries} specific, targeted search queries that would retrieve relevant medical knowledge about:\n",
    "   - Conditions matching these symptoms and signs\n",
    "   - Diagnostic criteria related to the question\n",
    "   - Relevant medical information needed to answer the question accurately\n",
    "\n",
    "Format your response as a JSON object with this field:\n",
    "\"queries\": [\n",
    "  \"First search query\",\n",
    "  \"Second search query\",\n",
    "  \"Third search query\"\n",
    "]\n",
    "\n",
    "Guidelines for effective queries:\n",
    "- Use precise medical terminology and descriptive terms\n",
    "- Include key visual features and clinical symptoms\n",
    "- Be specific to dermatological conditions\n",
    "- Each query should be 3-10 words, focused and concise\n",
    "- Queries should be diverse to retrieve complementary information\n",
    "\n",
    "Example of good queries for a skin rash:\n",
    "- \"erythematous maculopapular rash differential diagnosis\"\n",
    "- \"pruritic vesicular eruption trunk treatment options\"\n",
    "- \"chronic scaling plaques psoriasis vs eczema\"\n",
    "\"\"\"\n",
    "\n",
    "    def _extract_queries_from_text(self, text):\n",
    "        \"\"\"Extract queries from text response if JSON parsing fails.\"\"\"\n",
    "        queries = []\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            # Look for patterns like \"1. query\" or \"- query\"\n",
    "            if (line.startswith('-') or \n",
    "                line.startswith('*') or \n",
    "                re.match(r'^\\d+\\.', line)):\n",
    "                query = re.sub(r'^[-*\\d\\.\\s\"\\']+', '', line).strip()\n",
    "                if query and len(query.split()) <= 15:  # Reasonable length for a query\n",
    "                    queries.append(query)\n",
    "                    \n",
    "        return queries if queries else None\n",
    "    \n",
    "    def _generate_fallback_queries(self, question_text, question_type, integrated_evidence):\n",
    "        \"\"\"Generate simple queries as fallback.\"\"\"\n",
    "        queries = []\n",
    "        \n",
    "        # Add question-based query\n",
    "        queries.append(question_text.strip())\n",
    "        \n",
    "        # Extract from integrated evidence\n",
    "        if \"INTEGRATED_FINDINGS\" in integrated_evidence:\n",
    "            findings = integrated_evidence[\"INTEGRATED_FINDINGS\"]\n",
    "            \n",
    "            # Extract skin description\n",
    "            if \"SKIN_DESCRIPTION\" in findings:\n",
    "                desc = findings[\"SKIN_DESCRIPTION\"]\n",
    "                if isinstance(desc, str) and len(desc) > 10:\n",
    "                    queries.append(f\"skin condition {desc[:50]}\")\n",
    "            \n",
    "            # Extract lesion color\n",
    "            if \"LESION_COLOR\" in findings:\n",
    "                color = findings[\"LESION_COLOR\"]\n",
    "                if isinstance(color, str) and len(color) > 5:\n",
    "                    queries.append(f\"dermatology {color[:30]} skin condition\")\n",
    "            \n",
    "            # Extract overall impression\n",
    "            if \"OVERALL_IMPRESSION\" in findings:\n",
    "                impression = findings[\"OVERALL_IMPRESSION\"]\n",
    "                if isinstance(impression, str) and len(impression) > 10:\n",
    "                    queries.append(f\"dermatological diagnosis {impression[:50]}\")\n",
    "        \n",
    "        # Ensure we have at least one query\n",
    "        if not queries:\n",
    "            queries.append(f\"dermatology {question_type.lower()} diagnosis\")\n",
    "        \n",
    "        return queries[:3]  # Return up to 3 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b925e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DermatologyKnowledgeRetriever:\n",
    "    \"\"\"Retrieves knowledge from the dermatology knowledge base.\"\"\"\n",
    "    \n",
    "    def __init__(self, kb_manager, query_generator):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge retriever.\n",
    "        \n",
    "        Args:\n",
    "            kb_manager: KnowledgeBaseManager instance\n",
    "            query_generator: QueryGenerator instance\n",
    "        \"\"\"\n",
    "        self.kb_manager = kb_manager\n",
    "        self.query_generator = query_generator\n",
    "    \n",
    "    def retrieve_knowledge(self, question_text, question_type, options, integrated_evidence):\n",
    "        \"\"\"\n",
    "        Retrieve relevant knowledge for a dermatological question.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: Type of question being asked\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with retrieved knowledge\n",
    "        \"\"\"\n",
    "        # Check if we should use RAG for this question type\n",
    "        rag_config = Config.QUESTION_TYPE_RETRIEVAL_CONFIG.get(\n",
    "            question_type, Config.DEFAULT_RAG_CONFIG\n",
    "        )\n",
    "        \n",
    "        if not rag_config[\"use_rag\"]:\n",
    "            return {\n",
    "                \"retrieved\": False,\n",
    "                \"reason\": f\"RAG not enabled for question type: {question_type}\",\n",
    "                \"results\": []\n",
    "            }\n",
    "        \n",
    "        # Generate search queries\n",
    "        queries = self.query_generator.generate_queries(\n",
    "            question_text, question_type, options, integrated_evidence\n",
    "        )\n",
    "        \n",
    "        if not queries:\n",
    "            return {\n",
    "                \"retrieved\": False,\n",
    "                \"reason\": \"Failed to generate search queries\",\n",
    "                \"results\": []\n",
    "            }\n",
    "        \n",
    "        # Retrieve results for each query\n",
    "        all_results = []\n",
    "        \n",
    "        for query in queries:\n",
    "            results = self.kb_manager.hybrid_search(query)\n",
    "            \n",
    "            if not results.empty:\n",
    "                # Convert to list of dictionaries for easier handling\n",
    "                for _, row in results.iterrows():\n",
    "                    all_results.append({\n",
    "                        \"query\": query,\n",
    "                        \"topic\": row['topic'],\n",
    "                        \"information\": row['information'],\n",
    "                        \"relevance_score\": float(row.get('cross_score', 0.0))\n",
    "                    })\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_results = []\n",
    "        seen_topics = set()\n",
    "        \n",
    "        for result in sorted(all_results, key=lambda x: x['relevance_score'], reverse=True):\n",
    "            if result['topic'] not in seen_topics:\n",
    "                unique_results.append(result)\n",
    "                seen_topics.add(result['topic'])\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"retrieved\": len(unique_results) > 0,\n",
    "            \"queries\": queries,\n",
    "            \"results\": unique_results[:Config.TOP_K_RERANK]  # Limit to top-k unique results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779ca6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAnalysisService:\n",
    "    \"\"\"Service for analyzing dermatological images.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def analyze_images(self, image_paths, encounter_id):\n",
    "        \"\"\"\n",
    "        Analyze multiple dermatological images for an encounter.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of paths to images\n",
    "            encounter_id: Encounter identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with individual and aggregated analyses\n",
    "        \"\"\"\n",
    "        image_analyses = []\n",
    "        \n",
    "        structured_prompt = self._create_dermatology_prompt()\n",
    "        \n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            analysis = self._analyze_single_image(\n",
    "                img_path, \n",
    "                structured_prompt, \n",
    "                encounter_id, \n",
    "                idx, \n",
    "                len(image_paths)\n",
    "            )\n",
    "            image_analyses.append(analysis)\n",
    "        \n",
    "        aggregated_analysis = self._aggregate_analyses(image_analyses, encounter_id)\n",
    "        \n",
    "        return {\n",
    "            \"encounter_id\": encounter_id,\n",
    "            \"image_count\": len(image_paths),\n",
    "            \"individual_analyses\": image_analyses,\n",
    "            \"aggregated_analysis\": aggregated_analysis\n",
    "        }\n",
    "    \n",
    "    def _create_dermatology_prompt(self):\n",
    "        \"\"\"Create the structured dermatology analysis prompt.\"\"\"\n",
    "        return \"\"\"As dermatology specialist analyzing skin images, extract and structure all clinically relevant information from this dermatological image.\n",
    "\n",
    "Organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is cannot be determined, state \"Cannot determine from image\".\n",
    "\"\"\"\n",
    "    \n",
    "    def _analyze_single_image(self, img_path, prompt, encounter_id, idx, total_images):\n",
    "        \"\"\"Analyze a single dermatological image.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            print(f\"Analyzing image {idx+1}/{total_images} for encounter {encounter_id}\")\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt, image]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "            \n",
    "            structured_analysis = parse_json_response(analysis_text)\n",
    "            \n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"structured_analysis\": structured_analysis\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image {img_path}: {str(e)}\")\n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _aggregate_analyses(self, image_analyses, encounter_id):\n",
    "        \"\"\"Aggregate structured analyses from multiple images.\"\"\"\n",
    "        valid_analyses = [a for a in image_analyses if \"error\" not in a and \"structured_analysis\" in a]\n",
    "        \n",
    "        if not valid_analyses:\n",
    "            return {\n",
    "                \"error\": \"No valid analyses to aggregate\",\n",
    "                \"message\": \"Unable to generate aggregated analysis due to errors in individual analyses.\"\n",
    "            }\n",
    "        \n",
    "        if len(valid_analyses) == 1:\n",
    "            return valid_analyses[0][\"structured_analysis\"]\n",
    "        \n",
    "        analysis_jsons = []\n",
    "        for analysis in valid_analyses:\n",
    "            analysis_json = json.dumps(analysis[\"structured_analysis\"])\n",
    "            analysis_jsons.append(f\"Image {analysis['image_index']} ({analysis['image_path']}): {analysis_json}\")\n",
    "        \n",
    "        aggregation_prompt = self._create_aggregation_prompt(analysis_jsons)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[aggregation_prompt]\n",
    "            )\n",
    "            \n",
    "            aggregation_text = response.text\n",
    "            \n",
    "            aggregated_analysis = parse_json_response(aggregation_text)\n",
    "            \n",
    "            return aggregated_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating aggregated analysis for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"aggregation_error\": \"Failed to generate aggregated analysis\"\n",
    "            }\n",
    "    \n",
    "    def _create_aggregation_prompt(self, analysis_jsons):\n",
    "        \"\"\"Create a prompt for aggregating multiple image analyses.\"\"\"\n",
    "        return f\"\"\"As dermatology specialist reviewing multiple skin image analyses for the same patient, combine these analyses and organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "    \n",
    "{' '.join(analysis_jsons)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38e4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalContextAnalyzer:\n",
    "    \"\"\"Service for analyzing clinical context.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def extract_clinical_context(self, query_context, encounter_id):\n",
    "        \"\"\"\n",
    "        Extract structured clinical information from an encounter's query context.\n",
    "        \n",
    "        Args:\n",
    "            query_context: The query context text\n",
    "            encounter_id: Encounter identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured clinical information\n",
    "        \"\"\"\n",
    "        clinical_text = self._extract_clinical_text(query_context)\n",
    "        \n",
    "        if not clinical_text:\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"clinical_summary\": \"No clinical information available\"\n",
    "            }\n",
    "        \n",
    "        prompt = self._create_clinical_context_prompt(clinical_text)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            structured_context = parse_json_response(response.text)\n",
    "            \n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"structured_clinical_context\": structured_context\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting clinical context for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_clinical_text(self, query_context):\n",
    "        \"\"\"Extract clinical text from query context.\"\"\"\n",
    "        clinical_lines = []\n",
    "        capturing = False\n",
    "        for line in query_context.split('\\n'):\n",
    "            if \"Background Clinical Information\" in line:\n",
    "                capturing = True\n",
    "                continue\n",
    "            elif \"Available Options\" in line:\n",
    "                capturing = False\n",
    "            elif capturing:\n",
    "                clinical_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(clinical_lines).strip()\n",
    "    \n",
    "    def _create_clinical_context_prompt(self, clinical_text):\n",
    "        \"\"\"Create prompt for extracting structured clinical information.\"\"\"\n",
    "        return f\"\"\"You are a dermatology specialist analyzing patient information. \n",
    "Extract and structure all clinically relevant information from this patient description:\n",
    "\n",
    "{clinical_text}\n",
    "\n",
    "Organize your response in the following JSON structure:\n",
    "\n",
    "1. DEMOGRAPHICS: Age, sex, and any other demographic data\n",
    "2. SITE_LOCATION: Body parts affected by the condition as described in the text\n",
    "3. SKIN_DESCRIPTION: Any mention of lesion morphology (flat, raised, depressed), texture, surface characteristics (scales, crust, fluid), appearance of lesion boundaries\n",
    "4. LESION_COLOR: Any description of color(s) of affected areas, color variations, comparison to normal skin\n",
    "5. LESION_COUNT: Any information about number of lesions, single vs multiple presentation, distribution pattern\n",
    "6. EXTENT: How widespread the condition appears based on the description, localized vs widespread\n",
    "7. TEXTURE: Any description of tactile qualities, smooth vs rough, notable textural features\n",
    "8. ONSET_INDICATORS: Information about onset, duration, progression, or evolution of symptoms\n",
    "9. ITCH_INDICATORS: Mentions of scratching, itchiness, or other sensory symptoms\n",
    "10. OTHER_SYMPTOMS: Any additional symptoms mentioned (pain, burning, etc.)\n",
    "11. TRIGGERS: Identified factors that worsen/improve the condition\n",
    "12. HISTORY: Relevant past medical history or previous treatments\n",
    "13. DIAGNOSTIC_CONSIDERATIONS: Any mentioned or suggested diagnoses in the text\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is \n",
    "not available, indicate \"Not mentioned\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66d1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceIntegrator:\n",
    "    \"\"\"Integrates visual, clinical, and knowledge-based evidence.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def integrate_evidence(self, image_analysis, clinical_context, question_type, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Integrate image analysis with clinical context and retrieved knowledge.\n",
    "        \n",
    "        Args:\n",
    "            image_analysis: Structured image analysis\n",
    "            clinical_context: Structured clinical context\n",
    "            question_type: Type of question being asked\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with integrated evidence\n",
    "        \"\"\"\n",
    "        # Determine weighting based on question type\n",
    "        weights = self._get_weights_for_question(question_type)\n",
    "        \n",
    "        # Create prompt for integration\n",
    "        prompt = self._create_integration_prompt(\n",
    "            image_analysis,\n",
    "            clinical_context,\n",
    "            question_type,\n",
    "            weights,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            integration_text = response.text\n",
    "            \n",
    "            integrated_evidence = parse_json_response(integration_text)\n",
    "            \n",
    "            return integrated_evidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error integrating evidence: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"message\": \"Failed to integrate evidence\"\n",
    "            }\n",
    "    \n",
    "    def _get_weights_for_question(self, question_type):\n",
    "        \"\"\"\n",
    "        Determine evidence weighting based on question type.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with weights for each evidence type\n",
    "        \"\"\"\n",
    "        weights = {\n",
    "            \"Site Location\": {\"image\": 0.8, \"clinical\": 0.2, \"knowledge\": 0.0},\n",
    "            \"Lesion Color\": {\"image\": 0.9, \"clinical\": 0.1, \"knowledge\": 0.0},\n",
    "            \"Size\": {\"image\": 0.8, \"clinical\": 0.2, \"knowledge\": 0.0},\n",
    "            \"Skin Description\": {\"image\": 0.7, \"clinical\": 0.3, \"knowledge\": 0.2},\n",
    "            \"Duration of Symptoms\": {\"image\": 0.3, \"clinical\": 0.7, \"knowledge\": 0.2},\n",
    "            \"Itch\": {\"image\": 0.4, \"clinical\": 0.6, \"knowledge\": 0.3},\n",
    "            \"Extent\": {\"image\": 0.7, \"clinical\": 0.3, \"knowledge\": 0.1},\n",
    "            \"Treatment\": {\"image\": 0.1, \"clinical\": 0.9, \"knowledge\": 0.7},\n",
    "            \"Lesion Evolution\": {\"image\": 0.3, \"clinical\": 0.7, \"knowledge\": 0.4},\n",
    "            \"Texture\": {\"image\": 0.6, \"clinical\": 0.4, \"knowledge\": 0.2},\n",
    "            \"Specific Diagnosis\": {\"image\": 0.5, \"clinical\": 0.5, \"knowledge\": 0.8},\n",
    "            \"Count\": {\"image\": 0.8, \"clinical\": 0.2, \"knowledge\": 0.0},\n",
    "            \"Differential\": {\"image\": 0.5, \"clinical\": 0.5, \"knowledge\": 0.8},\n",
    "        }\n",
    "        \n",
    "        # Get weights from config\n",
    "        rag_config = Config.QUESTION_TYPE_RETRIEVAL_CONFIG.get(\n",
    "            question_type, Config.DEFAULT_RAG_CONFIG\n",
    "        )\n",
    "        knowledge_weight = rag_config[\"weight\"] if rag_config[\"use_rag\"] else 0.0\n",
    "        \n",
    "        # Default weights if question type not found\n",
    "        default = {\"image\": 0.5, \"clinical\": 0.5, \"knowledge\": knowledge_weight}\n",
    "        type_weights = weights.get(question_type, default)\n",
    "        \n",
    "        # Override knowledge weight if specified in config\n",
    "        type_weights[\"knowledge\"] = knowledge_weight\n",
    "        \n",
    "        return type_weights\n",
    "    \n",
    "    def _create_integration_prompt(self, image_analysis, clinical_context, question_type, weights, retrieved_knowledge=None):\n",
    "        \"\"\"Create prompt for evidence integration.\"\"\"\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\n",
    "For this {question_type} question, image evidence has {weights['image']*100}% weight, clinical evidence has {weights['clinical']*100}% weight, and medical knowledge has {weights['knowledge']*100}% weight.\n",
    "\"\"\"\n",
    "        \n",
    "        # Base prompt\n",
    "        prompt = f\"\"\"As a dermatology specialist, integrate the visual findings from images with the clinical history.\n",
    "\n",
    "IMAGE ANALYSIS:\n",
    "{json.dumps(image_analysis.get(\"aggregated_analysis\", {}), indent=2)}\n",
    "\n",
    "CLINICAL CONTEXT:\n",
    "{json.dumps(clinical_context.get(\"structured_clinical_context\", {}), indent=2)}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "Pay special attention to potential contradictions between visual findings and clinical history. Even minor inconsistencies should be noted as contradictions. Look for cases where clinical context suggests features not visible in images or where visual findings seem to contradict patient-reported symptoms or history.\n",
    "\n",
    "Organize your response in a JSON structure with the following elements:\n",
    "\n",
    "1. INTEGRATED_FINDINGS: For each key dermatological feature, combine visual and clinical evidence\n",
    "   - SIZE\n",
    "   - SITE_LOCATION\n",
    "   - SKIN_DESCRIPTION\n",
    "   - LESION_COLOR\n",
    "   - LESION_COUNT\n",
    "   - EXTENT\n",
    "   - TEXTURE\n",
    "   - ONSET_DURATION\n",
    "   - SYMPTOMS\n",
    "\n",
    "2. CONCORDANCE_ASSESSMENT: For each feature, assess if visual and clinical evidence are:\n",
    "   - CONCORDANT: Visual and clinical evidence agree\n",
    "   - DISCORDANT: Visual and clinical evidence conflict (explain the conflict)\n",
    "   - COMPLEMENTARY: Evidence sources provide different but non-conflicting information\n",
    "   - MISSING_VISUAL: Clinical description present but not visible in images\n",
    "   - MISSING_CLINICAL: Visible in images but not mentioned in clinical context\n",
    "\n",
    "3. CONTRADICTIONS: List any specific contradictions between visual and clinical evidence\n",
    "   - For each contradiction, explain what the conflict is and assess which source is more reliable\n",
    "\n",
    "4. WEIGHTED_EVIDENCE_PROFILE: Synthesize the most reliable information for each category\n",
    "   - Apply the provided weights to determine the most reliable facts for each feature\n",
    "   - Explain where you've prioritized one source over another\n",
    "\n",
    "5. CONFIDENCE_SCORES: Score the confidence (0.0-1.0) in the integrated evidence for each feature\n",
    "\n",
    "Be specific, concise, and use medical terminology where appropriate.\n",
    "\"\"\"\n",
    "\n",
    "        # Add knowledge summary section if present\n",
    "        if has_knowledge:\n",
    "            prompt += \"\"\"\n",
    "\n",
    "6. MEDICAL_KNOWLEDGE_INSIGHTS: Summarize key insights from retrieved medical knowledge\n",
    "   - How the retrieved knowledge confirms or challenges the observed findings\n",
    "   - Additional relevant diagnostic or management considerations\n",
    "   - Typical clinical patterns or expected features that align with observations\n",
    "\"\"\"\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d97a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningEngine:\n",
    "    \"\"\"Applies reasoning to determine the best answer.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def apply_initial_reasoning(self, question_text, question_type, options, integrated_evidence, model_predictions, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Apply initial reasoning to determine the most likely answer.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            model_predictions: Model predictions to consider\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with reasoning and answer\n",
    "        \"\"\"\n",
    "        model_prediction_text = self._format_model_predictions(model_predictions)\n",
    "        \n",
    "        multiple_answers_allowed = question_type in [\"Site Location\", \"Size\", \"Skin Description\"]\n",
    "        \n",
    "        prompt = self._create_reasoning_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            model_prediction_text,\n",
    "            multiple_answers_allowed,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reasoning_text = response.text\n",
    "            \n",
    "            reasoning_result = parse_json_response(reasoning_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(reasoning_result.get('answer', ''), options)\n",
    "            reasoning_result['validated_answer'] = validated_answer\n",
    "            \n",
    "            # Ensure confidence doesn't exceed 0.8 unless absolutely confident\n",
    "            confidence = reasoning_result.get('confidence', 0.0)\n",
    "            if isinstance(confidence, str):\n",
    "                try:\n",
    "                    confidence = float(confidence)\n",
    "                except:\n",
    "                    confidence = 0.0\n",
    "                    \n",
    "            # Apply randomization to reduce overconfidence and encourage reflection\n",
    "            # Multiply by a random factor between 0.9 and 1.0\n",
    "            randomized_confidence = confidence * random.uniform(0.9, 1.0)\n",
    "            \n",
    "            # Cap at 0.8 unless perfect confidence (1.0)\n",
    "            if 0.95 < confidence < 1.0:\n",
    "                randomized_confidence = 0.95\n",
    "            \n",
    "            reasoning_result['confidence'] = randomized_confidence\n",
    "            \n",
    "            return reasoning_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying initial reasoning: {str(e)}\")\n",
    "            return {\n",
    "                \"reasoning\": f\"Error: {str(e)}\",\n",
    "                \"answer\": \"Not mentioned\",\n",
    "                \"validated_answer\": \"Not mentioned\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _format_model_predictions(self, model_predictions):\n",
    "        \"\"\"Format model predictions for the prompt.\"\"\"\n",
    "        model_prediction_text = \"\"\n",
    "        for model_name, predictions in model_predictions.items():\n",
    "            combined_pred = predictions.get('model_prediction', '')\n",
    "            if isinstance(combined_pred, float) and pd.isna(combined_pred):\n",
    "                combined_pred = \"No prediction\"\n",
    "            model_prediction_text += f\"- {model_name}: {combined_pred}\\n\"\n",
    "        return model_prediction_text\n",
    "\n",
    "    def _create_reasoning_prompt(self, question_text, question_type, options, integrated_evidence, model_prediction_text, multiple_answers_allowed, retrieved_knowledge=None):\n",
    "        \"\"\"Create a prompt for the reasoning layer.\"\"\"\n",
    "        specialized_guidance = \"\"\n",
    "        \n",
    "        if question_type == \"Size\" and all(option in \", \".join(options) for option in [\"size of thumb nail\", \"size of palm\", \"larger area\"]):\n",
    "            specialized_guidance = \"\"\"\n",
    "SPECIALIZED GUIDANCE FOR SIZE ASSESSMENT:\n",
    "When answering this size-related question, interpret the options as follows:\n",
    "- \"size of thumb nail\": Individual lesions or affected areas approximately 1-2 cm in diameter\n",
    "- \"size of palm\": Affected areas larger than the size of a thumb nail and roughly the size of a palm (approximately 1% of body surface area), which may include multiple smaller lesions across a region\n",
    "- \"larger area\": Widespread involvement significantly larger than a palm, affecting a substantial portion(s) of the body\n",
    "\n",
    "IMPORTANT: For cases with multiple small lesions that are visible in the images, but without extensive widespread involvement across large body regions, \"size of palm\" is likely the most appropriate answer.\n",
    "\"\"\"\n",
    "        elif question_type == \"Lesion Color\" and \"combination\" in \", \".join(options):\n",
    "            specialized_guidance = \"\"\"\n",
    "SPECIALIZED GUIDANCE FOR LESION COLOR:\n",
    "When answering color-related questions, pay careful attention to whether there are multiple distinct colors present across the affected areas. \"Combination\" would be appropriate when different lesions display different colors (e.g., some lesions appear red while others appear white), or when individual lesions show mixed or varied coloration patterns.\n",
    "\"\"\"\n",
    "\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        if multiple_answers_allowed:\n",
    "            task_description = \"\"\"\n",
    "Based on all the evidence above, determine the most accurate answer(s) to the question. Your task is to:\n",
    "1. Analyze the integrated evidence\n",
    "2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "3. Provide a detailed reasoning for your conclusion\n",
    "4. Select the final answer(s) from the available options\n",
    "5. Provide a confidence score from 0.0 to 1.0 for your answer. Be conservative in your confidence assessment. Consider all possible sources of uncertainty, including image quality limitations, interpretation ambiguity, and potential contradictions. Confidence scores should rarely exceed 0.8 unless evidence is absolutely conclusive and unambiguous.\n",
    "\n",
    "If selecting multiple answers is appropriate, provide them in a comma-separated list. If no answer can be determined, select \"Not mentioned\".\n",
    "\"\"\"\n",
    "        else:\n",
    "            task_description = \"\"\"\n",
    "Based on all the evidence above, determine the SINGLE most accurate answer to the question. Your task is to:\n",
    "1. Analyze the integrated evidence\n",
    "2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "3. Provide a detailed reasoning for your conclusion\n",
    "4. Select ONLY ONE answer option that is most accurate\n",
    "5. Provide a confidence score from 0.0 to 1.0 for your answer. Be conservative in your confidence assessment. Consider all possible sources of uncertainty, including image quality limitations, interpretation ambiguity, and potential contradictions. Confidence scores should rarely exceed 0.8 unless evidence is absolutely conclusive and unambiguous.\n",
    "\n",
    "For this question type, you must select ONLY ONE option as your answer. If no answer can be determined, select \"Not mentioned\".\n",
    "\"\"\"\n",
    "\n",
    "        response_format = \"\"\"\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"reasoning\": Your step-by-step reasoning process\n",
    "2. \"answer\": Your final answer(s) as a single string or comma-separated list of options\n",
    "3. \"confidence\": A score from 0.0 to 1.0 representing your confidence level in this answer\n",
    "4. \"evidence_used\": The key evidence that supports your answer\n",
    "5. \"uncertainty_factors\": Any factors that reduce your confidence\n",
    "6. \"counterfactual\": What evidence would make you choose a different answer\n",
    "\"\"\"\n",
    "\n",
    "        if has_knowledge:\n",
    "            response_format += \"\"\"\n",
    "7. \"knowledge_contribution\": How the retrieved medical knowledge influenced your reasoning and answer\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert analyzing dermatological findings. Use the provided evidence to determine the most accurate answer(s) for the following question:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "MODEL PREDICTIONS:\n",
    "{model_prediction_text}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "{specialized_guidance}\n",
    "\n",
    "IMPORTANT: While multiple model predictions are provided, be aware that these predictions can be inaccurate or inconsistent. Do not assume majority agreement equals correctness. Evaluate the evidence critically and independently from these predictions. Your job is to determine the correct answer based primarily on the integrated evidence, treating model predictions as secondary suggestions that may contain errors.\n",
    "\n",
    "{task_description}\n",
    "\n",
    "{response_format}\n",
    "\n",
    "When providing your answer, strictly adhere to the available options and only select from them.\n",
    "\"\"\"\n",
    "\n",
    "        return base_prompt\n",
    "\n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "178a2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfReflectionEngine:\n",
    "    \"\"\"Applies self-reflection to the reasoning process.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def apply_reflection(self, question_text, question_type, options, integrated_evidence, reasoning_result, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Apply self-reflection to the initial reasoning result.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence\n",
    "            reasoning_result: Initial reasoning result\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with reflection results\n",
    "        \"\"\"\n",
    "        prompt = self._create_reflection_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            reasoning_result,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reflection_text = response.text\n",
    "            \n",
    "            reflection_result = parse_json_response(reflection_text)\n",
    "            \n",
    "            if 'revised_answer' in reflection_result:\n",
    "                validated_answer = self._validate_answer(reflection_result.get('revised_answer', ''), options)\n",
    "                reflection_result['validated_revised_answer'] = validated_answer\n",
    "            \n",
    "            return reflection_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying reflection: {str(e)}\")\n",
    "            return {\n",
    "                \"reflection\": f\"Error: {str(e)}\",\n",
    "                \"requires_revision\": False,\n",
    "                \"confidence\": reasoning_result.get('confidence', 0.0),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _create_reflection_prompt(self, question_text, question_type, options, integrated_evidence, reasoning_result, retrieved_knowledge=None):\n",
    "        \"\"\"Create a prompt for the self-reflection layer.\"\"\"\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert critically reviewing your own reasoning about a dermatological question. \n",
    "Carefully examine the initial reasoning and check for errors, biases, and inconsistencies:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "INITIAL REASONING:\n",
    "{json.dumps(reasoning_result, indent=2)}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "Your task is to:\n",
    "1. Critically examine the initial reasoning for errors, biases, or incomplete analysis\n",
    "2. Identify any evidence that was overlooked or misinterpreted\n",
    "3. Evaluate whether the confidence level was appropriate\n",
    "4. Determine if a different answer would be more accurate\n",
    "5. Check if the evidence truly supports the chosen answer\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"reflection\": Your critical review of the initial reasoning\n",
    "2. \"overlooked_evidence\": Any important evidence that was missed or undervalued\n",
    "3. \"misinterpreted_evidence\": Any evidence that was incorrectly interpreted\n",
    "4. \"reasoning_gaps\": Logical gaps or assumptions in the initial reasoning\n",
    "5. \"confidence_assessment\": Was the confidence level appropriate? Why or why not?\n",
    "6. \"requires_revision\": Boolean indicating if the answer needs to be revised (true/false)\n",
    "7. \"revised_answer\": If revision is needed, the corrected answer\n",
    "8. \"revised_confidence\": If revision is needed, the corrected confidence level (0.0-1.0)\n",
    "9. \"revision_explanation\": If revision is needed, the explanation for the change\n",
    "\"\"\"\n",
    "\n",
    "        if has_knowledge:\n",
    "            base_prompt += \"\"\"\n",
    "10. \"knowledge_utilization_assessment\": Assessment of how well the initial reasoning utilized the available medical knowledge\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt += \"\"\"\n",
    "Be particularly careful to identify:\n",
    "- Cherry-picking: Did the initial reasoning focus only on evidence supporting its conclusion?\n",
    "- Overconfidence: Was the confidence level too high given the available evidence?\n",
    "- Alternative explanations: Are there valid alternative interpretations of the evidence?\n",
    "- Implicit assumptions: Were there unstated assumptions in the reasoning process?\n",
    "\n",
    "Be honest and thorough in your self-reflection, even if it means acknowledging errors in the initial reasoning.\n",
    "\"\"\"\n",
    "\n",
    "        return base_prompt\n",
    "\n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f93935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReAnalysisEngine:\n",
    "    \"\"\"Handles re-analysis when initial reasoning is insufficient.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def deep_analysis(self, question_text, question_type, options, integrated_evidence, reasoning_result, reflection_result, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Perform a deeper analysis based on reflection results.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence\n",
    "            reasoning_result: Initial reasoning result\n",
    "            reflection_result: Self-reflection result\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with deep analysis result\n",
    "        \"\"\"\n",
    "        prompt = self._create_deep_analysis_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            reasoning_result,\n",
    "            reflection_result,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "            \n",
    "            deep_analysis = parse_json_response(analysis_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(deep_analysis.get('final_answer', ''), options)\n",
    "            deep_analysis['validated_final_answer'] = validated_answer\n",
    "            \n",
    "            return deep_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error performing deep analysis: {str(e)}\")\n",
    "            return {\n",
    "                \"deep_reasoning\": f\"Error: {str(e)}\",\n",
    "                \"final_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                \"validated_final_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                \"final_confidence\": reasoning_result.get('confidence', 0.0),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _create_deep_analysis_prompt(self, question_text, question_type, options, integrated_evidence, reasoning_result, reflection_result, retrieved_knowledge=None):\n",
    "        \"\"\"Create a prompt for deep analysis.\"\"\"\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert performing a deep analysis for a dermatological question after identifying issues with initial reasoning.\n",
    "Review all evidence and reasoning paths comprehensively:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "INITIAL REASONING:\n",
    "{json.dumps(reasoning_result, indent=2)}\n",
    "\n",
    "REFLECTION:\n",
    "{json.dumps(reflection_result, indent=2)}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "Your task is to:\n",
    "1. Re-examine ALL available evidence with fresh eyes\n",
    "2. Address the specific issues highlighted in the reflection\n",
    "3. Consider each answer option systematically\n",
    "4. Weigh evidence for and against each potential answer\n",
    "5. Determine the most accurate answer based on comprehensive analysis\n",
    "\n",
    "For issues identified in reflection:\n",
    "- Overlooked evidence: {reflection_result.get('overlooked_evidence', 'None identified')}\n",
    "- Misinterpreted evidence: {reflection_result.get('misinterpreted_evidence', 'None identified')}\n",
    "- Reasoning gaps: {reflection_result.get('reasoning_gaps', 'None identified')}\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"deep_reasoning\": Your comprehensive analysis considering all evidence and perspectives\n",
    "2. \"systematic_assessment\": Assessment of evidence for EACH possible answer option\n",
    "3. \"final_answer\": Your conclusion after deep analysis\n",
    "4. \"final_confidence\": Your confidence level after deep analysis (0.0-1.0)\n",
    "5. \"key_determinants\": The most important factors that determined your final answer\n",
    "6. \"remaining_uncertainties\": Any unresolved questions or limitations\n",
    "\"\"\"\n",
    "\n",
    "        if has_knowledge:\n",
    "            base_prompt += \"\"\"\n",
    "7. \"knowledge_integration\": How you've incorporated medical knowledge into your final analysis\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt += \"\"\"\n",
    "Be thorough, balanced, and precise in your analysis. Consider the evidence holistically and avoid the pitfalls identified in the reflection phase.\n",
    "\"\"\"\n",
    "\n",
    "        return base_prompt\n",
    "\n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184c497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticDermatologyPipeline:\n",
    "    \"\"\"Main pipeline for agentic dermatology analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        if api_key is None:\n",
    "            load_dotenv()\n",
    "            api_key = os.getenv(\"API_KEY\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        \n",
    "        # Initialize knowledge base and retrieval components\n",
    "        print(\"Initializing knowledge base...\")\n",
    "        self.kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        # Initialize query generator\n",
    "        self.query_generator = QueryGenerator(self.client)\n",
    "        \n",
    "        # Initialize knowledge retriever\n",
    "        self.knowledge_retriever = DermatologyKnowledgeRetriever(\n",
    "            self.kb_manager,\n",
    "            self.query_generator\n",
    "        )\n",
    "        \n",
    "        # Initialize analysis components\n",
    "        self.image_analyzer = ImageAnalysisService(self.client)\n",
    "        self.clinical_analyzer = ClinicalContextAnalyzer(self.client)\n",
    "        self.evidence_integrator = EvidenceIntegrator(self.client)\n",
    "        self.reasoning_engine = ReasoningEngine(self.client)\n",
    "        self.reflection_engine = SelfReflectionEngine(self.client)\n",
    "        self.reanalysis_engine = ReAnalysisEngine(self.client)\n",
    "    \n",
    "    def process_single_encounter(self, agentic_data, encounter_id):\n",
    "        \"\"\"\n",
    "        Process a single encounter with all its questions using the agentic pipeline.\n",
    "\n",
    "        Args:\n",
    "            agentic_data: AgenticRAGData instance containing all encounter data\n",
    "            encounter_id: The specific encounter ID to process\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with all questions processed with agentic reasoning for this encounter\n",
    "        \"\"\"\n",
    "        all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "        encounter_pairs = [pair for pair in all_pairs if pair[0] == encounter_id]\n",
    "\n",
    "        if not encounter_pairs:\n",
    "            print(f\"No data found for encounter {encounter_id}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Processing {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "\n",
    "        encounter_results = {encounter_id: {}}\n",
    "\n",
    "        # Extract image analysis once per encounter\n",
    "        print(f\"Computing image analysis for {encounter_id}\")\n",
    "        sample_data = agentic_data.get_combined_data(encounter_pairs[0][0], encounter_pairs[0][1])\n",
    "        image_analysis = self.image_analyzer.analyze_images(sample_data['images'], encounter_id)\n",
    "\n",
    "        # Extract clinical context once per encounter\n",
    "        print(f\"Extracting clinical context for {encounter_id}\")\n",
    "        clinical_context = self.clinical_analyzer.extract_clinical_context(\n",
    "            sample_data['query_context'], \n",
    "            encounter_id\n",
    "        )\n",
    "\n",
    "        for i, (encounter_id, base_qid) in enumerate(encounter_pairs):\n",
    "            print(f\"Processing question {i+1}/{len(encounter_pairs)}: {base_qid}\")\n",
    "\n",
    "            sample_data = agentic_data.get_combined_data(encounter_id, base_qid)\n",
    "            if not sample_data:\n",
    "                print(f\"Warning: No data found for {encounter_id}, {base_qid}\")\n",
    "                continue\n",
    "\n",
    "            # Extract question details\n",
    "            question_text = sample_data['query_context'].split(\"MAIN QUESTION TO ANSWER:\")[1].split(\"\\n\")[0].strip()\n",
    "            question_type = sample_data['question_type']\n",
    "            options = sample_data['options']\n",
    "            model_predictions = sample_data['model_predictions']\n",
    "\n",
    "            # Check if we should retrieve knowledge for this question type\n",
    "            rag_config = Config.QUESTION_TYPE_RETRIEVAL_CONFIG.get(\n",
    "                question_type, Config.DEFAULT_RAG_CONFIG\n",
    "            )\n",
    "\n",
    "            retrieved_knowledge = None\n",
    "            if rag_config[\"use_rag\"]:\n",
    "                # Integrate evidence without knowledge first (needed for query generation)\n",
    "                print(f\"Initial evidence integration for {encounter_id}, {base_qid}\")\n",
    "                initial_integrated_evidence = self.evidence_integrator.integrate_evidence(\n",
    "                    image_analysis,\n",
    "                    clinical_context,\n",
    "                    question_type\n",
    "                )\n",
    "                \n",
    "                # Retrieve knowledge based on integrated evidence\n",
    "                print(f\"Retrieving knowledge for {encounter_id}, {base_qid}\")\n",
    "                retrieved_knowledge = self.knowledge_retriever.retrieve_knowledge(\n",
    "                    question_text,\n",
    "                    question_type,\n",
    "                    options,\n",
    "                    initial_integrated_evidence\n",
    "                )\n",
    "            \n",
    "            # Integrate evidence with retrieved knowledge\n",
    "            print(f\"Integrating all evidence for {encounter_id}, {base_qid}\")\n",
    "            integrated_evidence = self.evidence_integrator.integrate_evidence(\n",
    "                image_analysis,\n",
    "                clinical_context,\n",
    "                question_type,\n",
    "                retrieved_knowledge\n",
    "            )\n",
    "\n",
    "            # Initial reasoning\n",
    "            print(f\"Initial reasoning for {encounter_id}, {base_qid}\")\n",
    "            reasoning_result = self.reasoning_engine.apply_initial_reasoning(\n",
    "                question_text,\n",
    "                question_type,\n",
    "                options,\n",
    "                integrated_evidence,\n",
    "                model_predictions,\n",
    "                retrieved_knowledge\n",
    "            )\n",
    "\n",
    "            # Determine if self-reflection is needed based on confidence\n",
    "            confidence = reasoning_result.get('confidence', 0.0)\n",
    "            if isinstance(confidence, str):\n",
    "                try:\n",
    "                    confidence = float(confidence)\n",
    "                except:\n",
    "                    confidence = 0.0\n",
    "\n",
    "            final_result = reasoning_result\n",
    "            reflection_path = []\n",
    "\n",
    "            # Apply self-reflection if confidence is below threshold\n",
    "            if confidence < Config.CONFIDENCE_THRESHOLD:\n",
    "                print(f\"Confidence {confidence} below threshold. Applying self-reflection.\")\n",
    "\n",
    "                reflection_result = self.reflection_engine.apply_reflection(\n",
    "                    question_text,\n",
    "                    question_type,\n",
    "                    options,\n",
    "                    integrated_evidence,\n",
    "                    reasoning_result,\n",
    "                    retrieved_knowledge\n",
    "                )\n",
    "                reflection_path.append(reflection_result)\n",
    "\n",
    "                # Determine if re-analysis is needed based on reflection\n",
    "                requires_revision = reflection_result.get('requires_revision', False)\n",
    "                if requires_revision:\n",
    "                    print(f\"Reflection indicates revision needed. Performing deep analysis.\")\n",
    "\n",
    "                    deep_analysis = self.reanalysis_engine.deep_analysis(\n",
    "                        question_text,\n",
    "                        question_type,\n",
    "                        options,\n",
    "                        integrated_evidence,\n",
    "                        reasoning_result,\n",
    "                        reflection_result,\n",
    "                        retrieved_knowledge\n",
    "                    )\n",
    "                    reflection_path.append(deep_analysis)\n",
    "\n",
    "                    final_result = {\n",
    "                        \"reasoning\": deep_analysis.get('deep_reasoning', ''),\n",
    "                        \"answer\": deep_analysis.get('final_answer', 'Not mentioned'),\n",
    "                        \"validated_answer\": deep_analysis.get('validated_final_answer', 'Not mentioned'),\n",
    "                        \"confidence\": deep_analysis.get('final_confidence', 0.0)\n",
    "                    }\n",
    "                else:\n",
    "                    # Use original answer but with updated confidence if available\n",
    "                    revised_confidence = reflection_result.get('revised_confidence', reasoning_result.get('confidence', 0.0))\n",
    "                    final_result = {\n",
    "                        \"reasoning\": reasoning_result.get('reasoning', ''),\n",
    "                        \"answer\": reasoning_result.get('answer', 'Not mentioned'),\n",
    "                        \"validated_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                        \"confidence\": revised_confidence\n",
    "                    }\n",
    "\n",
    "            encounter_results[encounter_id][base_qid] = {\n",
    "                \"query_context\": sample_data['query_context'],\n",
    "                \"options\": sample_data['options'],\n",
    "                \"model_predictions\": sample_data['model_predictions'],\n",
    "                \"retrieved_knowledge\": retrieved_knowledge,\n",
    "                \"integrated_evidence\": integrated_evidence,\n",
    "                \"reasoning_result\": reasoning_result,\n",
    "                \"reflection_path\": reflection_path,\n",
    "                \"final_result\": final_result,\n",
    "                \"final_answer\": final_result.get('validated_answer', 'Not mentioned')\n",
    "            }\n",
    "\n",
    "        output_file = os.path.join(Config.OUTPUT_DIR, f\"agentic_rag_results_{encounter_id}.json\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(encounter_results, f, indent=2)\n",
    "\n",
    "        print(f\"Processed all {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "        return encounter_results\n",
    "    \n",
    "    def format_results_for_evaluation(self, encounter_results, output_file):\n",
    "        \"\"\"Format results for official evaluation.\"\"\"\n",
    "        QIDS = [\n",
    "            \"CQID010-001\",\n",
    "            \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",\n",
    "            \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",\n",
    "            \"CQID015-001\",\n",
    "            \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "            \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",\n",
    "            \"CQID025-001\",\n",
    "            \"CQID034-001\",\n",
    "            \"CQID035-001\",\n",
    "            \"CQID036-001\",\n",
    "        ]\n",
    "        \n",
    "        qid_variants = {}\n",
    "        for qid in QIDS:\n",
    "            base_qid, variant = qid.split('-')\n",
    "            if base_qid not in qid_variants:\n",
    "                qid_variants[base_qid] = []\n",
    "            qid_variants[base_qid].append(qid)\n",
    "        \n",
    "        required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "        \n",
    "        formatted_predictions = []\n",
    "        for encounter_id, questions in encounter_results.items():\n",
    "            encounter_base_qids = set(questions.keys())\n",
    "            if not required_base_qids.issubset(encounter_base_qids):\n",
    "                print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "                continue\n",
    "            \n",
    "            pred_entry = {'encounter_id': encounter_id}\n",
    "            \n",
    "            for base_qid, question_data in questions.items():\n",
    "                if base_qid not in qid_variants:\n",
    "                    continue\n",
    "                \n",
    "                final_answer = question_data['final_answer']\n",
    "                options = question_data['options']\n",
    "                \n",
    "                not_mentioned_index = self._find_not_mentioned_index(options)\n",
    "                \n",
    "                self._process_answers(\n",
    "                    pred_entry, \n",
    "                    base_qid, \n",
    "                    final_answer, \n",
    "                    options, \n",
    "                    qid_variants, \n",
    "                    not_mentioned_index\n",
    "                )\n",
    "            \n",
    "            formatted_predictions.append(pred_entry)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(formatted_predictions, f, indent=2)\n",
    "        \n",
    "        print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "        return formatted_predictions\n",
    "    \n",
    "    def _find_not_mentioned_index(self, options):\n",
    "        \"\"\"Find the index of 'Not mentioned' in options.\"\"\"\n",
    "        for i, opt in enumerate(options):\n",
    "            if opt.lower() == \"not mentioned\":\n",
    "                return i\n",
    "        return len(options) - 1\n",
    "    \n",
    "    def _process_answers(self, pred_entry, base_qid, final_answer, options, qid_variants, not_mentioned_index):\n",
    "        \"\"\"Process answers and add to prediction entry.\"\"\"\n",
    "        if ',' in final_answer:\n",
    "            answer_parts = [part.strip() for part in final_answer.split(',')]\n",
    "            answer_indices = []\n",
    "            \n",
    "            for part in answer_parts:\n",
    "                found = False\n",
    "                for i, opt in enumerate(options):\n",
    "                    if part.lower() == opt.lower():\n",
    "                        answer_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found:\n",
    "                    answer_indices.append(not_mentioned_index)\n",
    "            \n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            for i, idx in enumerate(answer_indices):\n",
    "                if i < len(available_variants):\n",
    "                    pred_entry[available_variants[i]] = idx\n",
    "            \n",
    "            for i in range(len(answer_indices), len(available_variants)):\n",
    "                pred_entry[available_variants[i]] = not_mentioned_index\n",
    "            \n",
    "        else:\n",
    "            answer_index = not_mentioned_index\n",
    "            \n",
    "            for i, opt in enumerate(options):\n",
    "                if final_answer.lower() == opt.lower():\n",
    "                    answer_index = i\n",
    "                    break\n",
    "            \n",
    "            pred_entry[qid_variants[base_qid][0]] = answer_index\n",
    "            \n",
    "            if len(qid_variants[base_qid]) > 1:\n",
    "                for i in range(1, len(qid_variants[base_qid])):\n",
    "                    pred_entry[qid_variants[base_qid][i]] = not_mentioned_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "275c6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_encounters_pipeline():\n",
    "    \"\"\"Run the agentic pipeline for all available encounters.\"\"\"\n",
    "    # Load model predictions and validation dataset\n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(Config.MODEL_PREDICTIONS_DIR)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    validation_df = DataLoader.load_validation_dataset(Config.VAL_DATASET_PATH)\n",
    "    \n",
    "    # Create agentic data and pipeline\n",
    "    agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "    pipeline = AgenticDermatologyPipeline()\n",
    "    \n",
    "    # Get all unique encounter IDs\n",
    "    all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "    unique_encounter_ids = sorted(list(set(pair[0] for pair in all_pairs)))\n",
    "    print(f\"Found {len(unique_encounter_ids)} unique encounters to process\")\n",
    "    \n",
    "    # Process each encounter\n",
    "    all_encounter_results = {}\n",
    "    for i, encounter_id in enumerate(unique_encounter_ids):\n",
    "        print(f\"Processing encounter {i+1}/{len(unique_encounter_ids)}: {encounter_id}...\")\n",
    "        \n",
    "        try:\n",
    "            encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "            if encounter_results:\n",
    "                all_encounter_results.update(encounter_results)\n",
    "                \n",
    "            # Save intermediate results periodically\n",
    "            if (i+1) % 5 == 0 or (i+1) == len(unique_encounter_ids):\n",
    "                timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                intermediate_output_file = os.path.join(\n",
    "                    Config.OUTPUT_DIR, \n",
    "                    f\"intermediate_agentic_rag_results_{i+1}_of_{len(unique_encounter_ids)}_{timestamp}.json\"\n",
    "                )\n",
    "                with open(intermediate_output_file, 'w') as f:\n",
    "                    json.dump(all_encounter_results, f, indent=2)\n",
    "                print(f\"Saved intermediate results after processing {i+1} encounters\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing encounter {encounter_id}: {str(e)}\")\n",
    "            # Save error information\n",
    "            error_file = os.path.join(\n",
    "                Config.OUTPUT_DIR, \n",
    "                f\"error_encounter_{encounter_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "            )\n",
    "            with open(error_file, 'w') as f:\n",
    "                f.write(f\"Error processing encounter {encounter_id}: {str(e)}\\n\")\n",
    "                f.write(f\"Traceback:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "    # Format and save final predictions\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(\n",
    "        Config.OUTPUT_DIR, \n",
    "        f\"data_cvqa_sys_agentic_rag_all_{timestamp}.json\"\n",
    "    )\n",
    "    \n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(all_encounter_results, output_file)\n",
    "    \n",
    "    print(f\"Processed {len(formatted_predictions)} encounters successfully\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432ed21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_encounter_pipeline(encounter_id):\n",
    "    \"\"\"Run the agentic pipeline for a single encounter.\"\"\"\n",
    "    # Load model predictions and validation dataset\n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(Config.MODEL_PREDICTIONS_DIR)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    validation_df = DataLoader.load_validation_dataset(Config.VAL_DATASET_PATH)\n",
    "    \n",
    "    # Create agentic data and pipeline\n",
    "    agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "    pipeline = AgenticDermatologyPipeline()\n",
    "    \n",
    "    # Process the encounter\n",
    "    encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "    \n",
    "    # Format and save predictions\n",
    "    output_file = os.path.join(\n",
    "        Config.OUTPUT_DIR, \n",
    "        f\"data_cvqa_sys_agentic_rag_{encounter_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    )\n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(encounter_results, output_file)\n",
    "    \n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb019882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /storage/home/hcoda1/2/kthakrar3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing knowledge base at /storage/scratch1/2/kthakrar3/mediqa-magic-v2/knowledge_db\n",
      "Initializing BM25 index...\n",
      "BM25 index initialization complete.\n",
      "Initializing knowledge base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /storage/home/hcoda1/2/kthakrar3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing knowledge base at /storage/scratch1/2/kthakrar3/mediqa-magic-v2/knowledge_db\n",
      "Initializing BM25 index...\n",
      "BM25 index initialization complete.\n",
      "Processing 9 questions for encounter ENC00854\n",
      "Computing image analysis for ENC00854\n",
      "Analyzing image 1/3 for encounter ENC00854\n",
      "Analyzing image 2/3 for encounter ENC00854\n",
      "Analyzing image 3/3 for encounter ENC00854\n",
      "Extracting clinical context for ENC00854\n",
      "Processing question 1/9: CQID010\n",
      "Initial evidence integration for ENC00854, CQID010\n",
      "Retrieving knowledge for ENC00854, CQID010\n",
      "Integrating all evidence for ENC00854, CQID010\n",
      "Initial reasoning for ENC00854, CQID010\n",
      "Processing question 2/9: CQID011\n",
      "Integrating all evidence for ENC00854, CQID011\n",
      "Initial reasoning for ENC00854, CQID011\n",
      "Processing question 3/9: CQID012\n",
      "Integrating all evidence for ENC00854, CQID012\n",
      "Initial reasoning for ENC00854, CQID012\n",
      "Confidence 0.7251415215811634 below threshold. Applying self-reflection.\n",
      "Processing question 4/9: CQID015\n",
      "Initial evidence integration for ENC00854, CQID015\n",
      "Retrieving knowledge for ENC00854, CQID015\n",
      "Integrating all evidence for ENC00854, CQID015\n",
      "Initial reasoning for ENC00854, CQID015\n",
      "Processing question 5/9: CQID020\n",
      "Initial evidence integration for ENC00854, CQID020\n",
      "Retrieving knowledge for ENC00854, CQID020\n",
      "Integrating all evidence for ENC00854, CQID020\n",
      "Initial reasoning for ENC00854, CQID020\n",
      "Confidence 0.7514726377653083 below threshold. Applying self-reflection.\n",
      "Processing question 6/9: CQID025\n",
      "Initial evidence integration for ENC00854, CQID025\n",
      "Retrieving knowledge for ENC00854, CQID025\n",
      "Integrating all evidence for ENC00854, CQID025\n",
      "Initial reasoning for ENC00854, CQID025\n",
      "Processing question 7/9: CQID034\n",
      "Integrating all evidence for ENC00854, CQID034\n",
      "Initial reasoning for ENC00854, CQID034\n",
      "Processing question 8/9: CQID035\n",
      "Integrating all evidence for ENC00854, CQID035\n",
      "Initial reasoning for ENC00854, CQID035\n",
      "Confidence 0.7930539045264896 below threshold. Applying self-reflection.\n",
      "Processing question 9/9: CQID036\n",
      "Initial evidence integration for ENC00854, CQID036\n",
      "Retrieving knowledge for ENC00854, CQID036\n",
      "Integrating all evidence for ENC00854, CQID036\n",
      "Initial reasoning for ENC00854, CQID036\n",
      "Processed all 9 questions for encounter ENC00854\n",
      "Formatted predictions saved to /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/data_cvqa_sys_agentic_rag_ENC00854_20250501_134358.json (1 complete encounters)\n",
      "Processed encounter ENC00854 with 1 prediction entries\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the knowledge base first (if needed)\n",
    "    kb_manager = KnowledgeBaseManager()\n",
    "    \n",
    "    # Run for a single encounter\n",
    "    encounter_id = \"ENC00854\"\n",
    "    formatted_predictions = run_single_encounter_pipeline(encounter_id)\n",
    "    print(f\"Processed encounter {encounter_id} with {len(formatted_predictions)} prediction entries\")\n",
    "    \n",
    "    # Or run for all encounters\n",
    "    # formatted_predictions = run_all_encounters_pipeline()\n",
    "    # print(f\"Total complete encounters processed: {len(formatted_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfb72406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'encounter_id': 'ENC00854',\n",
       "  'CQID010-001': 1,\n",
       "  'CQID011-001': 2,\n",
       "  'CQID011-002': 7,\n",
       "  'CQID011-003': 7,\n",
       "  'CQID011-004': 7,\n",
       "  'CQID011-005': 7,\n",
       "  'CQID011-006': 7,\n",
       "  'CQID012-001': 0,\n",
       "  'CQID012-002': 1,\n",
       "  'CQID012-003': 3,\n",
       "  'CQID012-004': 3,\n",
       "  'CQID012-005': 3,\n",
       "  'CQID012-006': 3,\n",
       "  'CQID015-001': 5,\n",
       "  'CQID020-001': 0,\n",
       "  'CQID020-002': 3,\n",
       "  'CQID020-003': 6,\n",
       "  'CQID020-004': 9,\n",
       "  'CQID020-005': 9,\n",
       "  'CQID020-006': 9,\n",
       "  'CQID020-007': 9,\n",
       "  'CQID020-008': 9,\n",
       "  'CQID020-009': 9,\n",
       "  'CQID025-001': 1,\n",
       "  'CQID034-001': 8,\n",
       "  'CQID035-001': 1,\n",
       "  'CQID036-001': 1}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
