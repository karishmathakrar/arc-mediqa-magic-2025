{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/hf_cache\n"
     ]
    }
   ],
   "source": [
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/coda1/p-dsgt_clef2025/0/kthakrar3/hf_cache'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.pop(\"TRANSFORMERS_CACHE\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\"\n",
    "\n",
    "# where the downloaded model goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE: None\n",
      "HF_HOME: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\n"
     ]
    }
   ],
   "source": [
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/bin/python\n",
      "Python version: 3.10.10 (main, Apr 15 2024, 11:52:16) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
      "Python version info: sys.version_info(major=3, minor=10, micro=10, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>answer_index</th>\n",
       "      <th>question_en</th>\n",
       "      <th>options_en</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>responses</th>\n",
       "      <th>query_title_en</th>\n",
       "      <th>query_content_en</th>\n",
       "      <th>image_paths</th>\n",
       "      <th>responses_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U04473</td>\n",
       "      <td>[IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]</td>\n",
       "      <td>[{'author_id': 'U00217', 'content_zh': '银屑病，似与...</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...</td>\n",
       "      <td>[Psoriasis seems to have no relation to pleura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00002</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U06063</td>\n",
       "      <td>[IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...</td>\n",
       "      <td>[{'author_id': 'U11305', 'content_zh': '脚气', '...</td>\n",
       "      <td>What is on the bottom of the right foot?</td>\n",
       "      <td>The patient is a 50-year-old male, who has bee...</td>\n",
       "      <td>['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...</td>\n",
       "      <td>[Beriberi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00003</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U00780</td>\n",
       "      <td>[IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...</td>\n",
       "      <td>[{'author_id': 'U01131', 'content_zh': '瘙痒症，有无...</td>\n",
       "      <td>Interpreting Images - Is it magical skin?</td>\n",
       "      <td>Male, 65 years old, skin lesions as shown in t...</td>\n",
       "      <td>['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...</td>\n",
       "      <td>[Pruritus, is there any other special medical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00004</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>2</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>widespread</td>\n",
       "      <td>U00209</td>\n",
       "      <td>[IMG_ENC00004_00001.jpg, IMG_ENC00004_00002.jpg]</td>\n",
       "      <td>[{'author_id': 'U06715', 'content_zh': '肢端角化病？...</td>\n",
       "      <td>Skin Disease</td>\n",
       "      <td>Male, 15 years old, keratosis on both palms, s...</td>\n",
       "      <td>['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...</td>\n",
       "      <td>[Acrokeratosis?, Progressive Symmetrical Eryth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00005</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U09050</td>\n",
       "      <td>[IMG_ENC00005_00001.jpg]</td>\n",
       "      <td>[{'author_id': 'U09402', 'content_zh': '是否神经性皮...</td>\n",
       "      <td>Perifollicular atrophy?</td>\n",
       "      <td>Young female, silver-gray dot-like atrophy spo...</td>\n",
       "      <td>['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...</td>\n",
       "      <td>[Is it neurodermatitis?, Impotence?, Lichen Sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id          qid  answer_index                        question_en  \\\n",
       "0     ENC00001  CQID010-001             1  How much of the body is affected?   \n",
       "1     ENC00002  CQID010-001             1  How much of the body is affected?   \n",
       "2     ENC00003  CQID010-001             1  How much of the body is affected?   \n",
       "3     ENC00004  CQID010-001             2  How much of the body is affected?   \n",
       "4     ENC00005  CQID010-001             1  How much of the body is affected?   \n",
       "\n",
       "                                          options_en   answer_text author_id  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...  limited area    U04473   \n",
       "1  ['single spot', 'limited area', 'widespread', ...  limited area    U06063   \n",
       "2  ['single spot', 'limited area', 'widespread', ...  limited area    U00780   \n",
       "3  ['single spot', 'limited area', 'widespread', ...    widespread    U00209   \n",
       "4  ['single spot', 'limited area', 'widespread', ...  limited area    U09050   \n",
       "\n",
       "                                           image_ids  \\\n",
       "0   [IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]   \n",
       "1  [IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...   \n",
       "2  [IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...   \n",
       "3   [IMG_ENC00004_00001.jpg, IMG_ENC00004_00002.jpg]   \n",
       "4                           [IMG_ENC00005_00001.jpg]   \n",
       "\n",
       "                                           responses  \\\n",
       "0  [{'author_id': 'U00217', 'content_zh': '银屑病，似与...   \n",
       "1  [{'author_id': 'U11305', 'content_zh': '脚气', '...   \n",
       "2  [{'author_id': 'U01131', 'content_zh': '瘙痒症，有无...   \n",
       "3  [{'author_id': 'U06715', 'content_zh': '肢端角化病？...   \n",
       "4  [{'author_id': 'U09402', 'content_zh': '是否神经性皮...   \n",
       "\n",
       "                              query_title_en  \\\n",
       "0       Pleural effusion accompanied by rash   \n",
       "1   What is on the bottom of the right foot?   \n",
       "2  Interpreting Images - Is it magical skin?   \n",
       "3                               Skin Disease   \n",
       "4                    Perifollicular atrophy?   \n",
       "\n",
       "                                    query_content_en  \\\n",
       "0  A patient with pleural effusion is accompanied...   \n",
       "1  The patient is a 50-year-old male, who has bee...   \n",
       "2  Male, 65 years old, skin lesions as shown in t...   \n",
       "3  Male, 15 years old, keratosis on both palms, s...   \n",
       "4  Young female, silver-gray dot-like atrophy spo...   \n",
       "\n",
       "                                         image_paths  \\\n",
       "0  ['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...   \n",
       "1  ['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...   \n",
       "2  ['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...   \n",
       "3  ['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...   \n",
       "4  ['C:\\\\Users\\\\karishma\\\\OneDrive\\\\Projects\\\\med...   \n",
       "\n",
       "                                        responses_en  \n",
       "0  [Psoriasis seems to have no relation to pleura...  \n",
       "1                                         [Beriberi]  \n",
       "2  [Pruritus, is there any other special medical ...  \n",
       "3  [Acrokeratosis?, Progressive Symmetrical Eryth...  \n",
       "4  [Is it neurodermatitis?, Impotence?, Lichen Sc...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_file = os.path.join(\"2025_dataset\", \"train\", \"final_df.csv\")\n",
    "train_images_dir = os.path.join(\"2025_dataset\", \"train\", \"images_train\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "\n",
    "train_df['image_ids'] = train_df['image_ids'].apply(eval)\n",
    "train_df['responses_en'] = train_df['responses_en'].apply(eval)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 samples for training\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe shape: (10, 6)\n",
      "Columns: ['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sample row:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>question_en</th>\n",
       "      <th>options_en</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>image_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00002</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00003</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id          qid                        question_en  \\\n",
       "0     ENC00001  CQID010-001  How much of the body is affected?   \n",
       "1     ENC00002  CQID010-001  How much of the body is affected?   \n",
       "2     ENC00003  CQID010-001  How much of the body is affected?   \n",
       "\n",
       "                                          options_en   answer_text  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "1  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "2  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "\n",
       "                                           image_ids  \n",
       "0   [IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]  \n",
       "1  [IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...  \n",
       "2  [IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = train_df[['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids']]\n",
    "\n",
    "print(f\"Filtered dataframe shape: {train_df.shape}\")\n",
    "print(\"Columns:\", train_df.columns.tolist())\n",
    "\n",
    "display(\"Sample row:\", train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            image_paths = [os.path.join(train_images_dir, img_id) for img_id in row['image_ids']]\n",
    "            \n",
    "            if not all(os.path.exists(img_path) for img_path in image_paths):\n",
    "                continue\n",
    "\n",
    "            valid_images = []\n",
    "            for img_path in image_paths:\n",
    "                try:\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.load()\n",
    "                    valid_images.append(img_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Corrupt or unreadable image at {img_path} — {e}\")\n",
    "\n",
    "            if len(valid_images) != len(image_paths):\n",
    "                continue\n",
    "            \n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(eval(row['options_en']))])\n",
    "            query_text = f\"Question: {row['question_en']} Options: {options_text}\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"encounter_id\": row['encounter_id'],\n",
    "                \"qid\": row['qid'],\n",
    "                \"query_text\": query_text,\n",
    "                \"image_paths\": valid_images,\n",
    "                \"answer_text\": row['answer_text']\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        images = [Image.open(path).convert(\"RGB\") for path in example['image_paths']]\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"You are an AI assistant answering medical questions based on images.\"}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    *[{\"type\": \"image\", \"image\": img} for img in images],\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"processed_data\"):\n",
    "    total_processed = 0\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"processed_data_debug\"\n",
    "\n",
    "if os.path.exists(processed_data_dir):\n",
    "    shutil.rmtree(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6242326b37d54194b78f10dbb705e242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 examples so far\n",
      "Total processed examples: 10\n"
     ]
    }
   ],
   "source": [
    "total_examples = preprocess_dataset(train_df, batch_size=500, save_dir=processed_data_dir)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data (first example):\n",
      "Encounter ID: ENC00001\n",
      "Question ID: CQID010-001\n",
      "Query text: Question: How much of the body is affected? Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned\n",
      "Number of images: 2\n",
      "Image paths: ['2025_dataset/train/images_train/IMG_ENC00001_00001.jpg', '2025_dataset/train/images_train/IMG_ENC00001_00002.jpg']\n",
      "Answer text: limited area\n"
     ]
    }
   ],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[0]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "print(f\"Number of images: {len(sample_data['image_paths'])}\")\n",
    "print(f\"Image paths: {sample_data['image_paths']}\")\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data (first example):\n",
      "Encounter ID: ENC00002\n",
      "Question ID: CQID010-001\n",
      "Query text: Question: How much of the body is affected? Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned\n",
      "Number of images: 4\n",
      "Image paths: ['2025_dataset/train/images_train/IMG_ENC00002_00001.jpg', '2025_dataset/train/images_train/IMG_ENC00002_00002.jpg', '2025_dataset/train/images_train/IMG_ENC00002_00003.jpg', '2025_dataset/train/images_train/IMG_ENC00002_00004.jpg']\n",
      "Answer text: limited area\n"
     ]
    }
   ],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[1]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "print(f\"Number of images: {len(sample_data['image_paths'])}\")\n",
    "print(f\"Image paths: {sample_data['image_paths']}\")\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data (first example):\n",
      "Encounter ID: ENC00003\n",
      "Question ID: CQID010-001\n",
      "Query text: Question: How much of the body is affected? Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned\n",
      "Number of images: 5\n",
      "Image paths: ['2025_dataset/train/images_train/IMG_ENC00003_00001.jpg', '2025_dataset/train/images_train/IMG_ENC00003_00002.jpg', '2025_dataset/train/images_train/IMG_ENC00003_00003.jpg', '2025_dataset/train/images_train/IMG_ENC00003_00004.jpg', '2025_dataset/train/images_train/IMG_ENC00003_00005.jpg']\n",
      "Answer text: limited area\n"
     ]
    }
   ],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[2]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "print(f\"Number of images: {len(sample_data['image_paths'])}\")\n",
    "print(f\"Image paths: {sample_data['image_paths']}\")\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 10\n",
      "\n",
      "First example roles:\n",
      "System role: {'role': 'system', 'content': [{'type': 'text', 'text': 'You are an AI assistant answering medical questions based on images.'}]}\n",
      "User role: {'role': 'user', 'content': [{'type': 'text', 'text': 'Question: How much of the body is affected? Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=1944x2541 at 0x15541D9B75E0>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=2592x1944 at 0x15541D9CC190>}]}\n",
      "Assistant role: {'role': 'assistant', 'content': [{'type': 'text', 'text': 'limited area'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[0]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 10\n",
      "\n",
      "First example roles:\n",
      "System role: {'role': 'system', 'content': [{'type': 'text', 'text': 'You are an AI assistant answering medical questions based on images.'}]}\n",
      "User role: {'role': 'user', 'content': [{'type': 'text', 'text': 'Question: How much of the body is affected? Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=2560x1920 at 0x15541D782B30>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=2560x1920 at 0x15541DA19270>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=2560x1920 at 0x15541D903880>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=2560x1920 at 0x15541D902E00>}]}\n",
      "Assistant role: {'role': 'assistant', 'content': [{'type': 'text', 'text': 'limited area'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[1]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 10\n",
      "\n",
      "First example roles:\n",
      "System role: {'role': 'system', 'content': [{'type': 'text', 'text': 'You are an AI assistant answering medical questions based on images.'}]}\n",
      "User role: {'role': 'user', 'content': [{'type': 'text', 'text': 'Question: How much of the body is affected? Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=600x400 at 0x15544D0510F0>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=600x401 at 0x155550D22380>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=600x402 at 0x155550D20940>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=600x400 at 0x155550D21CC0>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=600x399 at 0x155550D23700>}]}\n",
      "Assistant role: {'role': 'assistant', 'content': [{'type': 'text', 'text': 'limited area'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[2]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ab7b6ec08e4f05b02975545259eceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc7608ffc7f4a72b39aed01bb0fd1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16. Use a different GPU.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs, token=hf_token)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current chat template: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current chat template: {processor.tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_chat_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}\n",
    "<start_of_turn>user\n",
    "{% for content in message['content'] %}\n",
    "{% if content['type'] == 'text' %}{{ content['text'] }}{% elif content['type'] == 'image' %}<image>{% endif %}\n",
    "{% endfor %}\n",
    "<end_of_turn>\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "<start_of_turn>model\n",
    "{% for content in message['content'] %}\n",
    "{% if content['type'] == 'text' %}{{ content['text'] }}{% endif %}\n",
    "{% endfor %}\n",
    "<end_of_turn>\n",
    "{% elif message['role'] == 'system' %}\n",
    "<start_of_turn>system\n",
    "{% for content in message['content'] %}\n",
    "{% if content['type'] == 'text' %}{{ content['text'] }}{% endif %}\n",
    "{% endfor %}\n",
    "<end_of_turn>\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "<start_of_turn>model\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "\n",
    "# Set template\n",
    "processor.tokenizer.chat_template = gemma_chat_template\n",
    "processor.chat_template = gemma_chat_template\n",
    "\n",
    "# Ensure <image> is recognized as a token\n",
    "if \"<image>\" not in processor.tokenizer.get_vocab():\n",
    "    processor.tokenizer.add_special_tokens({'additional_special_tokens': ['<image>']})\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Set boi_token for template rendering\n",
    "processor.boi_token = \"<image>\"\n",
    "processor.tokenizer.special_tokens_map['boi_token'] = \"<image>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "<start_of_turn>user\n",
      "{% for content in message['content'] %}\n",
      "{% if content['type'] == 'text' %}{{ content['text'] }}{% elif content['type'] == 'image' %}<image>{% endif %}\n",
      "{% endfor %}\n",
      "<end_of_turn>\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "<start_of_turn>model\n",
      "{% for content in message['content'] %}\n",
      "{% if content['type'] == 'text' %}{{ content['text'] }}{% endif %}\n",
      "{% endfor %}\n",
      "<end_of_turn>\n",
      "{% elif message['role'] == 'system' %}\n",
      "<start_of_turn>system\n",
      "{% for content in message['content'] %}\n",
      "{% if content['type'] == 'text' %}{{ content['text'] }}{% endif %}\n",
      "{% endfor %}\n",
      "<end_of_turn>\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "{% if add_generation_prompt %}\n",
      "<start_of_turn>model\n",
      "{% endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current chat template: {processor.tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vision_info(messages):\n",
    "    \"\"\"\n",
    "    Extracts images from a structured messages list.\n",
    "    Returns a list of PIL Image objects in RGB format.\n",
    "    \"\"\"\n",
    "    image_inputs = []\n",
    "    # print(f\"Processing messages: {messages}\")\n",
    "    \n",
    "    for msg in messages:\n",
    "        content = msg.get(\"content\", [])\n",
    "        # print(f\"Message content: {content}\")\n",
    "        \n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "        \n",
    "        for element in content:\n",
    "            # print(f\"Checking element: {type(element)}\")\n",
    "            if isinstance(element, dict) and (\n",
    "                \"image\" in element or element.get(\"type\") == \"image\"\n",
    "            ):\n",
    "                # print(\"Found image element!\")\n",
    "                if \"image\" in element:\n",
    "                    image = element[\"image\"]\n",
    "                else:\n",
    "                    image = element\n",
    "                \n",
    "                if hasattr(image, 'convert'):\n",
    "                    image = image.convert(\"RGB\")\n",
    "                    image_inputs.append(image)\n",
    "                    # print(f\"Added image: {image.size}\")\n",
    "                else:\n",
    "                    # print(f\"Element is not a PIL image: {type(image)}\")\n",
    "                    pass\n",
    "    \n",
    "    # print(f\"Total images found: {len(image_inputs)}\")\n",
    "    return image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_image():\n",
    "    \"\"\"Create a small black image as a placeholder.\"\"\"\n",
    "    return Image.new('RGB', (224, 224), color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image> token ID: 262145\n"
     ]
    }
   ],
   "source": [
    "# Ensure <image> is properly registered as a special token\n",
    "if \"<image>\" not in processor.tokenizer.get_vocab():\n",
    "    # Add the token to the vocabulary\n",
    "    processor.tokenizer.add_special_tokens({'additional_special_tokens': ['<image>']})\n",
    "    # VERY IMPORTANT: Resize the model's embeddings to match the new vocabulary size\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Check what token ID is assigned to <image>\n",
    "image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "print(f\"<image> token ID: {image_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images_per_example = []\n",
    "    \n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        if not image_inputs:\n",
    "            print(f\"Using dummy image — Example roles: {[m['role'] for m in example['messages']]}\")\n",
    "            image_inputs = [create_dummy_image()]\n",
    "        \n",
    "        # Apply chat template with hardcoded <image> token\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Count image tokens explicitly\n",
    "        num_image_tokens = text.count(\"<image>\")\n",
    "        \n",
    "        # Ensure number of images matches tokens\n",
    "        if len(image_inputs) < num_image_tokens:\n",
    "            needed_dummies = num_image_tokens - len(image_inputs)\n",
    "            image_inputs += [create_dummy_image()] * needed_dummies\n",
    "        elif len(image_inputs) > num_image_tokens:\n",
    "            # Never truncate to zero\n",
    "            image_inputs = image_inputs[:max(1, num_image_tokens)]\n",
    "        \n",
    "        texts.append(text.strip())\n",
    "        images_per_example.append(image_inputs)\n",
    "    \n",
    "    # Use the processor directly as in the documentation\n",
    "    batch = processor(\n",
    "        text=texts, \n",
    "        images=images_per_example,  # Pass as list of lists\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Create labels for training\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get image token ID the same way the model does\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "    \n",
    "    # Mask tokens that shouldn't contribute to the loss\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10\n",
      "Sampling 3 examples from dataset\n",
      "Sample size: 3\n",
      "First example keys: ['messages']\n",
      "Collated batch contains: ['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values', 'labels']\n",
      "Input_ids shape: torch.Size([3, 1359])\n",
      "Labels shape: torch.Size([3, 1359])\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "    pass\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"Collated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch with 8 examples\n",
      "Processed batch with 2 examples\n",
      "Processed all 10 examples\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    # Process each batch. Note: in training, pass this to model.forward()\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUGGING TEMPLATE APPLICATION ===\n",
      "\n",
      "Example 0:\n",
      "Message structure:\n",
      "  Message 0 role: system\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Message 1 role: user\n",
      "  Content types: ['text', 'image', 'image']\n",
      "  Image content count: 2\n",
      "  Message 2 role: assistant\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Image tokens in text: 2\n",
      "  Text preview: <start_of_turn>system\n",
      "You are an AI assistant answering medical questions based on images.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Question: How much of the body is affected? Options: 1. single spot, 2. limi...\n",
      "\n",
      "Example 1:\n",
      "Message structure:\n",
      "  Message 0 role: system\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Message 1 role: user\n",
      "  Content types: ['text', 'image', 'image', 'image', 'image']\n",
      "  Image content count: 4\n",
      "  Message 2 role: assistant\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Image tokens in text: 4\n",
      "  Text preview: <start_of_turn>system\n",
      "You are an AI assistant answering medical questions based on images.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Question: How much of the body is affected? Options: 1. single spot, 2. limi...\n",
      "\n",
      "Example 2:\n",
      "Message structure:\n",
      "  Message 0 role: system\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Message 1 role: user\n",
      "  Content types: ['text', 'image', 'image', 'image', 'image', 'image']\n",
      "  Image content count: 5\n",
      "  Message 2 role: assistant\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Image tokens in text: 5\n",
      "  Text preview: <start_of_turn>system\n",
      "You are an AI assistant answering medical questions based on images.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Question: How much of the body is affected? Options: 1. single spot, 2. limi...\n"
     ]
    }
   ],
   "source": [
    "def debug_template_application(examples):\n",
    "    print(\"\\n=== DEBUGGING TEMPLATE APPLICATION ===\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        \n",
    "        # First, check message structure\n",
    "        print(\"Message structure:\")\n",
    "        for j, msg in enumerate(example[\"messages\"]):\n",
    "            role = msg.get(\"role\", \"unknown\")\n",
    "            print(f\"  Message {j} role: {role}\")\n",
    "            \n",
    "            content = msg.get(\"content\", [])\n",
    "            if not isinstance(content, list):\n",
    "                content = [content]\n",
    "                \n",
    "            print(f\"  Content types: {[c.get('type') if isinstance(c, dict) else type(c).__name__ for c in content]}\")\n",
    "            \n",
    "            # Check specifically for image content\n",
    "            image_count = sum(1 for c in content if isinstance(c, dict) and c.get('type') == 'image')\n",
    "            print(f\"  Image content count: {image_count}\")\n",
    "        \n",
    "        # Next, check template application\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Count image tokens in generated text\n",
    "        img_token_count = text.count(\"<image>\")\n",
    "        print(f\"  Image tokens in text: {img_token_count}\")\n",
    "        \n",
    "        # Print text snippet to see if tokens appear\n",
    "        print(f\"  Text preview: {text[:200]}...\")\n",
    "        \n",
    "        # Check if there's a mismatch\n",
    "        if img_token_count == 0:\n",
    "            print(\"  WARNING: No image tokens found in text!\")\n",
    "            \n",
    "            # Examine raw content in more detail\n",
    "            print(\"  Detailed content examination:\")\n",
    "            for j, msg in enumerate(example[\"messages\"]):\n",
    "                if msg.get(\"role\") == \"user\":\n",
    "                    content = msg.get(\"content\", [])\n",
    "                    if not isinstance(content, list):\n",
    "                        content = [content]\n",
    "                    \n",
    "                    for k, item in enumerate(content):\n",
    "                        if isinstance(item, dict):\n",
    "                            print(f\"    Item {k}: type={item.get('type')}, keys={list(item.keys())}\")\n",
    "                            if item.get('type') == 'image':\n",
    "                                print(f\"    Found image content item\")\n",
    "                            else:\n",
    "                                print(f\"    Item has type={item.get('type')}, not 'image'\")\n",
    "                                \n",
    "debug_template_application(sample_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-product-description\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,  # Critical for custom datasets\n",
    "    label_names=[\"labels\"],  # Explicitly setting label_names\n",
    ")\n",
    "\n",
    "# args.remove_unused_columns = False # Not needed in our case but leaving it in in case the loaded data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test with a small batch\n",
    "# test_indices = list(range(min(3, len(dataset))))\n",
    "# test_batch = [dataset[i] for i in test_indices]\n",
    "\n",
    "# Now initialize your actual trainer with the regular collate_fn\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=19.036113739013672, metrics={'train_runtime': 67.0704, 'train_samples_per_second': 0.149, 'train_steps_per_second': 0.03, 'total_flos': 174718733364480.0, 'train_loss': 19.036113739013672})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you can test your model, make sure to free the memory.\n",
    "\n",
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
