{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.pop(\"TRANSFORMERS_CACHE\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\"\n",
    "\n",
    "# where the downloaded model goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.executable)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = os.path.join(\"2025_dataset\", \"train\", \"final_df.csv\")\n",
    "train_images_dir = os.path.join(\"2025_dataset\", \"train\", \"images_train\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "\n",
    "train_df['image_ids'] = train_df['image_ids'].apply(eval)\n",
    "train_df['responses_en'] = train_df['responses_en'].apply(eval)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids']]\n",
    "\n",
    "print(f\"Filtered dataframe shape: {train_df.shape}\")\n",
    "print(\"Columns:\", train_df.columns.tolist())\n",
    "\n",
    "display(\"Sample row:\", train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            image_paths = [os.path.join(train_images_dir, img_id) for img_id in row['image_ids']]\n",
    "            \n",
    "            if not all(os.path.exists(img_path) for img_path in image_paths):\n",
    "                continue\n",
    "\n",
    "            valid_images = []\n",
    "            for img_path in image_paths:\n",
    "                try:\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.load()\n",
    "                    valid_images.append(img_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Corrupt or unreadable image at {img_path} — {e}\")\n",
    "\n",
    "            if len(valid_images) != len(image_paths):\n",
    "                continue\n",
    "            \n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(eval(row['options_en']))])\n",
    "            query_text = f\"Question: {row['question_en']} Options: {options_text}\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"encounter_id\": row['encounter_id'],\n",
    "                \"qid\": row['qid'],\n",
    "                \"query_text\": query_text,\n",
    "                \"image_paths\": valid_images,\n",
    "                \"answer_text\": row['answer_text']\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        images = [Image.open(path).convert(\"RGB\") for path in example['image_paths']]\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"You are an AI assistant answering medical questions based on images.\"}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    *[{\"type\": \"image\", \"image\": img} for img in images],\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"processed_data\"):\n",
    "    total_processed = 0\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"processed_data_debug\"\n",
    "\n",
    "if os.path.exists(processed_data_dir):\n",
    "    shutil.rmtree(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples = preprocess_dataset(train_df, batch_size=500, save_dir=processed_data_dir)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[0]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "print(f\"Number of images: {len(sample_data['image_paths'])}\")\n",
    "print(f\"Image paths: {sample_data['image_paths']}\")\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[1]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "print(f\"Number of images: {len(sample_data['image_paths'])}\")\n",
    "print(f\"Image paths: {sample_data['image_paths']}\")\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[2]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "print(f\"Number of images: {len(sample_data['image_paths'])}\")\n",
    "print(f\"Image paths: {sample_data['image_paths']}\")\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[0]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[1]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[2]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16. Use a different GPU.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs, token=hf_token)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current chat template: {processor.tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_chat_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}\n",
    "<start_of_turn>user\n",
    "{% for content in message['content'] %}\n",
    "{% if content['type'] == 'text' %}{{ content['text'] }}{% elif content['type'] == 'image' %}<image>{% endif %}\n",
    "{% endfor %}\n",
    "<end_of_turn>\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "<start_of_turn>model\n",
    "{% for content in message['content'] %}\n",
    "{% if content['type'] == 'text' %}{{ content['text'] }}{% endif %}\n",
    "{% endfor %}\n",
    "<end_of_turn>\n",
    "{% elif message['role'] == 'system' %}\n",
    "<start_of_turn>system\n",
    "{% for content in message['content'] %}\n",
    "{% if content['type'] == 'text' %}{{ content['text'] }}{% endif %}\n",
    "{% endfor %}\n",
    "<end_of_turn>\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "<start_of_turn>model\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "\n",
    "# Set template\n",
    "processor.tokenizer.chat_template = gemma_chat_template\n",
    "processor.chat_template = gemma_chat_template\n",
    "\n",
    "# Ensure <image> is recognized as a token\n",
    "if \"<image>\" not in processor.tokenizer.get_vocab():\n",
    "    processor.tokenizer.add_special_tokens({'additional_special_tokens': ['<image>']})\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Set boi_token for template rendering\n",
    "processor.boi_token = \"<image>\"\n",
    "processor.tokenizer.special_tokens_map['boi_token'] = \"<image>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current chat template: {processor.tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vision_info(messages):\n",
    "    \"\"\"\n",
    "    Extracts images from a structured messages list.\n",
    "    Returns a list of PIL Image objects in RGB format.\n",
    "    \"\"\"\n",
    "    image_inputs = []\n",
    "    # print(f\"Processing messages: {messages}\")\n",
    "    \n",
    "    for msg in messages:\n",
    "        content = msg.get(\"content\", [])\n",
    "        # print(f\"Message content: {content}\")\n",
    "        \n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "        \n",
    "        for element in content:\n",
    "            # print(f\"Checking element: {type(element)}\")\n",
    "            if isinstance(element, dict) and (\n",
    "                \"image\" in element or element.get(\"type\") == \"image\"\n",
    "            ):\n",
    "                # print(\"Found image element!\")\n",
    "                if \"image\" in element:\n",
    "                    image = element[\"image\"]\n",
    "                else:\n",
    "                    image = element\n",
    "                \n",
    "                if hasattr(image, 'convert'):\n",
    "                    image = image.convert(\"RGB\")\n",
    "                    image_inputs.append(image)\n",
    "                    # print(f\"Added image: {image.size}\")\n",
    "                else:\n",
    "                    # print(f\"Element is not a PIL image: {type(image)}\")\n",
    "                    pass\n",
    "    \n",
    "    # print(f\"Total images found: {len(image_inputs)}\")\n",
    "    return image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_image():\n",
    "    \"\"\"Create a small black image as a placeholder.\"\"\"\n",
    "    return Image.new('RGB', (224, 224), color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure <image> is properly registered as a special token\n",
    "if \"<image>\" not in processor.tokenizer.get_vocab():\n",
    "    # Add the token to the vocabulary\n",
    "    processor.tokenizer.add_special_tokens({'additional_special_tokens': ['<image>']})\n",
    "    # VERY IMPORTANT: Resize the model's embeddings to match the new vocabulary size\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Check what token ID is assigned to <image>\n",
    "image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "print(f\"<image> token ID: {image_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images_per_example = []\n",
    "    \n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        if not image_inputs:\n",
    "            print(f\"Using dummy image — Example roles: {[m['role'] for m in example['messages']]}\")\n",
    "            image_inputs = [create_dummy_image()]\n",
    "        \n",
    "        # Apply chat template with hardcoded <image> token\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Count image tokens explicitly\n",
    "        num_image_tokens = text.count(\"<image>\")\n",
    "        \n",
    "        # Ensure number of images matches tokens\n",
    "        if len(image_inputs) < num_image_tokens:\n",
    "            needed_dummies = num_image_tokens - len(image_inputs)\n",
    "            image_inputs += [create_dummy_image()] * needed_dummies\n",
    "        elif len(image_inputs) > num_image_tokens:\n",
    "            # Never truncate to zero\n",
    "            image_inputs = image_inputs[:max(1, num_image_tokens)]\n",
    "        \n",
    "        texts.append(text.strip())\n",
    "        images_per_example.append(image_inputs)\n",
    "    \n",
    "    # Use the processor directly as in the documentation\n",
    "    batch = processor(\n",
    "        text=texts, \n",
    "        images=images_per_example,  # Pass as list of lists\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Create labels for training\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get image token ID the same way the model does\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "    \n",
    "    # Mask tokens that shouldn't contribute to the loss\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "    pass\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"Collated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    # Process each batch. Note: in training, pass this to model.forward()\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_template_application(examples):\n",
    "    print(\"\\n=== DEBUGGING TEMPLATE APPLICATION ===\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        \n",
    "        # First, check message structure\n",
    "        print(\"Message structure:\")\n",
    "        for j, msg in enumerate(example[\"messages\"]):\n",
    "            role = msg.get(\"role\", \"unknown\")\n",
    "            print(f\"  Message {j} role: {role}\")\n",
    "            \n",
    "            content = msg.get(\"content\", [])\n",
    "            if not isinstance(content, list):\n",
    "                content = [content]\n",
    "                \n",
    "            print(f\"  Content types: {[c.get('type') if isinstance(c, dict) else type(c).__name__ for c in content]}\")\n",
    "            \n",
    "            # Check specifically for image content\n",
    "            image_count = sum(1 for c in content if isinstance(c, dict) and c.get('type') == 'image')\n",
    "            print(f\"  Image content count: {image_count}\")\n",
    "        \n",
    "        # Next, check template application\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Count image tokens in generated text\n",
    "        img_token_count = text.count(\"<image>\")\n",
    "        print(f\"  Image tokens in text: {img_token_count}\")\n",
    "        \n",
    "        # Print text snippet to see if tokens appear\n",
    "        print(f\"  Text preview: {text[:200]}...\")\n",
    "        \n",
    "        # Check if there's a mismatch\n",
    "        if img_token_count == 0:\n",
    "            print(\"  WARNING: No image tokens found in text!\")\n",
    "            \n",
    "            # Examine raw content in more detail\n",
    "            print(\"  Detailed content examination:\")\n",
    "            for j, msg in enumerate(example[\"messages\"]):\n",
    "                if msg.get(\"role\") == \"user\":\n",
    "                    content = msg.get(\"content\", [])\n",
    "                    if not isinstance(content, list):\n",
    "                        content = [content]\n",
    "                    \n",
    "                    for k, item in enumerate(content):\n",
    "                        if isinstance(item, dict):\n",
    "                            print(f\"    Item {k}: type={item.get('type')}, keys={list(item.keys())}\")\n",
    "                            if item.get('type') == 'image':\n",
    "                                print(f\"    Found image content item\")\n",
    "                            else:\n",
    "                                print(f\"    Item has type={item.get('type')}, not 'image'\")\n",
    "                                \n",
    "debug_template_application(sample_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-product-description\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,  # Critical for custom datasets\n",
    "    label_names=[\"labels\"],  # Explicitly setting label_names\n",
    ")\n",
    "\n",
    "# args.remove_unused_columns = False # Not needed in our case but leaving it in in case the loaded data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test with a small batch\n",
    "# test_indices = list(range(min(3, len(dataset))))\n",
    "# test_batch = [dataset[i] for i in test_indices]\n",
    "\n",
    "# Now initialize your actual trainer with the regular collate_fn\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you can test your model, make sure to free the memory.\n",
    "\n",
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
