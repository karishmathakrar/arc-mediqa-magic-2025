{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.51.0  # Use this specific version to avoid embedding mismatch bug\n",
    "!pip install -U datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U peft\n",
    "!pip install -U trl\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face token (from environment variable or enter directly)\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")  # Or set it directly here\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Choose your model\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"  # Or other Llama model\n",
    "\n",
    "# Configure quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset - replace with your actual data source\n",
    "dataset = load_dataset(\"your_dataset_name_or_path\")\n",
    "\n",
    "# Define formatting function for your data\n",
    "def formatting_prompts_func(examples):\n",
    "    # Adjust based on your dataset structure\n",
    "    inputs = examples[\"input_column\"]\n",
    "    outputs = examples[\"output_column\"]\n",
    "    \n",
    "    # Format according to Llama chat template\n",
    "    texts = []\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        # Add EOS token if needed\n",
    "        if not output_text.endswith(tokenizer.eos_token):\n",
    "            output_text += tokenizer.eos_token\n",
    "            \n",
    "        # Create messages format\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": input_text},\n",
    "            {\"role\": \"assistant\", \"content\": output_text}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Process dataset\n",
    "processed_dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,  # Reduced from 64 to save memory\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments optimized for A100 40GB\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./finetuned-llama\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,  # Simulates larger batch size\n",
    "    optim=\"adamw_torch_fused\",  # or \"paged_adamw_32bit\"\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,  # Reduced from 2e-4 for stability\n",
    "    fp16=True,  # Use this for A100\n",
    "    bf16=False,  # Only use bf16 if you have adequate hardware support\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",  # Changed from constant for better convergence\n",
    "    group_by_length=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Create train/validation split if not already done\n",
    "if \"validation\" not in processed_dataset:\n",
    "    processed_dataset = processed_dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45076a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./finetuned-llama-final\")\n",
    "tokenizer.save_pretrained(\"./finetuned-llama-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_path = \"./finetuned-llama-final\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Test with a sample prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Your test prompt here\"}\n",
    "]\n",
    "outputs = pipe(messages, max_new_tokens=128, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae3317",
   "metadata": {},
   "source": [
    "Key Differences from Failed Examples\n",
    "Looking at the Reddit post about the \"terrible llama 3.2 vision finetune,\" here are some improvements I've incorporated in the code above:\n",
    "\n",
    "1) Improved LoRA Configuration: The failed example used r=8 with only [\"q_proj\", \"v_proj\"] as target modules. I've increased to r=16 and targeted more modules.\n",
    "2) Learning Rate: Lowered from 2e-4 to 1e-4 for better stability.\n",
    "3) Learning Schedule: Changed from constant to cosine learning rate schedule.\n",
    "4) Validation Integration: Added proper validation split to monitor overfitting.\n",
    "5) Batch Processing: Optimized batch size and gradient accumulation for your hardware.\n",
    "\n",
    "These changes should help address the overfitting issues mentioned in the Reddit post while making efficient use of your A100's memory.\n",
    "For working specifically with vision models, you would need to make further adjustments to handle image inputs, but this code provides the foundation for text-based fine-tuning of Llama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb268b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
