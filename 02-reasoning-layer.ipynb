{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "918bc4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import json\n",
    "import io\n",
    "import traceback\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28df1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, use_finetuning=True, use_test_dataset=True):\n",
    "        \"\"\"\n",
    "        Initialize arguments with options for dataset and model type.\n",
    "        \n",
    "        Parameters:\n",
    "        - use_finetuning: Whether to use the fine-tuned model predictions (True) or base model predictions (False)\n",
    "        - use_test_dataset: Whether to use the test dataset (True) or validation dataset (False)\n",
    "        \"\"\"\n",
    "        self.use_finetuning = use_finetuning\n",
    "        self.use_test_dataset = use_test_dataset\n",
    "        \n",
    "        # Base directory paths\n",
    "        self.base_dir = os.getcwd()\n",
    "        self.output_dir = os.path.join(self.base_dir, \"outputs\")\n",
    "        self.model_predictions_dir = os.path.join(self.output_dir, \"05022025\")\n",
    "        \n",
    "        # Set paths based on dataset type\n",
    "        if self.use_test_dataset:\n",
    "            self.dataset_name = \"test\"\n",
    "            self.dataset_path = os.path.join(self.output_dir, \"test_dataset.csv\")\n",
    "            self.images_dir = os.path.join(self.base_dir, \"2025_dataset\", \"test\", \"images_test\")\n",
    "            self.prediction_prefix = \"aggregated_test_predictions_\"\n",
    "        else:\n",
    "            self.dataset_name = \"validation\"\n",
    "            self.dataset_path = os.path.join(self.output_dir, \"val_dataset.csv\")\n",
    "            self.images_dir = os.path.join(self.base_dir, \"2025_dataset\", \"valid\", \"images_valid\")\n",
    "            self.prediction_prefix = \"aggregated_predictions_\"\n",
    "        \n",
    "        # Set model type suffix\n",
    "        self.model_type = \"finetuned\" if self.use_finetuning else \"base\"\n",
    "        \n",
    "        # Other configurations\n",
    "        self.gemini_model = \"gemini-2.5-flash-preview-04-17\"\n",
    "        \n",
    "        print(f\"\\nConfiguration initialized:\")\n",
    "        print(f\"- Using {'test' if self.use_test_dataset else 'validation'} dataset\")\n",
    "        print(f\"- Looking for {self.model_type} model predictions\")\n",
    "        print(f\"- Dataset path: {self.dataset_path}\")\n",
    "        print(f\"- Images directory: {self.images_dir}\")\n",
    "        print(f\"- Prediction file prefix: {self.prediction_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf1a1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def get_latest_aggregated_files(args):\n",
    "        \"\"\"Get the latest aggregated prediction files for each model.\"\"\"\n",
    "        # Use the appropriate pattern based on args\n",
    "        pattern = os.path.join(args.model_predictions_dir, f\"{args.prediction_prefix}*_{args.model_type}_*.csv\")\n",
    "        print(f\"Searching for files with pattern: {pattern}\")\n",
    "        \n",
    "        agg_files = glob.glob(pattern)\n",
    "        print(f\"Found {len(agg_files)} aggregated prediction files\")\n",
    "        \n",
    "        if len(agg_files) == 0:\n",
    "            return []\n",
    "        \n",
    "        latest_files = {}\n",
    "        \n",
    "        for file_path in agg_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(f\"_{args.model_type}_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            model_part = parts[0].replace(args.prediction_prefix, \"\")\n",
    "            model_name = model_part\n",
    "            \n",
    "            timestamps = re.findall(r'(\\d+)', parts[1])\n",
    "            if len(timestamps) < 2:\n",
    "                print(f\"Warning: Could not find timestamps in {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            timestamp = int(timestamps[1])\n",
    "            \n",
    "            if model_name not in latest_files or timestamp > latest_files[model_name]['timestamp']:\n",
    "                latest_files[model_name] = {\n",
    "                    'file_path': file_path,\n",
    "                    'timestamp': timestamp\n",
    "                }\n",
    "        \n",
    "        print(\"\\nSelected latest file for each model:\")\n",
    "        for model, info in latest_files.items():\n",
    "            print(f\"  {model}: {os.path.basename(info['file_path'])}\")\n",
    "        \n",
    "        return [info['file_path'] for _, info in latest_files.items()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_model_predictions(args):\n",
    "        \"\"\"Load all model predictions from aggregated files.\"\"\"\n",
    "        latest_files = DataLoader.get_latest_aggregated_files(args)\n",
    "        \n",
    "        if not latest_files:\n",
    "            print(\"No aggregated prediction files found. Cannot proceed.\")\n",
    "            return {}\n",
    "        \n",
    "        model_predictions = {}\n",
    "        \n",
    "        for file_path in latest_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(f\"_{args.model_type}_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            model_name = parts[0].replace(args.prediction_prefix, \"\")\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                model_predictions[model_name] = df\n",
    "                \n",
    "                print(f\"Successfully loaded {model_name} predictions with {len(df)} rows\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(model_predictions)} model prediction sets\")\n",
    "        return model_predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_validation_dataset(args):\n",
    "        \"\"\"Load the dataset based on args configuration.\"\"\"\n",
    "        print(f\"Loading {args.dataset_name} dataset from {args.dataset_path}\")\n",
    "        df = pd.read_csv(args.dataset_path)\n",
    "        print(f\"Loaded {args.dataset_name} dataset with {len(df)} rows\")\n",
    "        \n",
    "        df = DataLoader.process_validation_dataset(df)\n",
    "        \n",
    "        encounter_question_data = defaultdict(lambda: {\n",
    "            'images': [],\n",
    "            'data': None\n",
    "        })\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            encounter_id = row['encounter_id']\n",
    "            base_qid = row['base_qid']\n",
    "            key = (encounter_id, base_qid)\n",
    "            \n",
    "            if 'image_path' in row and row['image_path']:\n",
    "                encounter_question_data[key]['images'].append(row['image_path'])\n",
    "            elif 'image_id' in row and row['image_id']:\n",
    "                image_path = os.path.join(args.images_dir, row['image_id'])\n",
    "                encounter_question_data[key]['images'].append(image_path)\n",
    "            \n",
    "            if encounter_question_data[key]['data'] is None:\n",
    "                encounter_question_data[key]['data'] = row.to_dict()\n",
    "        \n",
    "        print(f\"Created grouped {args.dataset_name} dataset with {len(encounter_question_data)} unique encounter-question pairs\")\n",
    "        \n",
    "        grouped_data = []\n",
    "        for (encounter_id, base_qid), data in encounter_question_data.items():\n",
    "            entry = data['data'].copy()\n",
    "            entry['all_images'] = data['images']\n",
    "            entry['encounter_id'] = encounter_id\n",
    "            entry['base_qid'] = base_qid\n",
    "            grouped_data.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(grouped_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_convert_options(options_str):\n",
    "        \"\"\"Safely convert a string representation of a list to an actual list.\"\"\"\n",
    "        if not isinstance(options_str, str):\n",
    "            return options_str\n",
    "            \n",
    "        try:\n",
    "            return ast.literal_eval(options_str)\n",
    "        except (SyntaxError, ValueError):\n",
    "            if options_str.startswith('[') and options_str.endswith(']'):\n",
    "                return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "            elif ',' in options_str:\n",
    "                return [opt.strip() for opt in options_str.split(',')]\n",
    "            else:\n",
    "                return [options_str]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_validation_dataset(val_df):\n",
    "        \"\"\"Process and clean the validation dataset.\"\"\"\n",
    "        if 'options_en' in val_df.columns:\n",
    "            val_df['options_en'] = val_df['options_en'].apply(DataLoader.safe_convert_options)\n",
    "            \n",
    "            def clean_options(options):\n",
    "                if not isinstance(options, list):\n",
    "                    return options\n",
    "                    \n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                return cleaned_options\n",
    "                \n",
    "            val_df['options_en_cleaned'] = val_df['options_en'].apply(clean_options)\n",
    "        \n",
    "        if 'question_text' in val_df.columns:\n",
    "            val_df['question_text_cleaned'] = val_df['question_text'].apply(\n",
    "                lambda q: q.replace(\" Please specify which affected area for each selection.\", \"\") \n",
    "                          if isinstance(q, str) and \"Please specify which affected area for each selection\" in q \n",
    "                          else q\n",
    "            )\n",
    "            \n",
    "            val_df['question_text_cleaned'] = val_df['question_text_cleaned'].apply(\n",
    "                lambda q: re.sub(r'^\\d+\\s+', '', q) if isinstance(q, str) else q\n",
    "            )\n",
    "        \n",
    "        if 'base_qid' not in val_df.columns and 'qid' in val_df.columns:\n",
    "            val_df['base_qid'] = val_df['qid'].apply(\n",
    "                lambda q: q.split('-')[0] if isinstance(q, str) and '-' in q else q\n",
    "            )\n",
    "        \n",
    "#         print(val_df)\n",
    "        return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f87f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def create_query_context(row):\n",
    "        \"\"\"Create query context from validation data similar to the inference process.\"\"\"\n",
    "        question = row.get('question_text_cleaned', row.get('question_text', 'What do you see in this image?'))\n",
    "        \n",
    "        metadata = \"\"\n",
    "        if 'question_type_en' in row:\n",
    "            metadata += f\"Type: {row['question_type_en']}\"\n",
    "            \n",
    "        if 'question_category_en' in row:\n",
    "            metadata += f\", Category: {row['question_category_en']}\"\n",
    "        \n",
    "        query_title = row.get('query_title_en', '')\n",
    "        query_content = row.get('query_content_en', '')\n",
    "        \n",
    "        clinical_context = \"\"\n",
    "        if query_title or query_content:\n",
    "            clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "            if query_title:\n",
    "                clinical_context += f\"{query_title}\\n\"\n",
    "            if query_content:\n",
    "                clinical_context += f\"{query_content}\\n\"\n",
    "        \n",
    "        options = row.get('options_en_cleaned', row.get('options_en', ['Yes', 'No', 'Not mentioned']))\n",
    "        if isinstance(options, list):\n",
    "            options_text = \", \".join(options)\n",
    "        else:\n",
    "            options_text = str(options)\n",
    "        \n",
    "        query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                     f\"Question Metadata: {metadata}\\n\"\n",
    "                     f\"{clinical_context}\"\n",
    "                     f\"Available Options (choose from these): {options_text}\")\n",
    "        \n",
    "#         print(query_text)\n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77972148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGData:\n",
    "    def __init__(self, all_models_df, validation_df):\n",
    "        self.all_models_df = all_models_df\n",
    "        self.validation_df = validation_df\n",
    "        \n",
    "        self.model_predictions = {}\n",
    "        for (encounter_id, base_qid), group in all_models_df.groupby(['encounter_id', 'base_qid']):\n",
    "            self.model_predictions[(encounter_id, base_qid)] = group\n",
    "        \n",
    "        self.validation_data = {}\n",
    "        for _, row in validation_df.iterrows():\n",
    "            self.validation_data[(row['encounter_id'], row['base_qid'])] = row\n",
    "    \n",
    "    def get_combined_data(self, encounter_id, base_qid):\n",
    "        \"\"\"Retrieve combined data for a specific encounter and question.\"\"\"\n",
    "        model_preds = self.model_predictions.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        val_data = self.validation_data.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        if model_preds is None:\n",
    "            print(f\"No model predictions found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "            \n",
    "        if val_data is None:\n",
    "            print(f\"No validation data found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "        \n",
    "        if 'query_context' not in val_data:\n",
    "            val_data['query_context'] = DataProcessor.create_query_context(val_data)\n",
    "        \n",
    "        model_predictions_dict = {}\n",
    "        for _, row in model_preds.iterrows():\n",
    "            model_name = row['model_name']\n",
    "            \n",
    "            model_predictions_dict[model_name] = self._process_model_predictions(row)\n",
    "        \n",
    "        return {\n",
    "            'encounter_id': encounter_id,\n",
    "            'base_qid': base_qid,\n",
    "            'query_context': val_data['query_context'],\n",
    "            'images': val_data.get('all_images', []),\n",
    "            'options': val_data.get('options_en_cleaned', val_data.get('options_en', [])),\n",
    "            'question_type': val_data.get('question_type_en', ''),\n",
    "            'question_category': val_data.get('question_category_en', ''),\n",
    "            'model_predictions': model_predictions_dict\n",
    "        }\n",
    "    \n",
    "    def _process_model_predictions(self, row):\n",
    "        \"\"\"Process model predictions from row data.\"\"\"\n",
    "#         unique_preds = row.get('unique_predictions', [])\n",
    "#         if isinstance(unique_preds, str):\n",
    "#             try:\n",
    "#                 unique_preds = ast.literal_eval(unique_preds)\n",
    "#             except:\n",
    "#                 unique_preds = [unique_preds]\n",
    "                \n",
    "#         raw_preds = row.get('all_raw_predictions', [])\n",
    "#         if isinstance(raw_preds, str):\n",
    "#             try:\n",
    "#                 raw_preds = ast.literal_eval(raw_preds)\n",
    "#             except:\n",
    "#                 raw_preds = [raw_preds]\n",
    "                \n",
    "#         sorted_preds = row.get('all_sorted_predictions', [])\n",
    "#         if isinstance(sorted_preds, str):\n",
    "#             try:\n",
    "#                 sorted_preds = ast.literal_eval(sorted_preds)\n",
    "#             except:\n",
    "#                 sorted_preds = [(str(raw_preds[0]), 1)] if raw_preds else []\n",
    "        \n",
    "        return {\n",
    "            'model_prediction': row.get('combined_prediction', ''),\n",
    "#             'unique_predictions': unique_preds,\n",
    "#             'all_raw_predictions': raw_preds,\n",
    "#             'all_sorted_predictions': sorted_preds\n",
    "        }\n",
    "    \n",
    "    def get_all_encounter_question_pairs(self):\n",
    "        \"\"\"Return a list of all unique encounter_id, base_qid pairs.\"\"\"\n",
    "        return list(self.validation_data.keys())\n",
    "    \n",
    "    def get_sample_data(self, n=5):\n",
    "        \"\"\"Get a sample of combined data for n random encounter-question pairs.\"\"\"\n",
    "        import random\n",
    "        \n",
    "        all_pairs = self.get_all_encounter_question_pairs()\n",
    "        sample_pairs = random.sample(all_pairs, min(n, len(all_pairs)))\n",
    "        \n",
    "        return [self.get_combined_data(encounter_id, base_qid) for encounter_id, base_qid in sample_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e690fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisService:\n",
    "    def __init__(self, api_key=None, args=None):\n",
    "        if api_key is None:\n",
    "            load_dotenv()\n",
    "            api_key = os.getenv(\"API_KEY\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.args = args\n",
    "    \n",
    "    def extract_dermatological_analysis(self, sample_data):\n",
    "        \"\"\"\n",
    "        Extract structured analysis of images for an encounter.\n",
    "        \n",
    "        Args:\n",
    "            sample_data: Dictionary containing encounter data with images\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured dermatological analysis\n",
    "        \"\"\"\n",
    "        encounter_id = sample_data['encounter_id']\n",
    "        image_paths = sample_data['images']\n",
    "        \n",
    "        image_analyses = []\n",
    "        \n",
    "        structured_prompt = self._create_dermatology_prompt()\n",
    "        \n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            analysis = self._analyze_single_image(\n",
    "                img_path, \n",
    "                structured_prompt, \n",
    "                encounter_id, \n",
    "                idx, \n",
    "                len(image_paths)\n",
    "            )\n",
    "            image_analyses.append(analysis)\n",
    "        \n",
    "        aggregated_analysis = self._aggregate_analyses(image_analyses, encounter_id)\n",
    "        \n",
    "        return {\n",
    "            \"encounter_id\": encounter_id,\n",
    "            \"image_count\": len(image_paths),\n",
    "            \"individual_analyses\": image_analyses,\n",
    "            \"aggregated_analysis\": aggregated_analysis\n",
    "        }\n",
    "    \n",
    "    def _create_dermatology_prompt(self):\n",
    "        \"\"\"Create the structured dermatology analysis prompt.\"\"\"\n",
    "        return \"\"\"As dermatology specialist analyzing skin images, extract and structure all clinically relevant information from this dermatological image.\n",
    "\n",
    "Organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is cannot be determined, state \"Cannot determine from image\".\n",
    "\"\"\"\n",
    "    \n",
    "    def _analyze_single_image(self, img_path, prompt, encounter_id, idx, total_images):\n",
    "        \"\"\"Analyze a single dermatological image.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "#             print(f\"Analyzing image {idx+1}/{total_images} for encounter {encounter_id}\")\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt, image]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "#             print(f\"Analysis text received (length: {len(analysis_text)})\")\n",
    "            \n",
    "            structured_analysis = self._parse_json_response(analysis_text)\n",
    "#             print(f\"Successfully parsed structured analysis for image {idx+1}\")\n",
    "            \n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"structured_analysis\": structured_analysis\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image {img_path}: {str(e)}\")\n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}\n",
    "    \n",
    "    def _aggregate_analyses(self, image_analyses, encounter_id):\n",
    "        \"\"\"Aggregate structured analyses from multiple images.\"\"\"\n",
    "        valid_analyses = [a for a in image_analyses if \"error\" not in a and \"structured_analysis\" in a]\n",
    "#         print(f\"Aggregating {len(valid_analyses)} valid structured analyses for encounter {encounter_id}\")\n",
    "        \n",
    "        if not valid_analyses:\n",
    "            return {\n",
    "                \"error\": \"No valid analyses to aggregate\",\n",
    "                \"message\": \"Unable to generate aggregated analysis due to errors in individual analyses.\"\n",
    "            }\n",
    "        \n",
    "        if len(valid_analyses) == 1:\n",
    "            return valid_analyses[0][\"structured_analysis\"]\n",
    "        \n",
    "        analysis_jsons = []\n",
    "        for analysis in valid_analyses:\n",
    "            analysis_json = json.dumps(analysis[\"structured_analysis\"])\n",
    "            analysis_jsons.append(f\"Image {analysis['image_index']} ({analysis['image_path']}): {analysis_json}\")\n",
    "        \n",
    "        aggregation_prompt = self._create_aggregation_prompt(analysis_jsons)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[aggregation_prompt]\n",
    "            )\n",
    "            \n",
    "            aggregation_text = response.text\n",
    "#             print(f\"Aggregated analysis received (length: {len(aggregation_text)})\")\n",
    "            \n",
    "            aggregated_analysis = self._parse_json_response(aggregation_text)\n",
    "#             print(\"Successfully parsed aggregated analysis\")\n",
    "            \n",
    "#             print(aggregated_analysis)\n",
    "            \n",
    "            return aggregated_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating aggregated analysis for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"aggregation_error\": \"Failed to generate aggregated analysis\"\n",
    "            }\n",
    "    \n",
    "    def _create_aggregation_prompt(self, analysis_jsons):\n",
    "        \"\"\"Create a prompt for aggregating multiple image analyses.\"\"\"\n",
    "        return f\"\"\"As dermatology specialist reviewing multiple skin image analyses for the same patient, combine these analyses and organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "    \n",
    "{' '.join(analysis_jsons)}\n",
    "\"\"\"\n",
    "    \n",
    "    def extract_clinical_context(self, sample_data):\n",
    "        \"\"\"\n",
    "        Extract structured clinical information from an encounter's query context.\n",
    "        \n",
    "        Args:\n",
    "            sample_data: Dictionary containing encounter data with query_context\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured clinical information\n",
    "        \"\"\"\n",
    "        encounter_id = sample_data['encounter_id']\n",
    "        \n",
    "        query_context = sample_data['query_context']\n",
    "        \n",
    "        clinical_text = self._extract_clinical_text(query_context)\n",
    "        \n",
    "        if not clinical_text:\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"clinical_summary\": \"No clinical information available\"\n",
    "            }\n",
    "        \n",
    "        prompt = self._create_clinical_context_prompt(clinical_text)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"structured_clinical_context\": response.text\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting clinical context for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_clinical_text(self, query_context):\n",
    "        \"\"\"Extract clinical text from query context.\"\"\"\n",
    "        clinical_lines = []\n",
    "        capturing = False\n",
    "        for line in query_context.split('\\n'):\n",
    "            if \"Background Clinical Information\" in line:\n",
    "                capturing = True\n",
    "                continue\n",
    "            elif \"Available Options\" in line:\n",
    "                capturing = False\n",
    "            elif capturing:\n",
    "                clinical_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(clinical_lines).strip()\n",
    "    \n",
    "    def _create_clinical_context_prompt(self, clinical_text):\n",
    "        \"\"\"Create prompt for extracting structured clinical information.\"\"\"\n",
    "        return f\"\"\"You are a dermatology specialist analyzing patient information. \n",
    "Extract and structure all clinically relevant information from this patient description:\n",
    "\n",
    "{clinical_text}\n",
    "\n",
    "Organize your response in the following JSON structure:\n",
    "\n",
    "1. DEMOGRAPHICS: Age, sex, and any other demographic data\n",
    "2. SITE_LOCATION: Body parts affected by the condition as described in the text\n",
    "3. SKIN_DESCRIPTION: Any mention of lesion morphology (flat, raised, depressed), texture, surface characteristics (scales, crust, fluid), appearance of lesion boundaries\n",
    "4. LESION_COLOR: Any description of color(s) of affected areas, color variations, comparison to normal skin\n",
    "5. LESION_COUNT: Any information about number of lesions, single vs multiple presentation, distribution pattern\n",
    "6. EXTENT: How widespread the condition appears based on the description, localized vs widespread\n",
    "7. TEXTURE: Any description of tactile qualities, smooth vs rough, notable textural features\n",
    "8. ONSET_INDICATORS: Information about onset, duration, progression, or evolution of symptoms\n",
    "9. ITCH_INDICATORS: Mentions of scratching, itchiness, or other sensory symptoms\n",
    "10. OTHER_SYMPTOMS: Any additional symptoms mentioned (pain, burning, etc.)\n",
    "11. TRIGGERS: Identified factors that worsen/improve the condition\n",
    "12. HISTORY: Relevant past medical history or previous treatments\n",
    "13. DIAGNOSTIC_CONSIDERATIONS: Any mentioned or suggested diagnoses in the text\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is \n",
    "not available, indicate \"Not mentioned\".\n",
    "\"\"\"\n",
    "    \n",
    "    def apply_reasoning_layer(self, encounter_id, base_qid, image_analysis, clinical_context, sample_data):\n",
    "        \"\"\"\n",
    "        Apply a reasoning layer to determine the best answer(s) for a specific encounter-question pair.\n",
    "        \n",
    "        Args:\n",
    "            encounter_id: The encounter ID\n",
    "            base_qid: The question ID\n",
    "            image_analysis: Structured image analysis for this encounter\n",
    "            clinical_context: Structured clinical context for this encounter\n",
    "            sample_data: Combined data for this encounter-question pair\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with reasoning and final answer(s)\n",
    "        \"\"\"\n",
    "        question_text = sample_data['query_context'].split(\"MAIN QUESTION TO ANSWER:\")[1].split(\"\\n\")[0].strip()\n",
    "        options = sample_data['options']\n",
    "        question_type = sample_data['question_type']\n",
    "        model_predictions = sample_data['model_predictions']\n",
    "        \n",
    "        model_prediction_text = self._format_model_predictions(model_predictions)\n",
    "        \n",
    "        prompt = self._create_reasoning_prompt(\n",
    "            question_text, \n",
    "            question_type, \n",
    "            options, \n",
    "            image_analysis, \n",
    "            clinical_context, \n",
    "            model_prediction_text\n",
    "        )\n",
    "        \n",
    "#         print(\"\\n==== REASONING PROMPT ====\")\n",
    "#         print(prompt)\n",
    "#         print(\"==========================\\n\")\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reasoning_text = response.text\n",
    "            \n",
    "#             print(\"\\n==== RAW LLM RESPONSE ====\")\n",
    "#             print(reasoning_text)\n",
    "#             print(\"===========================\\n\")\n",
    "            \n",
    "            reasoning_result = self._parse_json_response(reasoning_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(reasoning_result.get('answer', ''), options)\n",
    "            reasoning_result['validated_answer'] = validated_answer\n",
    "            \n",
    "#             print(\"\\n==== PROCESSED REASONING RESULT ====\")\n",
    "#             print(json.dumps(reasoning_result, indent=2))\n",
    "#             print(\"====================================\\n\")\n",
    "            \n",
    "            return reasoning_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying reasoning layer for {encounter_id}, {base_qid}: {str(e)}\")\n",
    "            return {\n",
    "                \"reasoning\": f\"Error: {str(e)}\",\n",
    "                \"answer\": \"Not mentioned\",\n",
    "                \"validated_answer\": \"Not mentioned\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _format_model_predictions(self, model_predictions):\n",
    "        \"\"\"Format model predictions for the prompt.\"\"\"\n",
    "        model_prediction_text = \"\"\n",
    "        for model_name, predictions in model_predictions.items():\n",
    "            combined_pred = predictions.get('model_prediction', '')\n",
    "            if isinstance(combined_pred, float) and pd.isna(combined_pred):\n",
    "                combined_pred = \"No prediction\"\n",
    "            model_prediction_text += f\"- {model_name}: {combined_pred}\\n\"\n",
    "#         print(model_prediction_text)\n",
    "        return model_prediction_text\n",
    "\n",
    "    def _create_reasoning_prompt(self, question_text, question_type, options, image_analysis, clinical_context, model_prediction_text):\n",
    "        \"\"\"Create a prompt for the reasoning layer.\"\"\"\n",
    "#         print(\"\\n--- Question:\", question_text)\n",
    "#         print(\"--- Question Type:\", question_type)\n",
    "#         print(\"--- Options:\", \", \".join(options))\n",
    "#         print(\"--- Image Analysis:\", json.dumps(image_analysis.get('aggregated_analysis', {}), indent=2)[:300] + \"...\" if len(json.dumps(image_analysis.get('aggregated_analysis', {}))) > 300 else json.dumps(image_analysis.get('aggregated_analysis', {})))\n",
    "#         print(\"--- Clinical Context:\", clinical_context.get('structured_clinical_context', '')[:300] + \"...\" if len(clinical_context.get('structured_clinical_context', '')) > 300 else clinical_context.get('structured_clinical_context', ''))\n",
    "#         print(\"--- Model Predictions:\", model_prediction_text)\n",
    "\n",
    "        specialized_guidance = \"\"\n",
    "        include_clinical_context = True\n",
    "\n",
    "        multiple_answers_allowed = question_type in [\"Site Location\", \"Size\", \"Skin Description\"]\n",
    "\n",
    "        if multiple_answers_allowed:\n",
    "            task_description = \"\"\"Based on all the evidence above, determine the most accurate answer(s) to the question. Your task is to:\n",
    "    1. Analyze the evidence from the image analysis{0}\n",
    "    2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "    3. Provide a brief reasoning for your conclusion\n",
    "    4. Select the final answer(s) from the available options\n",
    "\n",
    "    If selecting multiple answers is appropriate, provide them in a comma-separated list. If no answer can be determined, select \"Not mentioned\".\"\"\".format(' and clinical context' if include_clinical_context else '')\n",
    "        else:\n",
    "            task_description = \"\"\"Based on all the evidence above, determine the SINGLE most accurate answer to the question. Your task is to:\n",
    "    1. Analyze the evidence from the image analysis{0}\n",
    "    2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "    3. Provide a brief reasoning for your conclusion\n",
    "    4. Select ONLY ONE answer option that is most accurate\n",
    "\n",
    "    For this question type, you must select ONLY ONE option as your answer. If no answer can be determined, select \"Not mentioned\".\"\"\".format(' and clinical context' if include_clinical_context else '')\n",
    "\n",
    "        if question_type == \"Size\" and all(option in \", \".join(options) for option in [\"size of thumb nail\", \"size of palm\", \"larger area\"]):\n",
    "            specialized_guidance = \"\"\"\n",
    "    SPECIALIZED GUIDANCE FOR SIZE ASSESSMENT:\n",
    "    When answering this size-related question, interpret the options as follows:\n",
    "    - \"size of thumb nail\": Individual lesions or affected areas approximately 1-2 cm in diameter\n",
    "    - \"size of palm\": Affected areas larger than the size of a thumb nail and roughly the size of a palm (approximately 1% of body surface area), which may include multiple smaller lesions across a region\n",
    "    - \"larger area\": Widespread involvement significantly larger than a palm, affecting a substantial portion(s) of the body\n",
    "\n",
    "    IMPORTANT: For cases with multiple small lesions that are visible in the images, but without extensive widespread involvement across large body regions, \"size of palm\" is likely the most appropriate answer.\n",
    "\n",
    "    Base your assessment PRIMARILY on the current state shown in the IMAGES and their analysis, not on descriptions of progression or potential future spread mentioned in the clinical context. Prioritize what you can directly observe in the image analysis over clinical descriptions.\n",
    "    \"\"\"\n",
    "            include_clinical_context = False\n",
    "\n",
    "        elif question_type == \"Lesion Color\" and \"combination\" in \", \".join(options):\n",
    "            specialized_guidance = \"\"\"\n",
    "    SPECIALIZED GUIDANCE FOR LESION COLOR:\n",
    "    When answering color-related questions, pay careful attention to whether there are multiple distinct colors present across the affected areas. \"Combination\" would be appropriate when different lesions display different colors (e.g., some lesions appear red while others appear white), or when individual lesions show mixed or varied coloration patterns.\n",
    "    \"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert analyzing dermatological images. Use the provided evidence to determine the most accurate answer(s) for the following question:\n",
    "\n",
    "    QUESTION: {question_text}\n",
    "    QUESTION TYPE: {question_type}\n",
    "    OPTIONS: {\", \".join(options)}\n",
    "\n",
    "    IMAGE ANALYSIS:\n",
    "    {json.dumps(image_analysis['aggregated_analysis'], indent=2)}\n",
    "    \"\"\"\n",
    "\n",
    "        if include_clinical_context:\n",
    "            base_prompt += f\"\"\"\n",
    "    CLINICAL CONTEXT:\n",
    "    {clinical_context['structured_clinical_context']}\n",
    "    \"\"\"\n",
    "        else:\n",
    "            base_prompt += \"\"\"\n",
    "    NOTE: For this question type, the analysis is based primarily on image evidence rather than clinical descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "        return base_prompt + f\"\"\"\n",
    "    MODEL PREDICTIONS:\n",
    "    {model_prediction_text}\n",
    "\n",
    "    {specialized_guidance}\n",
    "\n",
    "    IMPORTANT: While multiple model predictions are provided, be aware that these predictions can be inaccurate or inconsistent. Do not assume majority agreement equals correctness. Evaluate the evidence critically and independently from these predictions. Your job is to determine the correct answer based primarily on the image analysis, treating model predictions as secondary suggestions that may contain errors.\n",
    "\n",
    "    {task_description}\n",
    "\n",
    "    Format your response as a JSON object with these fields:\n",
    "    1. \"reasoning\": Your step-by-step reasoning process\n",
    "    2. \"answer\": Your final answer(s) as a single string or comma-separated list of options\n",
    "\n",
    "    When providing your answer, strictly adhere to the available options and only select from them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "#         print(valid_answers)\n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8d771b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DermatologyPipeline:\n",
    "    def __init__(self, analysis_service):\n",
    "        self.analysis_service = analysis_service\n",
    "    \n",
    "    def process_single_encounter(self, agentic_data, encounter_id):\n",
    "        \"\"\"\n",
    "        Process a single encounter with all its questions using the reasoning layer.\n",
    "        \n",
    "        Args:\n",
    "            agentic_data: AgenticRAGData instance containing all encounter data\n",
    "            encounter_id: The specific encounter ID to process\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all questions processed with reasoning for this encounter\n",
    "        \"\"\"\n",
    "#         print(\"Printing everything in Dermatology Pipeline now\")\n",
    "        \n",
    "        all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "        encounter_pairs = [pair for pair in all_pairs if pair[0] == encounter_id]\n",
    "        \n",
    "        if not encounter_pairs:\n",
    "            print(f\"No data found for encounter {encounter_id}\")\n",
    "            return None\n",
    "        \n",
    "#         print(f\"Processing {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "        \n",
    "        encounter_results = {encounter_id: {}}\n",
    "        \n",
    "#         print(f\"Computing image analysis for {encounter_id}\")\n",
    "        sample_data = agentic_data.get_combined_data(encounter_pairs[0][0], encounter_pairs[0][1])\n",
    "#         print(sample_data)\n",
    "        image_analysis = self.analysis_service.extract_dermatological_analysis(sample_data)\n",
    "#         print(image_analysis)\n",
    "        \n",
    "#         print(f\"Computing clinical context for {encounter_id}\")\n",
    "        clinical_context = self.analysis_service.extract_clinical_context(sample_data)\n",
    "#         print(clinical_context)\n",
    "        \n",
    "        for i, (encounter_id, base_qid) in enumerate(encounter_pairs):\n",
    "#             print(f\"Processing question {i+1}/{len(encounter_pairs)}: {base_qid}\")\n",
    "            \n",
    "            sample_data = agentic_data.get_combined_data(encounter_id, base_qid)\n",
    "#             print(sample_data)\n",
    "            if not sample_data:\n",
    "                print(f\"Warning: No data found for {encounter_id}, {base_qid}\")\n",
    "                continue\n",
    "            \n",
    "#             print(f\"Applying reasoning layer for {encounter_id}, {base_qid}\")\n",
    "            reasoning_result = self.analysis_service.apply_reasoning_layer(\n",
    "                encounter_id,\n",
    "                base_qid,\n",
    "                image_analysis,\n",
    "                clinical_context,\n",
    "                sample_data\n",
    "            )\n",
    "#             print(reasoning_result)\n",
    "            \n",
    "            encounter_results[encounter_id][base_qid] = {\n",
    "                \"query_context\": sample_data['query_context'],\n",
    "                \"options\": sample_data['options'],\n",
    "                \"model_predictions\": sample_data['model_predictions'],\n",
    "                \"reasoning_result\": reasoning_result,\n",
    "                \"final_answer\": reasoning_result.get('validated_answer', 'Not mentioned')\n",
    "            }\n",
    "#             print(encounter_results)\n",
    "        \n",
    "#         output_file = os.path.join(Config.OUTPUT_DIR, f\"reasoning_results_{encounter_id}.json\")\n",
    "#         with open(output_file, \"w\") as f:\n",
    "#             json.dump(encounter_results, f, indent=2)\n",
    "            \n",
    "        output_file = os.path.join(self.analysis_service.args.output_dir, f\"reasoning_results_{encounter_id}.json\")\n",
    "    \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(encounter_results, f, indent=2)\n",
    "        \n",
    "#         print(f\"Processed all {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "        return encounter_results\n",
    "    \n",
    "    def format_results_for_evaluation(self, encounter_results, output_file):\n",
    "        \"\"\"Format results for official evaluation.\"\"\"\n",
    "        QIDS = [\n",
    "            \"CQID010-001\",\n",
    "            \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",\n",
    "            \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",\n",
    "            \"CQID015-001\",\n",
    "            \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "            \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",\n",
    "            \"CQID025-001\",\n",
    "            \"CQID034-001\",\n",
    "            \"CQID035-001\",\n",
    "            \"CQID036-001\",\n",
    "        ]\n",
    "        \n",
    "        qid_variants = {}\n",
    "        for qid in QIDS:\n",
    "            base_qid, variant = qid.split('-')\n",
    "            if base_qid not in qid_variants:\n",
    "                qid_variants[base_qid] = []\n",
    "            qid_variants[base_qid].append(qid)\n",
    "        \n",
    "        required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "        \n",
    "        formatted_predictions = []\n",
    "        for encounter_id, questions in encounter_results.items():\n",
    "            encounter_base_qids = set(questions.keys())\n",
    "            if not required_base_qids.issubset(encounter_base_qids):\n",
    "                print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "                continue\n",
    "            \n",
    "            pred_entry = {'encounter_id': encounter_id}\n",
    "            \n",
    "            for base_qid, question_data in questions.items():\n",
    "                if base_qid not in qid_variants:\n",
    "                    continue\n",
    "                \n",
    "                final_answer = question_data['final_answer']\n",
    "                options = question_data['options']\n",
    "                \n",
    "                not_mentioned_index = self._find_not_mentioned_index(options)\n",
    "                \n",
    "                self._process_answers(\n",
    "                    pred_entry, \n",
    "                    base_qid, \n",
    "                    final_answer, \n",
    "                    options, \n",
    "                    qid_variants, \n",
    "                    not_mentioned_index\n",
    "                )\n",
    "            \n",
    "            formatted_predictions.append(pred_entry)\n",
    "#             print(formatted_predictions)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(formatted_predictions, f, indent=2)\n",
    "        \n",
    "#         print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "#         print(formatted_predictions)\n",
    "        return formatted_predictions\n",
    "    \n",
    "    def _find_not_mentioned_index(self, options):\n",
    "        \"\"\"Find the index of 'Not mentioned' in options.\"\"\"\n",
    "        for i, opt in enumerate(options):\n",
    "            if opt.lower() == \"not mentioned\":\n",
    "                return i\n",
    "        return len(options) - 1\n",
    "    \n",
    "    def _process_answers(self, pred_entry, base_qid, final_answer, options, qid_variants, not_mentioned_index):\n",
    "        \"\"\"Process answers and add to prediction entry.\"\"\"\n",
    "        if ',' in final_answer:\n",
    "            answer_parts = [part.strip() for part in final_answer.split(',')]\n",
    "            answer_indices = []\n",
    "            \n",
    "            for part in answer_parts:\n",
    "                found = False\n",
    "                for i, opt in enumerate(options):\n",
    "                    if part.lower() == opt.lower():\n",
    "                        answer_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found:\n",
    "                    answer_indices.append(not_mentioned_index)\n",
    "            \n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            for i, idx in enumerate(answer_indices):\n",
    "                if i < len(available_variants):\n",
    "                    pred_entry[available_variants[i]] = idx\n",
    "            \n",
    "            for i in range(len(answer_indices), len(available_variants)):\n",
    "                pred_entry[available_variants[i]] = not_mentioned_index\n",
    "            \n",
    "        else:\n",
    "            answer_index = not_mentioned_index\n",
    "            \n",
    "            for i, opt in enumerate(options):\n",
    "                if final_answer.lower() == opt.lower():\n",
    "                    answer_index = i\n",
    "                    break\n",
    "            \n",
    "            pred_entry[qid_variants[base_qid][0]] = answer_index\n",
    "            \n",
    "            if len(qid_variants[base_qid]) > 1:\n",
    "                for i in range(1, len(qid_variants[base_qid])):\n",
    "                    pred_entry[qid_variants[base_qid][i]] = not_mentioned_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "749be10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_encounters_pipeline(args=None):\n",
    "    \"\"\"Run the pipeline for all available encounters and combine the results.\"\"\"\n",
    "    if args is None:\n",
    "        args = Args(use_finetuning=True, use_test_dataset=True)\n",
    "    \n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(args)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    dataset_df = DataLoader.load_validation_dataset(args)\n",
    "    agentic_data = AgenticRAGData(all_models_df, dataset_df)\n",
    "    \n",
    "    all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "    unique_encounter_ids = sorted(list(set(pair[0] for pair in all_pairs)))\n",
    "    print(f\"Found {len(unique_encounter_ids)} unique encounters to process\")\n",
    "    \n",
    "    analysis_service = AnalysisService(args=args)\n",
    "    \n",
    "    pipeline = DermatologyPipeline(analysis_service)\n",
    "    \n",
    "    all_encounter_results = {}\n",
    "    for i, encounter_id in enumerate(unique_encounter_ids):\n",
    "        print(f\"Processing encounter {i+1}/{len(unique_encounter_ids)}: {encounter_id}...\")\n",
    "        encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "        if encounter_results:\n",
    "            all_encounter_results.update(encounter_results)\n",
    "        \n",
    "        if (i+1) % 5 == 0 or (i+1) == len(unique_encounter_ids):\n",
    "            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            intermediate_output_file = os.path.join(\n",
    "                args.output_dir, \n",
    "                f\"intermediate_results_{i+1}_of_{len(unique_encounter_ids)}_{timestamp}.json\"\n",
    "            )\n",
    "            with open(intermediate_output_file, 'w') as f:\n",
    "                json.dump(all_encounter_results, f, indent=2)\n",
    "            print(f\"Saved intermediate results after processing {i+1} encounters\")\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(\n",
    "        args.output_dir, \n",
    "        f\"{args.dataset_name}_data_cvqa_sys_reasoned_all_{timestamp}.json\"\n",
    "    )\n",
    "    \n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(all_encounter_results, output_file)\n",
    "    \n",
    "    print(f\"Processed {len(formatted_predictions)} encounters successfully\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50e01e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_encounter_pipeline(encounter_id, args=None):\n",
    "    \"\"\"Run the pipeline for a single encounter.\"\"\"\n",
    "    if args is None:\n",
    "        args = Args(use_finetuning=True, use_test_dataset=True)\n",
    "    \n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(args)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    \n",
    "    dataset_df = DataLoader.load_validation_dataset(args)\n",
    "    agentic_data = AgenticRAGData(all_models_df, dataset_df)\n",
    "    \n",
    "    analysis_service = AnalysisService(args=args)\n",
    "    \n",
    "    pipeline = DermatologyPipeline(analysis_service)\n",
    "    encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "    \n",
    "    output_file = os.path.join(\n",
    "        args.output_dir, \n",
    "        f\"{args.dataset_name}_data_cvqa_sys_reasoned_{encounter_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    )\n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(encounter_results, output_file)\n",
    "    \n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d1d6adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration initialized:\n",
      "- Using test dataset\n",
      "- Looking for finetuned model predictions\n",
      "- Dataset path: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/test_dataset.csv\n",
      "- Images directory: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/2025_dataset/test/images_test\n",
      "- Prediction file prefix: aggregated_test_predictions_\n",
      "Searching for files with pattern: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/05022025/aggregated_test_predictions_*_finetuned_*.csv\n",
      "Found 7 aggregated prediction files\n",
      "\n",
      "Selected latest file for each model:\n",
      "  gemma-3-12b-it: aggregated_test_predictions_gemma-3-12b-it_finetuned_20250501_214322_20250501_214322.csv\n",
      "  Qwen2-VL-7B-Instruct: aggregated_test_predictions_Qwen2-VL-7B-Instruct_finetuned_20250501_175720_20250501_175720.csv\n",
      "  Llama-3.2-11B-Vision-Instruct: aggregated_test_predictions_Llama-3.2-11B-Vision-Instruct_finetuned_20250502_105951_20250502_105951.csv\n",
      "  gemma-3-4b-it: aggregated_test_predictions_gemma-3-4b-it_finetuned_20250501_220152_20250501_220152.csv\n",
      "  Qwen2.5-VL-7B-Instruct: aggregated_test_predictions_Qwen2.5-VL-7B-Instruct_finetuned_20250501_180704_20250501_180704.csv\n",
      "  Qwen2-VL-2B-Instruct: aggregated_test_predictions_Qwen2-VL-2B-Instruct_finetuned_20250501_165844_20250501_165844.csv\n",
      "  Qwen2.5-VL-3B-Instruct: aggregated_test_predictions_Qwen2.5-VL-3B-Instruct_finetuned_20250501_175205_20250501_175205.csv\n",
      "Successfully loaded gemma-3-12b-it predictions with 900 rows\n",
      "Successfully loaded Qwen2-VL-7B-Instruct predictions with 900 rows\n",
      "Successfully loaded Llama-3.2-11B-Vision-Instruct predictions with 900 rows\n",
      "Successfully loaded gemma-3-4b-it predictions with 900 rows\n",
      "Successfully loaded Qwen2.5-VL-7B-Instruct predictions with 900 rows\n",
      "Successfully loaded Qwen2-VL-2B-Instruct predictions with 900 rows\n",
      "Successfully loaded Qwen2.5-VL-3B-Instruct predictions with 900 rows\n",
      "Loaded 7 model prediction sets\n",
      "Loading test dataset from /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/test_dataset.csv\n",
      "Loaded test dataset with 2826 rows\n",
      "Created grouped test dataset with 900 unique encounter-question pairs\n",
      "Processed encounter ENC00908 with 1 prediction entries\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create args with desired configuration\n",
    "    args = Args(use_finetuning=True, use_test_dataset=True)\n",
    "    \n",
    "    # For testing a single encounter\n",
    "    encounter_id = \"ENC00908\"\n",
    "    formatted_predictions = run_single_encounter_pipeline(encounter_id, args)\n",
    "    print(f\"Processed encounter {encounter_id} with {len(formatted_predictions)} prediction entries\")\n",
    "    \n",
    "    # Alternatively, uncomment this to run all encounters\n",
    "    # formatted_predictions = run_all_encounters_pipeline(args)\n",
    "    # print(f\"Total complete encounters processed: {len(formatted_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff361877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'encounter_id': 'ENC00908',\n",
       "  'CQID010-001': 1,\n",
       "  'CQID011-001': 3,\n",
       "  'CQID011-002': 2,\n",
       "  'CQID011-003': 7,\n",
       "  'CQID011-004': 7,\n",
       "  'CQID011-005': 7,\n",
       "  'CQID011-006': 7,\n",
       "  'CQID012-001': 0,\n",
       "  'CQID012-002': 1,\n",
       "  'CQID012-003': 3,\n",
       "  'CQID012-004': 3,\n",
       "  'CQID012-005': 3,\n",
       "  'CQID012-006': 3,\n",
       "  'CQID015-001': 3,\n",
       "  'CQID020-001': 0,\n",
       "  'CQID020-002': 1,\n",
       "  'CQID020-003': 3,\n",
       "  'CQID020-004': 6,\n",
       "  'CQID020-005': 7,\n",
       "  'CQID020-006': 9,\n",
       "  'CQID020-007': 9,\n",
       "  'CQID020-008': 9,\n",
       "  'CQID020-009': 9,\n",
       "  'CQID025-001': 0,\n",
       "  'CQID034-001': 8,\n",
       "  'CQID035-001': 1,\n",
       "  'CQID036-001': 2}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fae7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
