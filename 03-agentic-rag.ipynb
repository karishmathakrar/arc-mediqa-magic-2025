{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39808731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import numpy as np\n",
    "import lancedb\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cd9944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, use_finetuning=True, use_test_dataset=True):\n",
    "        \"\"\"\n",
    "        Initialize arguments with options for dataset and model type.\n",
    "        \n",
    "        Parameters:\n",
    "        - use_finetuning: Whether to use the fine-tuned model predictions (True) or base model predictions (False)\n",
    "        - use_test_dataset: Whether to use the test dataset (True) or validation dataset (False)\n",
    "        \"\"\"\n",
    "        self.use_finetuning = use_finetuning\n",
    "        self.use_test_dataset = use_test_dataset\n",
    "        \n",
    "        # Base directory paths\n",
    "        self.base_dir = os.getcwd()\n",
    "        self.output_dir = os.path.join(self.base_dir, \"outputs\")\n",
    "        self.model_predictions_dir = os.path.join(self.output_dir, \"05022025\")\n",
    "        \n",
    "        # Set paths based on dataset type\n",
    "        if self.use_test_dataset:\n",
    "            self.dataset_name = \"test\"\n",
    "            self.dataset_path = os.path.join(self.output_dir, \"test_dataset.csv\")\n",
    "            self.images_dir = os.path.join(self.base_dir, \"2025_dataset\", \"test\", \"images_test\")\n",
    "            self.prediction_prefix = \"aggregated_test_predictions_\"\n",
    "        else:\n",
    "            self.dataset_name = \"validation\"\n",
    "            self.dataset_path = os.path.join(self.output_dir, \"val_dataset.csv\")\n",
    "            self.images_dir = os.path.join(self.base_dir, \"2025_dataset\", \"valid\", \"images_valid\")\n",
    "            self.prediction_prefix = \"aggregated_predictions_\"\n",
    "        \n",
    "        # Set model type suffix\n",
    "        self.model_type = \"finetuned\" if self.use_finetuning else \"base\"\n",
    "        \n",
    "        # Other configurations\n",
    "        self.gemini_model = \"gemini-2.5-flash-preview-04-17\"\n",
    "        \n",
    "        # Reflection configurations\n",
    "        self.max_reflection_cycles = 2\n",
    "        self.confidence_threshold = 0.75  # Threshold for accepting an answer without reflection\n",
    "        \n",
    "        # RAG-specific configurations\n",
    "        self.knowledge_db_path = os.path.join(self.base_dir, \"knowledge_db\")\n",
    "        self.embedding_model = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "        self.cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "        self.vector_dimension = 768\n",
    "        self.top_k_semantic = 7\n",
    "        self.top_k_keyword = 7\n",
    "        self.top_k_hybrid = 10\n",
    "        self.top_k_rerank = 5\n",
    "        \n",
    "        # Dataset\n",
    "        self.dataset_name_huggingface = \"brucewayne0459/Skin_diseases_and_care\"\n",
    "        \n",
    "        # Question type configurations for RAG\n",
    "        self.question_type_retrieval_config = {\n",
    "            \"Site Location\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "            \"Lesion Color\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "            \"Size\": {\"use_rag\": False, \"weight\": 0.1},\n",
    "            \"Skin Description\": {\"use_rag\": True, \"weight\": 0.3},\n",
    "            \"Onset\": {\"use_rag\": True, \"weight\": 0.4},\n",
    "            \"Itch\": {\"use_rag\": True, \"weight\": 0.4},\n",
    "            \"Extent\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "            \"Treatment\": {\"use_rag\": True, \"weight\": 0.7},\n",
    "            \"Lesion Evolution\": {\"use_rag\": True, \"weight\": 0.5},\n",
    "            \"Texture\": {\"use_rag\": True, \"weight\": 0.3},\n",
    "            \"Lesion Count\": {\"use_rag\": False, \"weight\": 0.1},\n",
    "            \"Differential\": {\"use_rag\": True, \"weight\": 0.8},\n",
    "            \"Specific Diagnosis\": {\"use_rag\": True, \"weight\": 0.8},\n",
    "        }\n",
    "        \n",
    "        # Default for question types not explicitly listed\n",
    "        self.default_rag_config = {\"use_rag\": True, \"weight\": 0.4}\n",
    "        \n",
    "        print(f\"\\nConfiguration initialized:\")\n",
    "        print(f\"- Using {'test' if self.use_test_dataset else 'validation'} dataset\")\n",
    "        print(f\"- Looking for {self.model_type} model predictions\")\n",
    "        print(f\"- Dataset path: {self.dataset_path}\")\n",
    "        print(f\"- Images directory: {self.images_dir}\")\n",
    "        print(f\"- Prediction file prefix: {self.prediction_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1584e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def get_latest_aggregated_files(args):\n",
    "        \"\"\"Get the latest aggregated prediction files for each model.\"\"\"\n",
    "        # Use the appropriate pattern based on args\n",
    "        pattern = os.path.join(args.model_predictions_dir, f\"{args.prediction_prefix}*_{args.model_type}_*.csv\")\n",
    "        \n",
    "        agg_files = glob.glob(pattern)\n",
    "        \n",
    "        if len(agg_files) == 0:\n",
    "            return []\n",
    "        \n",
    "        latest_files = {}\n",
    "        \n",
    "        for file_path in agg_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(f\"_{args.model_type}_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            model_part = parts[0].replace(args.prediction_prefix, \"\")\n",
    "            model_name = model_part\n",
    "            \n",
    "            timestamps = re.findall(r'(\\d+)', parts[1])\n",
    "            if len(timestamps) < 2:\n",
    "                print(f\"Warning: Could not find timestamps in {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            timestamp = int(timestamps[1])\n",
    "            \n",
    "            if model_name not in latest_files or timestamp > latest_files[model_name]['timestamp']:\n",
    "                latest_files[model_name] = {\n",
    "                    'file_path': file_path,\n",
    "                    'timestamp': timestamp\n",
    "                }\n",
    "        \n",
    "        return [info['file_path'] for _, info in latest_files.items()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_model_predictions(args):\n",
    "        \"\"\"Load all model predictions from aggregated files.\"\"\"\n",
    "        latest_files = DataLoader.get_latest_aggregated_files(args)\n",
    "        \n",
    "        if not latest_files:\n",
    "            print(\"No aggregated prediction files found. Cannot proceed.\")\n",
    "            return {}\n",
    "        \n",
    "        model_predictions = {}\n",
    "        \n",
    "        for file_path in latest_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(f\"_{args.model_type}_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            model_name = parts[0].replace(args.prediction_prefix, \"\")\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                model_predictions[model_name] = df\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        return model_predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def load_validation_dataset(args):\n",
    "        \"\"\"Load the validation dataset.\"\"\"\n",
    "        val_df = pd.read_csv(args.dataset_path)\n",
    "        \n",
    "        val_df = DataLoader.process_validation_dataset(val_df)\n",
    "        \n",
    "        encounter_question_data = defaultdict(lambda: {\n",
    "            'images': [],\n",
    "            'data': None\n",
    "        })\n",
    "        \n",
    "        for _, row in val_df.iterrows():\n",
    "            encounter_id = row['encounter_id']\n",
    "            base_qid = row['base_qid']\n",
    "            key = (encounter_id, base_qid)\n",
    "            \n",
    "            if 'image_path' in row and row['image_path']:\n",
    "                encounter_question_data[key]['images'].append(row['image_path'])\n",
    "            elif 'image_id' in row and row['image_id']:\n",
    "                image_path = os.path.join(args.images_dir, row['image_id'])\n",
    "                encounter_question_data[key]['images'].append(image_path)\n",
    "            \n",
    "            if encounter_question_data[key]['data'] is None:\n",
    "                encounter_question_data[key]['data'] = row.to_dict()\n",
    "        \n",
    "        grouped_data = []\n",
    "        for (encounter_id, base_qid), data in encounter_question_data.items():\n",
    "            entry = data['data'].copy()\n",
    "            entry['all_images'] = data['images']\n",
    "            entry['encounter_id'] = encounter_id\n",
    "            entry['base_qid'] = base_qid\n",
    "            grouped_data.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(grouped_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_convert_options(options_str):\n",
    "        \"\"\"Safely convert a string representation of a list to an actual list.\"\"\"\n",
    "        if not isinstance(options_str, str):\n",
    "            return options_str\n",
    "            \n",
    "        try:\n",
    "            return ast.literal_eval(options_str)\n",
    "        except (SyntaxError, ValueError):\n",
    "            if options_str.startswith('[') and options_str.endswith(']'):\n",
    "                return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "            elif ',' in options_str:\n",
    "                return [opt.strip() for opt in options_str.split(',')]\n",
    "            else:\n",
    "                return [options_str]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_validation_dataset(val_df):\n",
    "        \"\"\"Process and clean the validation dataset.\"\"\"\n",
    "        if 'options_en' in val_df.columns:\n",
    "            val_df['options_en'] = val_df['options_en'].apply(DataLoader.safe_convert_options)\n",
    "            \n",
    "            def clean_options(options):\n",
    "                if not isinstance(options, list):\n",
    "                    return options\n",
    "                    \n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                return cleaned_options\n",
    "                \n",
    "            val_df['options_en_cleaned'] = val_df['options_en'].apply(clean_options)\n",
    "        \n",
    "        if 'question_text' in val_df.columns:\n",
    "            val_df['question_text_cleaned'] = val_df['question_text'].apply(\n",
    "                lambda q: q.replace(\" Please specify which affected area for each selection.\", \"\") \n",
    "                          if isinstance(q, str) and \"Please specify which affected area for each selection\" in q \n",
    "                          else q\n",
    "            )\n",
    "            \n",
    "            val_df['question_text_cleaned'] = val_df['question_text_cleaned'].apply(\n",
    "                lambda q: re.sub(r'^\\d+\\s+', '', q) if isinstance(q, str) else q\n",
    "            )\n",
    "        \n",
    "        if 'base_qid' not in val_df.columns and 'qid' in val_df.columns:\n",
    "            val_df['base_qid'] = val_df['qid'].apply(\n",
    "                lambda q: q.split('-')[0] if isinstance(q, str) and '-' in q else q\n",
    "            )\n",
    "        \n",
    "#         print(val_df)\n",
    "        return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f144174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def create_query_context(row, args=None):\n",
    "        \"\"\"Create query context from validation data similar to the inference process.\"\"\"\n",
    "        question = row.get('question_text_cleaned', row.get('question_text', 'What do you see in this image?'))\n",
    "        \n",
    "        metadata = \"\"\n",
    "        if 'question_type_en' in row:\n",
    "            metadata += f\"Type: {row['question_type_en']}\"\n",
    "            \n",
    "        if 'question_category_en' in row:\n",
    "            metadata += f\", Category: {row['question_category_en']}\"\n",
    "        \n",
    "        query_title = row.get('query_title_en', '')\n",
    "        query_content = row.get('query_content_en', '')\n",
    "        \n",
    "        clinical_context = \"\"\n",
    "        if query_title or query_content:\n",
    "            clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "            if query_title:\n",
    "                clinical_context += f\"{query_title}\\n\"\n",
    "            if query_content:\n",
    "                clinical_context += f\"{query_content}\\n\"\n",
    "        \n",
    "        options = row.get('options_en_cleaned', row.get('options_en', ['Yes', 'No', 'Not mentioned']))\n",
    "        if isinstance(options, list):\n",
    "            options_text = \", \".join(options)\n",
    "        else:\n",
    "            options_text = str(options)\n",
    "        \n",
    "        query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                     f\"Question Metadata: {metadata}\\n\"\n",
    "                     f\"{clinical_context}\"\n",
    "                     f\"Available Options (choose from these): {options_text}\")\n",
    "        \n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f7d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGData:\n",
    "    def __init__(self, all_models_df, validation_df):\n",
    "        self.all_models_df = all_models_df\n",
    "        self.validation_df = validation_df\n",
    "        \n",
    "        self.model_predictions = {}\n",
    "        for (encounter_id, base_qid), group in all_models_df.groupby(['encounter_id', 'base_qid']):\n",
    "            self.model_predictions[(encounter_id, base_qid)] = group\n",
    "        \n",
    "        self.validation_data = {}\n",
    "        for _, row in validation_df.iterrows():\n",
    "            self.validation_data[(row['encounter_id'], row['base_qid'])] = row\n",
    "    \n",
    "    def get_combined_data(self, encounter_id, base_qid):\n",
    "        \"\"\"Retrieve combined data for a specific encounter and question.\"\"\"\n",
    "        model_preds = self.model_predictions.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        val_data = self.validation_data.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        if model_preds is None:\n",
    "            print(f\"No model predictions found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "            \n",
    "        if val_data is None:\n",
    "            print(f\"No validation data found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "        \n",
    "        if 'query_context' not in val_data:\n",
    "            val_data['query_context'] = DataProcessor.create_query_context(val_data)\n",
    "        \n",
    "        model_predictions_dict = {}\n",
    "        for _, row in model_preds.iterrows():\n",
    "            model_name = row['model_name']\n",
    "            \n",
    "            model_predictions_dict[model_name] = self._process_model_predictions(row)\n",
    "        \n",
    "        return {\n",
    "            'encounter_id': encounter_id,\n",
    "            'base_qid': base_qid,\n",
    "            'query_context': val_data['query_context'],\n",
    "            'images': val_data.get('all_images', []),\n",
    "            'options': val_data.get('options_en_cleaned', val_data.get('options_en', [])),\n",
    "            'question_type': val_data.get('question_type_en', ''),\n",
    "            'question_category': val_data.get('question_category_en', ''),\n",
    "            'model_predictions': model_predictions_dict\n",
    "        }\n",
    "    \n",
    "    def _process_model_predictions(self, row):\n",
    "        \"\"\"Process model predictions from row data.\"\"\"\n",
    "        return {\n",
    "            'model_prediction': row.get('combined_prediction', '')\n",
    "        }\n",
    "    \n",
    "    def get_all_encounter_question_pairs(self):\n",
    "        \"\"\"Return a list of all unique encounter_id, base_qid pairs.\"\"\"\n",
    "        return list(self.validation_data.keys())\n",
    "    \n",
    "    def get_sample_data(self, n=5):\n",
    "        \"\"\"Get a sample of combined data for n random encounter-question pairs.\"\"\"\n",
    "        import random\n",
    "        \n",
    "        all_pairs = self.get_all_encounter_question_pairs()\n",
    "        sample_pairs = random.sample(all_pairs, min(n, len(all_pairs)))\n",
    "        \n",
    "        return [self.get_combined_data(encounter_id, base_qid) for encounter_id, base_qid in sample_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(text):\n",
    "    \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "    cleaned_text = text\n",
    "    if \"```json\" in cleaned_text:\n",
    "        cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "    if \"```\" in cleaned_text:\n",
    "        cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not parse as JSON\")\n",
    "        return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227ce186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBaseManager:\n",
    "    \"\"\"Manages the dermatology knowledge base for RAG.\"\"\"\n",
    "\n",
    "    def __init__(self, args=None):\n",
    "        \"\"\"Initialize the knowledge base manager.\"\"\"\n",
    "        self.args = args\n",
    "        self.embedding_model = SentenceTransformer(args.embedding_model if args else \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "        self.cross_encoder = CrossEncoder(args.cross_encoder_model if args else \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "        # Initialize LanceDB\n",
    "        self.db_path = args.knowledge_db_path if args else os.path.join(os.getcwd(), \"knowledge_db\")\n",
    "        os.makedirs(self.db_path, exist_ok=True)\n",
    "        self.db = lancedb.connect(self.db_path)\n",
    "\n",
    "        # Check if the table exists, create it if not\n",
    "        self.table_name = \"dermatology_knowledge\"\n",
    "\n",
    "        if self.table_name not in self.db.table_names():\n",
    "            print(f\"Knowledge base not found. Creating new knowledge base at {self.db_path}\")\n",
    "            self._initialize_knowledge_base()\n",
    "        else:\n",
    "            print(f\"Using existing knowledge base at {self.db_path}\")\n",
    "            self.table = self.db.open_table(self.table_name)\n",
    "\n",
    "        # BM25 index for keyword search\n",
    "        self.tokenized_corpus = []\n",
    "        self.doc_ids = []\n",
    "        self._initialize_bm25_index()\n",
    "    \n",
    "    def _initialize_knowledge_base(self):\n",
    "        \"\"\"Initialize the knowledge base with the skin diseases dataset.\"\"\"\n",
    "        print(\"Loading dermatology dataset...\")\n",
    "        dataset_name = self.args.dataset_name_huggingface if self.args else \"brucewayne0459/Skin_diseases_and_care\"\n",
    "        dataset = load_dataset(dataset_name)\n",
    "\n",
    "        # Prepare data for LanceDB\n",
    "        data = []\n",
    "\n",
    "        print(\"Processing dataset and creating embeddings...\")\n",
    "        for i, item in enumerate(dataset['train']):\n",
    "            topic = item['Topic']\n",
    "            information = item['Information']\n",
    "\n",
    "            # Create document\n",
    "            combined_text = f\"Topic: {topic}\\n\\nInformation: {information}\"\n",
    "\n",
    "            # Create embedding\n",
    "            embedding = self.embedding_model.encode(combined_text)\n",
    "\n",
    "            # Add to data\n",
    "            data.append({\n",
    "                \"id\": i,\n",
    "                \"topic\": topic,\n",
    "                \"information\": information,\n",
    "                \"combined_text\": combined_text,\n",
    "                \"vector\": embedding.tolist()\n",
    "            })\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1} documents\")\n",
    "\n",
    "        # Convert the data to a pandas DataFrame\n",
    "        import pandas as pd\n",
    "        data_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create LanceDB table\n",
    "        print(\"Creating vector database...\")\n",
    "        self.table = self.db.create_table(\n",
    "            self.table_name,\n",
    "            data=data_df\n",
    "        )\n",
    "        print(\"Knowledge base initialization complete.\")\n",
    "    \n",
    "    def _initialize_bm25_index(self):\n",
    "        \"\"\"Initialize the BM25 index for keyword search without NLTK dependencies.\"\"\"\n",
    "        print(\"Initializing BM25 index...\")\n",
    "\n",
    "        # Query all documents from LanceDB\n",
    "        results = self.table.search().limit(10000).to_pandas()\n",
    "\n",
    "        # Common English stopwords - hardcoded to avoid NLTK dependency\n",
    "        common_stopwords = {\n",
    "            \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \n",
    "            \"by\", \"about\", \"from\", \"as\", \"of\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "            \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"can\", \"could\", \"will\",\n",
    "            \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\", \"this\", \"that\", \"these\",\n",
    "            \"those\", \"it\", \"its\", \"they\", \"them\", \"their\", \"he\", \"him\", \"his\", \"she\", \"her\"\n",
    "        }\n",
    "\n",
    "        for idx, row in results.iterrows():\n",
    "            doc_text = row['combined_text']\n",
    "            self.doc_ids.append(row['id'])\n",
    "\n",
    "            # Simple tokenization without NLTK\n",
    "            # Split by whitespace and remove punctuation\n",
    "            tokens = []\n",
    "            for token in doc_text.lower().split():\n",
    "                # Remove punctuation\n",
    "                token = ''.join(c for c in token if c.isalnum())\n",
    "                if token and token not in common_stopwords:\n",
    "                    tokens.append(token)\n",
    "\n",
    "            self.tokenized_corpus.append(tokens)\n",
    "\n",
    "        # Create BM25 index\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        print(\"BM25 index initialization complete.\")\n",
    "    \n",
    "    def semantic_search(self, query, top_k=None):\n",
    "        \"\"\"Perform semantic search using embeddings.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.args.top_k_semantic if self.args else 7\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        \n",
    "        # Search LanceDB\n",
    "        results = self.table.search(query_embedding.tolist()).limit(top_k).to_pandas()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def keyword_search(self, query, top_k=None):\n",
    "        \"\"\"Perform keyword search using BM25.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.args.top_k_keyword if self.args else 7\n",
    "\n",
    "        # Simple tokenization without NLTK\n",
    "        # Split by whitespace and remove punctuation\n",
    "        common_stopwords = {\n",
    "            \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \n",
    "            \"by\", \"about\", \"from\", \"as\", \"of\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "            \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"can\", \"could\", \"will\",\n",
    "            \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\", \"this\", \"that\", \"these\",\n",
    "            \"those\", \"it\", \"its\", \"they\", \"them\", \"their\", \"he\", \"him\", \"his\", \"she\", \"her\"\n",
    "        }\n",
    "\n",
    "        query_tokens = []\n",
    "        for token in query.lower().split():\n",
    "            # Remove punctuation\n",
    "            token = ''.join(c for c in token if c.isalnum())\n",
    "            if token and token not in common_stopwords:\n",
    "                query_tokens.append(token)\n",
    "\n",
    "        # Get BM25 scores\n",
    "        doc_scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(doc_scores)[::-1][:top_k]\n",
    "        \n",
    "        # Convert to document IDs and scores\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if doc_scores[idx] > 0:  # Only include if score is positive\n",
    "                doc_id = self.doc_ids[idx]\n",
    "                score = doc_scores[idx]\n",
    "                \n",
    "                # Get document from LanceDB\n",
    "                doc = self.table.search().where(f\"id = {doc_id}\").limit(1).to_pandas()\n",
    "                \n",
    "                if not doc.empty:\n",
    "                    results.append({\n",
    "                        \"id\": doc_id,\n",
    "                        \"topic\": doc['topic'].iloc[0],\n",
    "                        \"information\": doc['information'].iloc[0],\n",
    "                        \"combined_text\": doc['combined_text'].iloc[0],\n",
    "                        \"_distance\": 1.0 - min(score / 10.0, 1.0)  # Convert to distance metric (0 to 1)\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def hybrid_search(self, query, top_k=None):\n",
    "        \"\"\"Perform hybrid search combining semantic and keyword search.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.args.top_k_hybrid if self.args else 10\n",
    "        \n",
    "        # Perform both search types\n",
    "        semantic_results = self.semantic_search(query, top_k=top_k)\n",
    "        keyword_results = self.keyword_search(query, top_k=top_k)\n",
    "        \n",
    "        # Merge results and remove duplicates\n",
    "        combined_results = pd.concat([semantic_results, keyword_results])\n",
    "        combined_results = combined_results.drop_duplicates(subset=['id'])\n",
    "        \n",
    "        # Rerank the top results\n",
    "        if len(combined_results) > 0:\n",
    "            return self.rerank_results(combined_results, query, top_k=min(top_k, len(combined_results)))\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def rerank_results(self, results, query, top_k=None):\n",
    "        \"\"\"Rerank search results using a cross-encoder.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.args.top_k_rerank if self.args else 5\n",
    "        \n",
    "        if len(results) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Prepare input for cross-encoder\n",
    "        pairs = [(query, doc) for doc in results['combined_text'].tolist()]\n",
    "        \n",
    "        # Get scores from cross-encoder\n",
    "        cross_scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Add scores to results\n",
    "        results = results.copy()\n",
    "        results['cross_score'] = cross_scores\n",
    "        \n",
    "        # Sort by cross-encoder score\n",
    "        results = results.sort_values(by='cross_score', ascending=False).head(top_k)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a322d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisExtractor:\n",
    "    \"\"\"Extracts potential diagnoses from image analysis and clinical context.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_diagnoses(image_analysis, clinical_context, args=None):\n",
    "        \"\"\"\n",
    "        Extract potential diagnoses from image analysis and clinical context.\n",
    "        \n",
    "        Args:\n",
    "            image_analysis: Structured image analysis containing OVERALL_IMPRESSION\n",
    "            clinical_context: Structured clinical context analysis\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with diagnoses and confidence scores\n",
    "        \"\"\"\n",
    "        diagnoses = []\n",
    "        \n",
    "        # Extract from image analysis\n",
    "        if image_analysis and \"aggregated_analysis\" in image_analysis:\n",
    "            if \"OVERALL_IMPRESSION\" in image_analysis[\"aggregated_analysis\"]:\n",
    "                impression = image_analysis[\"aggregated_analysis\"][\"OVERALL_IMPRESSION\"]\n",
    "                if isinstance(impression, str):\n",
    "                    diagnoses.extend(DiagnosisExtractor._extract_from_text(impression, source=\"image_analysis\", confidence=0.7))\n",
    "        \n",
    "        # Extract from clinical context\n",
    "        if clinical_context and \"structured_clinical_context\" in clinical_context:\n",
    "            if \"DIAGNOSTIC_CONSIDERATIONS\" in clinical_context[\"structured_clinical_context\"]:\n",
    "                diagnostic_info = clinical_context[\"structured_clinical_context\"][\"DIAGNOSTIC_CONSIDERATIONS\"]\n",
    "                if isinstance(diagnostic_info, str):\n",
    "                    diagnoses.extend(DiagnosisExtractor._extract_from_text(diagnostic_info, source=\"clinical_context\", confidence=0.6))\n",
    "        \n",
    "        # If no diagnoses found, use extracted features to suggest potential diagnoses\n",
    "        if not diagnoses:\n",
    "            diagnoses = DiagnosisExtractor._suggest_from_features(image_analysis, clinical_context)\n",
    "            \n",
    "        return diagnoses\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_from_text(text, source, confidence):\n",
    "        \"\"\"Extract diagnoses from text.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Common diagnostic terms in dermatology\n",
    "        diagnostic_terms = [\n",
    "            \"eczema\", \"dermatitis\", \"psoriasis\", \"acne\", \"rosacea\", \"urticaria\", \n",
    "            \"melanoma\", \"carcinoma\", \"pemphigus\", \"pemphigoid\", \"lupus\", \"scleroderma\",\n",
    "            \"folliculitis\", \"cellulitis\", \"impetigo\", \"tinea\", \"herpes\", \"wart\",\n",
    "            \"vitiligo\", \"alopecia\", \"lichen\", \"keratosis\", \"prurigo\", \"rash\"\n",
    "        ]\n",
    "        \n",
    "        # Find diagnoses in text using regex patterns\n",
    "        diagnoses = []\n",
    "        \n",
    "        # Pattern 1: Diagnostic terms directly mentioned\n",
    "        for term in diagnostic_terms:\n",
    "            pattern = fr'\\b({term})[s\\s]\\b'\n",
    "            matches = re.finditer(pattern, text.lower())\n",
    "            for match in matches:\n",
    "                diagnoses.append({\n",
    "                    \"diagnosis\": match.group(1).capitalize(),\n",
    "                    \"confidence\": confidence,\n",
    "                    \"source\": source\n",
    "                })\n",
    "                \n",
    "        # Pattern 2: \"Consistent with\", \"suggestive of\", \"indicative of\" phrases\n",
    "        patterns = [\n",
    "            r'consistent with\\s+([^,.;]+)',\n",
    "            r'suggestive of\\s+([^,.;]+)',\n",
    "            r'indicative of\\s+([^,.;]+)',\n",
    "            r'compatible with\\s+([^,.;]+)',\n",
    "            r'diagnostic of\\s+([^,.;]+)',\n",
    "            r'likely\\s+([^,.;]+)',\n",
    "            r'probable\\s+([^,.;]+)',\n",
    "            r'possible\\s+([^,.;]+)',\n",
    "            r'suspected\\s+([^,.;]+)',\n",
    "            r'diagnosis of\\s+([^,.;]+)',\n",
    "            r'impression:\\s+([^,.;]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text.lower())\n",
    "            for match in matches:\n",
    "                diagnoses.append({\n",
    "                    \"diagnosis\": match.group(1).strip().capitalize(),\n",
    "                    \"confidence\": confidence * 0.9,  # Slightly lower confidence\n",
    "                    \"source\": source\n",
    "                })\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_diagnoses = []\n",
    "        seen = set()\n",
    "        for diag in diagnoses:\n",
    "            if diag[\"diagnosis\"].lower() not in seen:\n",
    "                seen.add(diag[\"diagnosis\"].lower())\n",
    "                unique_diagnoses.append(diag)\n",
    "        \n",
    "        return unique_diagnoses\n",
    "    \n",
    "    @staticmethod\n",
    "    def _suggest_from_features(image_analysis, clinical_context):\n",
    "        \"\"\"Suggest potential diagnoses based on extracted features.\"\"\"\n",
    "        diagnoses = []\n",
    "        features = {}\n",
    "        \n",
    "        # Extract features from image analysis\n",
    "        if image_analysis and \"aggregated_analysis\" in image_analysis:\n",
    "            analysis = image_analysis[\"aggregated_analysis\"]\n",
    "            \n",
    "            if \"SKIN_DESCRIPTION\" in analysis:\n",
    "                features[\"skin_description\"] = analysis[\"SKIN_DESCRIPTION\"]\n",
    "                \n",
    "            if \"LESION_COLOR\" in analysis:\n",
    "                features[\"lesion_color\"] = analysis[\"LESION_COLOR\"]\n",
    "                \n",
    "            if \"SITE_LOCATION\" in analysis:\n",
    "                features[\"site_location\"] = analysis[\"SITE_LOCATION\"]\n",
    "        \n",
    "        # Extract features from clinical context\n",
    "        if clinical_context and \"structured_clinical_context\" in clinical_context:\n",
    "            context = clinical_context[\"structured_clinical_context\"]\n",
    "            \n",
    "            if \"SYMPTOMS\" in context:\n",
    "                features[\"symptoms\"] = context[\"SYMPTOMS\"]\n",
    "                \n",
    "            if \"HISTORY\" in context:\n",
    "                features[\"history\"] = context[\"HISTORY\"]\n",
    "        \n",
    "        # Rule-based diagnosis suggestions based on features\n",
    "        if features:\n",
    "            # Example rules (simplified):\n",
    "            if \"hand\" in str(features.get(\"site_location\", \"\")).lower():\n",
    "                if \"scaling\" in str(features.get(\"skin_description\", \"\")).lower():\n",
    "                    diagnoses.append({\n",
    "                        \"diagnosis\": \"Hand eczema\",\n",
    "                        \"confidence\": 0.5,\n",
    "                        \"source\": \"feature_based\"\n",
    "                    })\n",
    "                    \n",
    "            if \"red\" in str(features.get(\"lesion_color\", \"\")).lower():\n",
    "                if \"itchy\" in str(features.get(\"symptoms\", \"\")).lower():\n",
    "                    diagnoses.append({\n",
    "                        \"diagnosis\": \"Contact dermatitis\",\n",
    "                        \"confidence\": 0.4,\n",
    "                        \"source\": \"feature_based\"\n",
    "                    })\n",
    "        \n",
    "        # Always include a generic term if no specific diagnoses were found\n",
    "        if not diagnoses:\n",
    "            diagnoses.append({\n",
    "                \"diagnosis\": \"Dermatosis\", \n",
    "                \"confidence\": 0.3,\n",
    "                \"source\": \"fallback\"\n",
    "            })\n",
    "            \n",
    "        return diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2009dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisBasedQueryGenerator:\n",
    "    \"\"\"Generates search queries based on extracted diagnoses.\"\"\"\n",
    "        \n",
    "    def __init__(self, client, args=None):\n",
    "        \"\"\"Initialize the query generator.\"\"\"\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "    \n",
    "    def generate_queries(self, question_text, question_type, options, integrated_evidence, diagnoses, num_queries=4):\n",
    "        \"\"\"\n",
    "        Generate search queries based on diagnoses and question type.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: Type of question being asked\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            diagnoses: List of extracted diagnoses\n",
    "            num_queries: Number of queries to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of search queries\n",
    "        \"\"\"\n",
    "        # Sort diagnoses by confidence\n",
    "        sorted_diagnoses = sorted(diagnoses, key=lambda x: x.get('confidence', 0), reverse=True)\n",
    "        \n",
    "        # Generate different types of queries based on question type\n",
    "        question_specific_queries = self._generate_question_specific_queries(\n",
    "            question_text, \n",
    "            question_type, \n",
    "            options, \n",
    "            sorted_diagnoses\n",
    "        )\n",
    "        \n",
    "        diagnosis_specific_queries = self._generate_diagnosis_specific_queries(\n",
    "            question_type,\n",
    "            sorted_diagnoses\n",
    "        )\n",
    "        \n",
    "        # Combine and prioritize queries\n",
    "        all_queries = question_specific_queries + diagnosis_specific_queries\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_queries = []\n",
    "        seen = set()\n",
    "        for query in all_queries:\n",
    "            if query.lower() not in seen:\n",
    "                seen.add(query.lower())\n",
    "                unique_queries.append(query)\n",
    "        \n",
    "        # Return at most num_queries\n",
    "        return unique_queries[:num_queries]\n",
    "    \n",
    "    def _generate_question_specific_queries(self, question_text, question_type, options, diagnoses):\n",
    "        \"\"\"Generate queries specific to the question type.\"\"\"\n",
    "        queries = []\n",
    "        \n",
    "        # For classification/terminology questions, focus on the classification system\n",
    "        classification_types = [\"Site Location\", \"Lesion Color\", \"Size\", \"Extent\", \"Lesion Count\"]\n",
    "        if question_type in classification_types:\n",
    "            # Query about the classification system\n",
    "            classification_terms = \", \".join([opt for opt in options if opt.lower() != \"not mentioned\"])\n",
    "            queries.append(f\"dermatology {question_type.lower()} classification {classification_terms}\")\n",
    "            \n",
    "            # Query about how to distinguish between options\n",
    "            if len(options) > 2:\n",
    "                queries.append(f\"how to distinguish between {classification_terms} in dermatology\")\n",
    "                \n",
    "            # For extent questions specifically\n",
    "            if question_type == \"Extent\":\n",
    "                queries.append(\"definition of widespread vs limited area skin condition dermatology\")\n",
    "        \n",
    "        # For diagnostic questions, use diagnoses\n",
    "        if question_type in [\"Differential\", \"Specific Diagnosis\"]:\n",
    "            if diagnoses:\n",
    "                top_diagnosis = diagnoses[0][\"diagnosis\"]\n",
    "                queries.append(f\"{top_diagnosis} diagnostic criteria dermatology\")\n",
    "                \n",
    "                # Add query for differential diagnosis\n",
    "                diagnoses_list = \", \".join([d[\"diagnosis\"] for d in diagnoses[:3]])\n",
    "                queries.append(f\"differential diagnosis {diagnoses_list}\")\n",
    "        \n",
    "        # For treatment questions\n",
    "        if question_type == \"Treatment\":\n",
    "            if diagnoses:\n",
    "                top_diagnosis = diagnoses[0][\"diagnosis\"]\n",
    "                queries.append(f\"{top_diagnosis} treatment options dermatology\")\n",
    "                \n",
    "                # Add body site if available\n",
    "                body_site = self._extract_body_site(question_text)\n",
    "                if body_site:\n",
    "                    queries.append(f\"{top_diagnosis} {body_site} treatment guidelines\")\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def _generate_diagnosis_specific_queries(self, question_type, diagnoses):\n",
    "        \"\"\"Generate queries that connect diagnoses with the question type.\"\"\"\n",
    "        queries = []\n",
    "        \n",
    "        if not diagnoses:\n",
    "            return queries\n",
    "            \n",
    "        # Use top diagnoses\n",
    "        for diagnosis in diagnoses[:2]:  # Use top 2 diagnoses\n",
    "            diag_name = diagnosis[\"diagnosis\"]\n",
    "            \n",
    "            # Connect diagnosis with question type\n",
    "            if question_type in [\"Site Location\", \"Extent\"]:\n",
    "                queries.append(f\"{diag_name} typical distribution pattern dermatology\")\n",
    "                queries.append(f\"{diag_name} localized versus widespread presentation\")\n",
    "                \n",
    "            elif question_type == \"Lesion Color\":\n",
    "                queries.append(f\"{diag_name} typical color appearance dermatology\")\n",
    "                \n",
    "            elif question_type == \"Texture\":\n",
    "                queries.append(f\"{diag_name} texture characteristics dermatology\")\n",
    "                \n",
    "            elif question_type == \"Itch\":\n",
    "                queries.append(f\"is {diag_name} itchy dermatology\")\n",
    "                \n",
    "            elif question_type == \"Onset\":\n",
    "                queries.append(f\"{diag_name} typical onset and progression\")\n",
    "                \n",
    "            else:\n",
    "                # General query connecting diagnosis and question type\n",
    "                queries.append(f\"{diag_name} {question_type.lower()} dermatology\")\n",
    "                \n",
    "        return queries\n",
    "    \n",
    "    def _extract_body_site(self, question_text):\n",
    "        \"\"\"Extract body site from question text.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        body_parts = [\n",
    "            \"hand\", \"foot\", \"arm\", \"leg\", \"face\", \"back\", \"chest\", \"abdomen\",\n",
    "            \"scalp\", \"neck\", \"finger\", \"toe\", \"elbow\", \"knee\", \"shoulder\",\n",
    "            \"palm\", \"sole\", \"trunk\", \"extremity\", \"head\"\n",
    "        ]\n",
    "        \n",
    "        for part in body_parts:\n",
    "            if re.search(r'\\b' + part + r'[s]?\\b', question_text.lower()):\n",
    "                return part\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b925e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisBasedKnowledgeRetriever:\n",
    "    \"\"\"Retrieves knowledge from the dermatology knowledge base using diagnosis-based approach.\"\"\"\n",
    "    \n",
    "    def __init__(self, kb_manager, query_generator, diagnosis_extractor, args=None):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge retriever.\n",
    "        \n",
    "        Args:\n",
    "            kb_manager: KnowledgeBaseManager instance\n",
    "            query_generator: DiagnosisBasedQueryGenerator instance\n",
    "            diagnosis_extractor: DiagnosisExtractor instance\n",
    "            args: Configuration arguments\n",
    "        \"\"\"\n",
    "        self.kb_manager = kb_manager\n",
    "        self.query_generator = query_generator\n",
    "        self.diagnosis_extractor = diagnosis_extractor\n",
    "        self.args = args\n",
    "    \n",
    "    def retrieve_knowledge(self, question_text, question_type, options, image_analysis, clinical_context, integrated_evidence):\n",
    "        \"\"\"\n",
    "        Retrieve relevant knowledge for a dermatological question using diagnoses.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: Type of question being asked\n",
    "            options: Available answer options\n",
    "            image_analysis: Structured image analysis\n",
    "            clinical_context: Structured clinical context\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with retrieved knowledge\n",
    "        \"\"\"\n",
    "        # Check if we should use RAG for this question type\n",
    "        if self.args:\n",
    "            rag_config = self.args.question_type_retrieval_config.get(\n",
    "                question_type, self.args.default_rag_config\n",
    "            )\n",
    "        else:\n",
    "            # Fallback if args not provided\n",
    "            default_config = {\"use_rag\": True, \"weight\": 0.4}\n",
    "            rag_config = {\n",
    "                \"Site Location\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "                \"Lesion Color\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "                \"Size\": {\"use_rag\": False, \"weight\": 0.1},\n",
    "                # Add other configurations as needed\n",
    "            }.get(question_type, default_config)\n",
    "        \n",
    "        if not rag_config[\"use_rag\"]:\n",
    "            return {\n",
    "                \"retrieved\": False,\n",
    "                \"reason\": f\"RAG not enabled for question type: {question_type}\",\n",
    "                \"results\": []\n",
    "            }\n",
    "        \n",
    "        # Extract potential diagnoses\n",
    "        diagnoses = self.diagnosis_extractor.extract_diagnoses(image_analysis, clinical_context)\n",
    "        \n",
    "        # Generate search queries based on diagnoses and question type\n",
    "        queries = self.query_generator.generate_queries(\n",
    "            question_text, \n",
    "            question_type, \n",
    "            options, \n",
    "            integrated_evidence,\n",
    "            diagnoses\n",
    "        )\n",
    "        \n",
    "        if not queries:\n",
    "            return {\n",
    "                \"retrieved\": False,\n",
    "                \"reason\": \"Failed to generate search queries\",\n",
    "                \"results\": []\n",
    "            }\n",
    "        \n",
    "        # Retrieve results for each query\n",
    "        all_results = []\n",
    "        \n",
    "        for query in queries:\n",
    "            results = self.kb_manager.hybrid_search(query)\n",
    "            \n",
    "            if not results.empty:\n",
    "                # Convert to list of dictionaries for easier handling\n",
    "                for _, row in results.iterrows():\n",
    "                    # Get cross-encoder score or fallback to distance metric\n",
    "                    relevance_score = float(row.get('cross_score', 1.0 - row.get('_distance', 0.5)))\n",
    "                    \n",
    "                    # Only include results with positive relevance scores\n",
    "                    if relevance_score > 0:\n",
    "                        all_results.append({\n",
    "                            \"query\": query,\n",
    "                            \"topic\": row['topic'],\n",
    "                            \"information\": row['information'],\n",
    "                            \"relevance_score\": relevance_score,\n",
    "                            \"diagnoses\": [d[\"diagnosis\"] for d in diagnoses[:3]]\n",
    "                        })\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_results = []\n",
    "        seen_topics = set()\n",
    "        \n",
    "        for result in sorted(all_results, key=lambda x: x['relevance_score'], reverse=True):\n",
    "            if result['topic'] not in seen_topics:\n",
    "                unique_results.append(result)\n",
    "                seen_topics.add(result['topic'])\n",
    "        \n",
    "        top_k = self.args.top_k_rerank if self.args else 5\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"retrieved\": len(unique_results) > 0,\n",
    "            \"queries\": queries,\n",
    "            \"diagnoses\": diagnoses,\n",
    "            \"results\": unique_results[:top_k]  # Limit to top-k unique results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779ca6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAnalysisService:\n",
    "    \"\"\"Service for analyzing dermatological images.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, args=None):\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "        \n",
    "    def analyze_images(self, image_paths, encounter_id):\n",
    "        \"\"\"\n",
    "        Analyze multiple dermatological images for an encounter.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of paths to images\n",
    "            encounter_id: Encounter identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with individual and aggregated analyses\n",
    "        \"\"\"\n",
    "        image_analyses = []\n",
    "        \n",
    "        structured_prompt = self._create_dermatology_prompt()\n",
    "        \n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            analysis = self._analyze_single_image(\n",
    "                img_path, \n",
    "                structured_prompt, \n",
    "                encounter_id, \n",
    "                idx, \n",
    "                len(image_paths)\n",
    "            )\n",
    "            image_analyses.append(analysis)\n",
    "        \n",
    "        aggregated_analysis = self._aggregate_analyses(image_analyses, encounter_id)\n",
    "        \n",
    "        return {\n",
    "            \"encounter_id\": encounter_id,\n",
    "            \"image_count\": len(image_paths),\n",
    "            \"individual_analyses\": image_analyses,\n",
    "            \"aggregated_analysis\": aggregated_analysis\n",
    "        }\n",
    "    \n",
    "    def _create_dermatology_prompt(self):\n",
    "        \"\"\"Create the structured dermatology analysis prompt.\"\"\"\n",
    "        return \"\"\"As dermatology specialist analyzing skin images, extract and structure all clinically relevant information from this dermatological image.\n",
    "\n",
    "Organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is cannot be determined, state \"Cannot determine from image\".\n",
    "\"\"\"\n",
    "    \n",
    "    def _analyze_single_image(self, img_path, prompt, encounter_id, idx, total_images):\n",
    "        \"\"\"Analyze a single dermatological image.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            print(f\"Analyzing image {idx+1}/{total_images} for encounter {encounter_id}\")\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt, image]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "            \n",
    "            structured_analysis = parse_json_response(analysis_text)\n",
    "            \n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"structured_analysis\": structured_analysis\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image {img_path}: {str(e)}\")\n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _aggregate_analyses(self, image_analyses, encounter_id):\n",
    "        \"\"\"Aggregate structured analyses from multiple images.\"\"\"\n",
    "        valid_analyses = [a for a in image_analyses if \"error\" not in a and \"structured_analysis\" in a]\n",
    "        \n",
    "        if not valid_analyses:\n",
    "            return {\n",
    "                \"error\": \"No valid analyses to aggregate\",\n",
    "                \"message\": \"Unable to generate aggregated analysis due to errors in individual analyses.\"\n",
    "            }\n",
    "        \n",
    "        if len(valid_analyses) == 1:\n",
    "            return valid_analyses[0][\"structured_analysis\"]\n",
    "        \n",
    "        analysis_jsons = []\n",
    "        for analysis in valid_analyses:\n",
    "            analysis_json = json.dumps(analysis[\"structured_analysis\"])\n",
    "            analysis_jsons.append(f\"Image {analysis['image_index']} ({analysis['image_path']}): {analysis_json}\")\n",
    "        \n",
    "        aggregation_prompt = self._create_aggregation_prompt(analysis_jsons)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[aggregation_prompt]\n",
    "            )\n",
    "            \n",
    "            aggregation_text = response.text\n",
    "            \n",
    "            aggregated_analysis = parse_json_response(aggregation_text)\n",
    "            \n",
    "            return aggregated_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating aggregated analysis for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"aggregation_error\": \"Failed to generate aggregated analysis\"\n",
    "            }\n",
    "    \n",
    "    def _create_aggregation_prompt(self, analysis_jsons):\n",
    "        \"\"\"Create a prompt for aggregating multiple image analyses.\"\"\"\n",
    "        return f\"\"\"As dermatology specialist reviewing multiple skin image analyses for the same patient, combine these analyses and organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "    \n",
    "{' '.join(analysis_jsons)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b38e4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalContextAnalyzer:\n",
    "    \"\"\"Service for analyzing clinical context.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, args=None):\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "        \n",
    "    def extract_clinical_context(self, query_context, encounter_id):\n",
    "        \"\"\"\n",
    "        Extract structured clinical information from an encounter's query context.\n",
    "        \n",
    "        Args:\n",
    "            query_context: The query context text\n",
    "            encounter_id: Encounter identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured clinical information\n",
    "        \"\"\"\n",
    "        clinical_text = self._extract_clinical_text(query_context)\n",
    "        \n",
    "        if not clinical_text:\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"clinical_summary\": \"No clinical information available\"\n",
    "            }\n",
    "        \n",
    "        prompt = self._create_clinical_context_prompt(clinical_text)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            structured_context = parse_json_response(response.text)\n",
    "            \n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"structured_clinical_context\": structured_context\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting clinical context for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_clinical_text(self, query_context):\n",
    "        \"\"\"Extract clinical text from query context.\"\"\"\n",
    "        clinical_lines = []\n",
    "        capturing = False\n",
    "        for line in query_context.split('\\n'):\n",
    "            if \"Background Clinical Information\" in line:\n",
    "                capturing = True\n",
    "                continue\n",
    "            elif \"Available Options\" in line:\n",
    "                capturing = False\n",
    "            elif capturing:\n",
    "                clinical_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(clinical_lines).strip()\n",
    "    \n",
    "    def _create_clinical_context_prompt(self, clinical_text):\n",
    "        \"\"\"Create prompt for extracting structured clinical information.\"\"\"\n",
    "        return f\"\"\"You are a dermatology specialist analyzing patient information. \n",
    "Extract and structure all clinically relevant information from this patient description:\n",
    "\n",
    "{clinical_text}\n",
    "\n",
    "Organize your response in the following JSON structure:\n",
    "\n",
    "1. DEMOGRAPHICS: Age, sex, and any other demographic data\n",
    "2. SITE_LOCATION: Body parts affected by the condition as described in the text\n",
    "3. SKIN_DESCRIPTION: Any mention of lesion morphology (flat, raised, depressed), texture, surface characteristics (scales, crust, fluid), appearance of lesion boundaries\n",
    "4. LESION_COLOR: Any description of color(s) of affected areas, color variations, comparison to normal skin\n",
    "5. LESION_COUNT: Any information about number of lesions, single vs multiple presentation, distribution pattern\n",
    "6. EXTENT: How widespread the condition appears based on the description, localized vs widespread\n",
    "7. TEXTURE: Any description of tactile qualities, smooth vs rough, notable textural features\n",
    "8. ONSET_INDICATORS: Information about onset, duration, progression, or evolution of symptoms\n",
    "9. ITCH_INDICATORS: Mentions of scratching, itchiness, or other sensory symptoms\n",
    "10. OTHER_SYMPTOMS: Any additional symptoms mentioned (pain, burning, etc.)\n",
    "11. TRIGGERS: Identified factors that worsen/improve the condition\n",
    "12. HISTORY: Relevant past medical history or previous treatments\n",
    "13. DIAGNOSTIC_CONSIDERATIONS: Any mentioned or suggested diagnoses in the text\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is \n",
    "not available, indicate \"Not mentioned\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f66d1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceIntegrator:\n",
    "    \"\"\"Integrates visual, clinical, and knowledge-based evidence.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, args=None):\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "        \n",
    "    def integrate_evidence(self, image_analysis, clinical_context, question_type, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Integrate image analysis with clinical context and retrieved knowledge.\n",
    "        \n",
    "        Args:\n",
    "            image_analysis: Structured image analysis\n",
    "            clinical_context: Structured clinical context\n",
    "            question_type: Type of question being asked\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with integrated evidence\n",
    "        \"\"\"\n",
    "        # Determine weighting based on question type\n",
    "        weights = self._get_weights_for_question(question_type)\n",
    "        \n",
    "        # Create prompt for integration\n",
    "        prompt = self._create_integration_prompt(\n",
    "            image_analysis,\n",
    "            clinical_context,\n",
    "            question_type,\n",
    "            weights,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            integration_text = response.text\n",
    "            \n",
    "            integrated_evidence = parse_json_response(integration_text)\n",
    "            \n",
    "            return integrated_evidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error integrating evidence: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"message\": \"Failed to integrate evidence\"\n",
    "            }\n",
    "    \n",
    "    def _get_weights_for_question(self, question_type):\n",
    "        \"\"\"\n",
    "        Determine evidence weighting based on question type.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with weights for each evidence type\n",
    "        \"\"\"\n",
    "        weights = {\n",
    "            \"Site Location\": {\"image\": 0.8, \"clinical\": 0.2, \"knowledge\": 0.0},\n",
    "            \"Lesion Color\": {\"image\": 0.9, \"clinical\": 0.1, \"knowledge\": 0.0},\n",
    "            \"Size\": {\"image\": 0.8, \"clinical\": 0.2, \"knowledge\": 0.0},\n",
    "            \"Skin Description\": {\"image\": 0.7, \"clinical\": 0.3, \"knowledge\": 0.2},\n",
    "            \"Duration of Symptoms\": {\"image\": 0.3, \"clinical\": 0.7, \"knowledge\": 0.2},\n",
    "            \"Itch\": {\"image\": 0.4, \"clinical\": 0.6, \"knowledge\": 0.3},\n",
    "            \"Extent\": {\"image\": 0.7, \"clinical\": 0.3, \"knowledge\": 0.1},\n",
    "            \"Treatment\": {\"image\": 0.1, \"clinical\": 0.9, \"knowledge\": 0.7},\n",
    "            \"Lesion Evolution\": {\"image\": 0.3, \"clinical\": 0.7, \"knowledge\": 0.4},\n",
    "            \"Texture\": {\"image\": 0.6, \"clinical\": 0.4, \"knowledge\": 0.2},\n",
    "            \"Specific Diagnosis\": {\"image\": 0.5, \"clinical\": 0.5, \"knowledge\": 0.8},\n",
    "            \"Count\": {\"image\": 0.8, \"clinical\": 0.2, \"knowledge\": 0.0},\n",
    "            \"Differential\": {\"image\": 0.5, \"clinical\": 0.5, \"knowledge\": 0.8},\n",
    "        }\n",
    "        \n",
    "        # Get weights from args or fallback to default\n",
    "        if self.args:\n",
    "            rag_config = self.args.question_type_retrieval_config.get(\n",
    "                question_type, self.args.default_rag_config\n",
    "            )\n",
    "        else:\n",
    "            # Fallback if args not provided\n",
    "            default_config = {\"use_rag\": True, \"weight\": 0.4}\n",
    "            rag_config = {\n",
    "                \"Site Location\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "                \"Lesion Color\": {\"use_rag\": False, \"weight\": 0.2},\n",
    "                # Add other known configurations\n",
    "            }.get(question_type, default_config)\n",
    "            \n",
    "        knowledge_weight = rag_config[\"weight\"] if rag_config[\"use_rag\"] else 0.0\n",
    "        \n",
    "        # Default weights if question type not found\n",
    "        default = {\"image\": 0.5, \"clinical\": 0.5, \"knowledge\": knowledge_weight}\n",
    "        type_weights = weights.get(question_type, default)\n",
    "        \n",
    "        # Override knowledge weight if specified in config\n",
    "        type_weights[\"knowledge\"] = knowledge_weight\n",
    "        \n",
    "        return type_weights\n",
    "    \n",
    "    def _create_integration_prompt(self, image_analysis, clinical_context, question_type, weights, retrieved_knowledge=None):\n",
    "        \"\"\"Create prompt for evidence integration.\"\"\"\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\n",
    "For this {question_type} question, image evidence has {weights['image']*100}% weight, clinical evidence has {weights['clinical']*100}% weight, and medical knowledge has {weights['knowledge']*100}% weight.\n",
    "\"\"\"\n",
    "        \n",
    "        # Base prompt\n",
    "        prompt = f\"\"\"As a dermatology specialist, integrate the visual findings from images with the clinical history.\n",
    "\n",
    "IMAGE ANALYSIS:\n",
    "{json.dumps(image_analysis.get(\"aggregated_analysis\", {}), indent=2)}\n",
    "\n",
    "CLINICAL CONTEXT:\n",
    "{json.dumps(clinical_context.get(\"structured_clinical_context\", {}), indent=2)}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "Pay special attention to potential contradictions between visual findings and clinical history. Even minor inconsistencies should be noted as contradictions. Look for cases where clinical context suggests features not visible in images or where visual findings seem to contradict patient-reported symptoms or history.\n",
    "\n",
    "Organize your response in a JSON structure with the following elements:\n",
    "\n",
    "1. INTEGRATED_FINDINGS: For each key dermatological feature, combine visual and clinical evidence\n",
    "   - SIZE\n",
    "   - SITE_LOCATION\n",
    "   - SKIN_DESCRIPTION\n",
    "   - LESION_COLOR\n",
    "   - LESION_COUNT\n",
    "   - EXTENT\n",
    "   - TEXTURE\n",
    "   - ONSET_DURATION\n",
    "   - SYMPTOMS\n",
    "\n",
    "2. CONCORDANCE_ASSESSMENT: For each feature, assess if visual and clinical evidence are:\n",
    "   - CONCORDANT: Visual and clinical evidence agree\n",
    "   - DISCORDANT: Visual and clinical evidence conflict (explain the conflict)\n",
    "   - COMPLEMENTARY: Evidence sources provide different but non-conflicting information\n",
    "   - MISSING_VISUAL: Clinical description present but not visible in images\n",
    "   - MISSING_CLINICAL: Visible in images but not mentioned in clinical context\n",
    "\n",
    "3. CONTRADICTIONS: List any specific contradictions between visual and clinical evidence\n",
    "   - For each contradiction, explain what the conflict is and assess which source is more reliable\n",
    "\n",
    "4. WEIGHTED_EVIDENCE_PROFILE: Synthesize the most reliable information for each category\n",
    "   - Apply the provided weights to determine the most reliable facts for each feature\n",
    "   - Explain where you've prioritized one source over another\n",
    "\n",
    "5. CONFIDENCE_SCORES: Score the confidence (0.0-1.0) in the integrated evidence for each feature\n",
    "\n",
    "Be specific, concise, and use medical terminology where appropriate.\n",
    "\"\"\"\n",
    "\n",
    "        # Add knowledge summary section if present\n",
    "        if has_knowledge:\n",
    "            prompt += \"\"\"\n",
    "\n",
    "6. MEDICAL_KNOWLEDGE_INSIGHTS: Summarize key insights from retrieved medical knowledge\n",
    "   - How the retrieved knowledge confirms or challenges the observed findings\n",
    "   - Additional relevant diagnostic or management considerations\n",
    "   - Typical clinical patterns or expected features that align with observations\n",
    "\"\"\"\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d97a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningEngine:\n",
    "    \"\"\"Applies reasoning to determine the best answer.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, args=None):\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "        \n",
    "    def apply_initial_reasoning(self, question_text, question_type, options, integrated_evidence, model_predictions, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Apply initial reasoning to determine the most likely answer.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            model_predictions: Model predictions to consider\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with reasoning and answer\n",
    "        \"\"\"\n",
    "        model_prediction_text = self._format_model_predictions(model_predictions)\n",
    "        \n",
    "        multiple_answers_allowed = question_type in [\"Site Location\", \"Size\", \"Skin Description\"]\n",
    "        \n",
    "        prompt = self._create_reasoning_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            model_prediction_text,\n",
    "            multiple_answers_allowed,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reasoning_text = response.text\n",
    "            \n",
    "            reasoning_result = parse_json_response(reasoning_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(reasoning_result.get('answer', ''), options)\n",
    "            reasoning_result['validated_answer'] = validated_answer\n",
    "            \n",
    "            # Ensure confidence doesn't exceed 0.8 unless absolutely confident\n",
    "            confidence = reasoning_result.get('confidence', 0.0)\n",
    "            if isinstance(confidence, str):\n",
    "                try:\n",
    "                    confidence = float(confidence)\n",
    "                except:\n",
    "                    confidence = 0.0\n",
    "                    \n",
    "            # Apply randomization to reduce overconfidence and encourage reflection\n",
    "            # Multiply by a random factor between 0.9 and 1.0\n",
    "            randomized_confidence = confidence * random.uniform(0.9, 1.0)\n",
    "            \n",
    "            # Cap at 0.8 unless perfect confidence (1.0)\n",
    "            if 0.95 < confidence < 1.0:\n",
    "                randomized_confidence = 0.95\n",
    "            \n",
    "            # Use confidence threshold from args if available\n",
    "            confidence_threshold = self.args.confidence_threshold if self.args else 0.75\n",
    "            if randomized_confidence > confidence_threshold:\n",
    "                randomized_confidence = confidence_threshold\n",
    "            \n",
    "            reasoning_result['confidence'] = randomized_confidence\n",
    "            \n",
    "            return reasoning_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying initial reasoning: {str(e)}\")\n",
    "            return {\n",
    "                \"reasoning\": f\"Error: {str(e)}\",\n",
    "                \"answer\": \"Not mentioned\",\n",
    "                \"validated_answer\": \"Not mentioned\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _format_model_predictions(self, model_predictions):\n",
    "        \"\"\"Format model predictions for the prompt.\"\"\"\n",
    "        model_prediction_text = \"\"\n",
    "        for model_name, predictions in model_predictions.items():\n",
    "            combined_pred = predictions.get('model_prediction', '')\n",
    "            if isinstance(combined_pred, float) and pd.isna(combined_pred):\n",
    "                combined_pred = \"No prediction\"\n",
    "            model_prediction_text += f\"- {model_name}: {combined_pred}\\n\"\n",
    "        return model_prediction_text\n",
    "\n",
    "    def _create_reasoning_prompt(self, question_text, question_type, options, integrated_evidence, model_prediction_text, multiple_answers_allowed, retrieved_knowledge=None):\n",
    "        \"\"\"Create a prompt for the reasoning layer.\"\"\"\n",
    "        specialized_guidance = \"\"\n",
    "        \n",
    "        if question_type == \"Size\" and all(option in \", \".join(options) for option in [\"size of thumb nail\", \"size of palm\", \"larger area\"]):\n",
    "            specialized_guidance = \"\"\"\n",
    "SPECIALIZED GUIDANCE FOR SIZE ASSESSMENT:\n",
    "When answering this size-related question, interpret the options as follows:\n",
    "- \"size of thumb nail\": Individual lesions or affected areas approximately 1-2 cm in diameter\n",
    "- \"size of palm\": Affected areas larger than the size of a thumb nail and roughly the size of a palm (approximately 1% of body surface area), which may include multiple smaller lesions across a region\n",
    "- \"larger area\": Widespread involvement significantly larger than a palm, affecting a substantial portion(s) of the body\n",
    "\n",
    "IMPORTANT: For cases with multiple small lesions that are visible in the images, but without extensive widespread involvement across large body regions, \"size of palm\" is likely the most appropriate answer.\n",
    "\"\"\"\n",
    "        elif question_type == \"Lesion Color\" and \"combination\" in \", \".join(options):\n",
    "            specialized_guidance = \"\"\"\n",
    "SPECIALIZED GUIDANCE FOR LESION COLOR:\n",
    "When answering color-related questions, pay careful attention to whether there are multiple distinct colors present across the affected areas. \"Combination\" would be appropriate when different lesions display different colors (e.g., some lesions appear red while others appear white), or when individual lesions show mixed or varied coloration patterns.\n",
    "\"\"\"\n",
    "\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        if multiple_answers_allowed:\n",
    "            task_description = \"\"\"\n",
    "Based on all the evidence above, determine the most accurate answer(s) to the question. Your task is to:\n",
    "1. Analyze the integrated evidence\n",
    "2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "3. Provide a detailed reasoning for your conclusion\n",
    "4. Select the final answer(s) from the available options\n",
    "5. Provide a confidence score from 0.0 to 1.0 for your answer. Be conservative in your confidence assessment. Consider all possible sources of uncertainty, including image quality limitations, interpretation ambiguity, and potential contradictions. Confidence scores should rarely exceed 0.8 unless evidence is absolutely conclusive and unambiguous.\n",
    "\n",
    "If selecting multiple answers is appropriate, provide them in a comma-separated list. If no answer can be determined, select \"Not mentioned\".\n",
    "\"\"\"\n",
    "        else:\n",
    "            task_description = \"\"\"\n",
    "Based on all the evidence above, determine the SINGLE most accurate answer to the question. Your task is to:\n",
    "1. Analyze the integrated evidence\n",
    "2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "3. Provide a detailed reasoning for your conclusion\n",
    "4. Select ONLY ONE answer option that is most accurate\n",
    "5. Provide a confidence score from 0.0 to 1.0 for your answer. Be conservative in your confidence assessment. Consider all possible sources of uncertainty, including image quality limitations, interpretation ambiguity, and potential contradictions. Confidence scores should rarely exceed 0.8 unless evidence is absolutely conclusive and unambiguous.\n",
    "\n",
    "For this question type, you must select ONLY ONE option as your answer. If no answer can be determined, select \"Not mentioned\".\n",
    "\"\"\"\n",
    "\n",
    "        response_format = \"\"\"\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"reasoning\": Your step-by-step reasoning process\n",
    "2. \"answer\": Your final answer(s) as a single string or comma-separated list of options\n",
    "3. \"confidence\": A score from 0.0 to 1.0 representing your confidence level in this answer\n",
    "4. \"evidence_used\": The key evidence that supports your answer\n",
    "5. \"uncertainty_factors\": Any factors that reduce your confidence\n",
    "6. \"counterfactual\": What evidence would make you choose a different answer\n",
    "\"\"\"\n",
    "\n",
    "        if has_knowledge:\n",
    "            response_format += \"\"\"\n",
    "7. \"knowledge_contribution\": How the retrieved medical knowledge influenced your reasoning and answer\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert analyzing dermatological findings. Use the provided evidence to determine the most accurate answer(s) for the following question:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "MODEL PREDICTIONS:\n",
    "{model_prediction_text}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "{specialized_guidance}\n",
    "\n",
    "IMPORTANT: While multiple model predictions are provided, be aware that these predictions can be inaccurate or inconsistent. Do not assume majority agreement equals correctness. Evaluate the evidence critically and independently from these predictions. Your job is to determine the correct answer based primarily on the integrated evidence, treating model predictions as secondary suggestions that may contain errors.\n",
    "\n",
    "{task_description}\n",
    "\n",
    "{response_format}\n",
    "\n",
    "When providing your answer, strictly adhere to the available options and only select from them.\n",
    "\"\"\"\n",
    "\n",
    "        return base_prompt\n",
    "\n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178a2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfReflectionEngine:\n",
    "    \"\"\"Applies self-reflection to the reasoning process.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, args=None):\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "    \n",
    "    def apply_reflection(self, question_text, question_type, options, integrated_evidence, reasoning_result, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Apply self-reflection to the initial reasoning result.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence\n",
    "            reasoning_result: Initial reasoning result\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with reflection results\n",
    "        \"\"\"\n",
    "        prompt = self._create_reflection_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            reasoning_result,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reflection_text = response.text\n",
    "            \n",
    "            reflection_result = parse_json_response(reflection_text)\n",
    "            \n",
    "            if 'revised_answer' in reflection_result:\n",
    "                validated_answer = self._validate_answer(reflection_result.get('revised_answer', ''), options)\n",
    "                reflection_result['validated_revised_answer'] = validated_answer\n",
    "            \n",
    "            return reflection_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying reflection: {str(e)}\")\n",
    "            return {\n",
    "                \"reflection\": f\"Error: {str(e)}\",\n",
    "                \"requires_revision\": False,\n",
    "                \"confidence\": reasoning_result.get('confidence', 0.0),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _create_reflection_prompt(self, question_text, question_type, options, integrated_evidence, reasoning_result, retrieved_knowledge=None):\n",
    "        \"\"\"Create a prompt for the self-reflection layer.\"\"\"\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert critically reviewing your own reasoning about a dermatological question. \n",
    "Carefully examine the initial reasoning and check for errors, biases, and inconsistencies:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "INITIAL REASONING:\n",
    "{json.dumps(reasoning_result, indent=2)}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "Your task is to:\n",
    "1. Critically examine the initial reasoning for errors, biases, or incomplete analysis\n",
    "2. Identify any evidence that was overlooked or misinterpreted\n",
    "3. Evaluate whether the confidence level was appropriate\n",
    "4. Determine if a different answer would be more accurate\n",
    "5. Check if the evidence truly supports the chosen answer\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"reflection\": Your critical review of the initial reasoning\n",
    "2. \"overlooked_evidence\": Any important evidence that was missed or undervalued\n",
    "3. \"misinterpreted_evidence\": Any evidence that was incorrectly interpreted\n",
    "4. \"reasoning_gaps\": Logical gaps or assumptions in the initial reasoning\n",
    "5. \"confidence_assessment\": Was the confidence level appropriate? Why or why not?\n",
    "6. \"requires_revision\": Boolean indicating if the answer needs to be revised (true/false)\n",
    "7. \"revised_answer\": If revision is needed, the corrected answer\n",
    "8. \"revised_confidence\": If revision is needed, the corrected confidence level (0.0-1.0)\n",
    "9. \"revision_explanation\": If revision is needed, the explanation for the change\n",
    "\"\"\"\n",
    "\n",
    "        if has_knowledge:\n",
    "            base_prompt += \"\"\"\n",
    "10. \"knowledge_utilization_assessment\": Assessment of how well the initial reasoning utilized the available medical knowledge\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt += \"\"\"\n",
    "Be particularly careful to identify:\n",
    "- Cherry-picking: Did the initial reasoning focus only on evidence supporting its conclusion?\n",
    "- Overconfidence: Was the confidence level too high given the available evidence?\n",
    "- Alternative explanations: Are there valid alternative interpretations of the evidence?\n",
    "- Implicit assumptions: Were there unstated assumptions in the reasoning process?\n",
    "\n",
    "Be honest and thorough in your self-reflection, even if it means acknowledging errors in the initial reasoning.\n",
    "\"\"\"\n",
    "\n",
    "        return base_prompt\n",
    "\n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f93935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReAnalysisEngine:\n",
    "    \"\"\"Handles re-analysis when initial reasoning is insufficient.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, args=None):\n",
    "        self.client = client\n",
    "        self.args = args\n",
    "    \n",
    "    def deep_analysis(self, question_text, question_type, options, integrated_evidence, reasoning_result, reflection_result, retrieved_knowledge=None):\n",
    "        \"\"\"\n",
    "        Perform a deeper analysis based on reflection results.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence\n",
    "            reasoning_result: Initial reasoning result\n",
    "            reflection_result: Self-reflection result\n",
    "            retrieved_knowledge: Retrieved knowledge from the knowledge base (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with deep analysis result\n",
    "        \"\"\"\n",
    "        prompt = self._create_deep_analysis_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            reasoning_result,\n",
    "            reflection_result,\n",
    "            retrieved_knowledge\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.args.gemini_model if self.args else \"gemini-2.5-flash-preview-04-17\",\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "            \n",
    "            deep_analysis = parse_json_response(analysis_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(deep_analysis.get('final_answer', ''), options)\n",
    "            deep_analysis['validated_final_answer'] = validated_answer\n",
    "            \n",
    "            return deep_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error performing deep analysis: {str(e)}\")\n",
    "            return {\n",
    "                \"deep_reasoning\": f\"Error: {str(e)}\",\n",
    "                \"final_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                \"validated_final_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                \"final_confidence\": reasoning_result.get('confidence', 0.0),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _create_deep_analysis_prompt(self, question_text, question_type, options, integrated_evidence, reasoning_result, reflection_result, retrieved_knowledge=None):\n",
    "        \"\"\"Create a prompt for deep analysis.\"\"\"\n",
    "        has_knowledge = retrieved_knowledge is not None and retrieved_knowledge.get('retrieved', False)\n",
    "        \n",
    "        knowledge_section = \"\"\n",
    "        if has_knowledge:\n",
    "            results = retrieved_knowledge.get('results', [])\n",
    "            if results:\n",
    "                knowledge_texts = []\n",
    "                for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                    knowledge_texts.append(f\"RESULT {i+1}:\\nTopic: {result['topic']}\\nInformation: {result['information']}\")\n",
    "                \n",
    "                knowledge_section = f\"\"\"\n",
    "RETRIEVED MEDICAL KNOWLEDGE:\n",
    "{json.dumps(knowledge_texts, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt = f\"\"\"You are a medical expert performing a deep analysis for a dermatological question after identifying issues with initial reasoning.\n",
    "Review all evidence and reasoning paths comprehensively:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "INITIAL REASONING:\n",
    "{json.dumps(reasoning_result, indent=2)}\n",
    "\n",
    "REFLECTION:\n",
    "{json.dumps(reflection_result, indent=2)}\n",
    "\n",
    "{knowledge_section}\n",
    "\n",
    "Your task is to:\n",
    "1. Re-examine ALL available evidence with fresh eyes\n",
    "2. Address the specific issues highlighted in the reflection\n",
    "3. Consider each answer option systematically\n",
    "4. Weigh evidence for and against each potential answer\n",
    "5. Determine the most accurate answer based on comprehensive analysis\n",
    "\n",
    "For issues identified in reflection:\n",
    "- Overlooked evidence: {reflection_result.get('overlooked_evidence', 'None identified')}\n",
    "- Misinterpreted evidence: {reflection_result.get('misinterpreted_evidence', 'None identified')}\n",
    "- Reasoning gaps: {reflection_result.get('reasoning_gaps', 'None identified')}\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"deep_reasoning\": Your comprehensive analysis considering all evidence and perspectives\n",
    "2. \"systematic_assessment\": Assessment of evidence for EACH possible answer option\n",
    "3. \"final_answer\": Your conclusion after deep analysis\n",
    "4. \"final_confidence\": Your confidence level after deep analysis (0.0-1.0)\n",
    "5. \"key_determinants\": The most important factors that determined your final answer\n",
    "6. \"remaining_uncertainties\": Any unresolved questions or limitations\n",
    "\"\"\"\n",
    "\n",
    "        if has_knowledge:\n",
    "            base_prompt += \"\"\"\n",
    "7. \"knowledge_integration\": How you've incorporated medical knowledge into your final analysis\n",
    "\"\"\"\n",
    "\n",
    "        base_prompt += \"\"\"\n",
    "Be thorough, balanced, and precise in your analysis. Consider the evidence holistically and avoid the pitfalls identified in the reflection phase.\n",
    "\"\"\"\n",
    "\n",
    "        return base_prompt\n",
    "\n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "184c497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticDermatologyPipeline:\n",
    "    \"\"\"Main pipeline for agentic dermatology analysis with diagnosis-based retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None, args=None):\n",
    "        if api_key is None:\n",
    "            api_key = \"AIzaSyCCb63iuGCupIS_EDZ8S0qb2-38DA7mUbM\"\n",
    "        \n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.args = args\n",
    "        \n",
    "        # Initialize knowledge base and retrieval components\n",
    "        print(\"Initializing knowledge base...\")\n",
    "        self.kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        # Initialize diagnosis extractor\n",
    "        self.diagnosis_extractor = DiagnosisExtractor()\n",
    "        \n",
    "        # Initialize query generator\n",
    "        self.query_generator = DiagnosisBasedQueryGenerator(self.client)\n",
    "        \n",
    "        # Initialize knowledge retriever\n",
    "        self.knowledge_retriever = DiagnosisBasedKnowledgeRetriever(\n",
    "            self.kb_manager,\n",
    "            self.query_generator,\n",
    "            self.diagnosis_extractor\n",
    "        )\n",
    "        \n",
    "        # Initialize analysis components\n",
    "        self.image_analyzer = ImageAnalysisService(self.client, args=args)\n",
    "        self.clinical_analyzer = ClinicalContextAnalyzer(self.client, args=args)\n",
    "        self.evidence_integrator = EvidenceIntegrator(self.client, args=args)\n",
    "        self.reasoning_engine = ReasoningEngine(self.client, args=args)\n",
    "        self.reflection_engine = SelfReflectionEngine(self.client, args=args)\n",
    "        self.reanalysis_engine = ReAnalysisEngine(self.client, args=args)\n",
    "    \n",
    "    def process_single_encounter(self, agentic_data, encounter_id):\n",
    "        \"\"\"\n",
    "        Process a single encounter with all its questions using the agentic pipeline.\n",
    "\n",
    "        Args:\n",
    "            agentic_data: AgenticRAGData instance containing all encounter data\n",
    "            encounter_id: The specific encounter ID to process\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with all questions processed with agentic reasoning for this encounter\n",
    "        \"\"\"\n",
    "        all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "        encounter_pairs = [pair for pair in all_pairs if pair[0] == encounter_id]\n",
    "\n",
    "        if not encounter_pairs:\n",
    "            print(f\"No data found for encounter {encounter_id}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Processing {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "\n",
    "        encounter_results = {encounter_id: {}}\n",
    "\n",
    "        # Extract image analysis once per encounter\n",
    "        print(f\"Computing image analysis for {encounter_id}\")\n",
    "        sample_data = agentic_data.get_combined_data(encounter_pairs[0][0], encounter_pairs[0][1])\n",
    "        image_analysis = self.image_analyzer.analyze_images(sample_data['images'], encounter_id)\n",
    "\n",
    "        # Extract clinical context once per encounter\n",
    "        print(f\"Extracting clinical context for {encounter_id}\")\n",
    "        clinical_context = self.clinical_analyzer.extract_clinical_context(\n",
    "            sample_data['query_context'], \n",
    "            encounter_id\n",
    "        )\n",
    "\n",
    "        for i, (encounter_id, base_qid) in enumerate(encounter_pairs):\n",
    "            print(f\"Processing question {i+1}/{len(encounter_pairs)}: {base_qid}\")\n",
    "\n",
    "            sample_data = agentic_data.get_combined_data(encounter_id, base_qid)\n",
    "            if not sample_data:\n",
    "                print(f\"Warning: No data found for {encounter_id}, {base_qid}\")\n",
    "                continue\n",
    "\n",
    "            # Extract question details\n",
    "            question_text = sample_data['query_context'].split(\"MAIN QUESTION TO ANSWER:\")[1].split(\"\\n\")[0].strip()\n",
    "            question_type = sample_data['question_type']\n",
    "            options = sample_data['options']\n",
    "            model_predictions = sample_data['model_predictions']\n",
    "            \n",
    "            # First, do initial evidence integration without knowledge retrieval\n",
    "            print(f\"Initial evidence integration for {encounter_id}, {base_qid}\")\n",
    "            initial_integrated_evidence = self.evidence_integrator.integrate_evidence(\n",
    "                image_analysis,\n",
    "                clinical_context,\n",
    "                question_type\n",
    "            )\n",
    "            \n",
    "            # Now use diagnosis-based knowledge retrieval\n",
    "            print(f\"Retrieving knowledge based on diagnoses for {encounter_id}, {base_qid}\")\n",
    "            retrieved_knowledge = self.knowledge_retriever.retrieve_knowledge(\n",
    "                question_text,\n",
    "                question_type,\n",
    "                options,\n",
    "                image_analysis,\n",
    "                clinical_context,\n",
    "                initial_integrated_evidence\n",
    "            )\n",
    "            \n",
    "            # Integrate evidence with retrieved knowledge\n",
    "            print(f\"Integrating all evidence for {encounter_id}, {base_qid}\")\n",
    "            integrated_evidence = self.evidence_integrator.integrate_evidence(\n",
    "                image_analysis,\n",
    "                clinical_context,\n",
    "                question_type,\n",
    "                retrieved_knowledge\n",
    "            )\n",
    "\n",
    "            # Initial reasoning\n",
    "            print(f\"Initial reasoning for {encounter_id}, {base_qid}\")\n",
    "            reasoning_result = self.reasoning_engine.apply_initial_reasoning(\n",
    "                question_text,\n",
    "                question_type,\n",
    "                options,\n",
    "                integrated_evidence,\n",
    "                model_predictions,\n",
    "                retrieved_knowledge\n",
    "            )\n",
    "\n",
    "            # Determine if self-reflection is needed based on confidence\n",
    "            confidence = reasoning_result.get('confidence', 0.0)\n",
    "            if isinstance(confidence, str):\n",
    "                try:\n",
    "                    confidence = float(confidence)\n",
    "                except:\n",
    "                    confidence = 0.0\n",
    "\n",
    "            final_result = reasoning_result\n",
    "            reflection_path = []\n",
    "\n",
    "            # Apply self-reflection if confidence is below threshold\n",
    "            confidence_threshold = self.args.confidence_threshold if self.args else 0.75  # Default to 0.75 if args not available\n",
    "            if confidence < confidence_threshold:\n",
    "                print(f\"Confidence {confidence} below threshold. Applying self-reflection.\")\n",
    "\n",
    "                reflection_result = self.reflection_engine.apply_reflection(\n",
    "                    question_text,\n",
    "                    question_type,\n",
    "                    options,\n",
    "                    integrated_evidence,\n",
    "                    reasoning_result,\n",
    "                    retrieved_knowledge\n",
    "                )\n",
    "                reflection_path.append(reflection_result)\n",
    "\n",
    "                # Determine if re-analysis is needed based on reflection\n",
    "                requires_revision = reflection_result.get('requires_revision', False)\n",
    "                if requires_revision:\n",
    "                    print(f\"Reflection indicates revision needed. Performing deep analysis.\")\n",
    "\n",
    "                    deep_analysis = self.reanalysis_engine.deep_analysis(\n",
    "                        question_text,\n",
    "                        question_type,\n",
    "                        options,\n",
    "                        integrated_evidence,\n",
    "                        reasoning_result,\n",
    "                        reflection_result,\n",
    "                        retrieved_knowledge\n",
    "                    )\n",
    "                    reflection_path.append(deep_analysis)\n",
    "\n",
    "                    final_result = {\n",
    "                        \"reasoning\": deep_analysis.get('deep_reasoning', ''),\n",
    "                        \"answer\": deep_analysis.get('final_answer', 'Not mentioned'),\n",
    "                        \"validated_answer\": deep_analysis.get('validated_final_answer', 'Not mentioned'),\n",
    "                        \"confidence\": deep_analysis.get('final_confidence', 0.0)\n",
    "                    }\n",
    "                else:\n",
    "                    # Use original answer but with updated confidence if available\n",
    "                    revised_confidence = reflection_result.get('revised_confidence', reasoning_result.get('confidence', 0.0))\n",
    "                    final_result = {\n",
    "                        \"reasoning\": reasoning_result.get('reasoning', ''),\n",
    "                        \"answer\": reasoning_result.get('answer', 'Not mentioned'),\n",
    "                        \"validated_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                        \"confidence\": revised_confidence\n",
    "                    }\n",
    "\n",
    "            encounter_results[encounter_id][base_qid] = {\n",
    "                \"query_context\": sample_data['query_context'],\n",
    "                \"options\": sample_data['options'],\n",
    "                \"model_predictions\": sample_data['model_predictions'],\n",
    "                \"retrieved_knowledge\": retrieved_knowledge,\n",
    "                \"integrated_evidence\": integrated_evidence,\n",
    "                \"reasoning_result\": reasoning_result,\n",
    "                \"reflection_path\": reflection_path,\n",
    "                \"final_result\": final_result,\n",
    "                \"final_answer\": final_result.get('validated_answer', 'Not mentioned')\n",
    "            }\n",
    "\n",
    "        output_file = os.path.join(self.args.output_dir if self.args else Config.OUTPUT_DIR, f\"diagnosis_based_rag_results_{encounter_id}.json\")\n",
    "        \n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(encounter_results, f, indent=2)\n",
    "\n",
    "        print(f\"Processed all {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "        return encounter_results\n",
    "    \n",
    "    def format_results_for_evaluation(self, encounter_results, output_file):\n",
    "        \"\"\"Format results for official evaluation.\"\"\"\n",
    "        QIDS = [\n",
    "            \"CQID010-001\",\n",
    "            \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",\n",
    "            \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",\n",
    "            \"CQID015-001\",\n",
    "            \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "            \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",\n",
    "            \"CQID025-001\",\n",
    "            \"CQID034-001\",\n",
    "            \"CQID035-001\",\n",
    "            \"CQID036-001\",\n",
    "        ]\n",
    "        \n",
    "        qid_variants = {}\n",
    "        for qid in QIDS:\n",
    "            base_qid, variant = qid.split('-')\n",
    "            if base_qid not in qid_variants:\n",
    "                qid_variants[base_qid] = []\n",
    "            qid_variants[base_qid].append(qid)\n",
    "        \n",
    "        required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "        \n",
    "        formatted_predictions = []\n",
    "        for encounter_id, questions in encounter_results.items():\n",
    "            encounter_base_qids = set(questions.keys())\n",
    "            if not required_base_qids.issubset(encounter_base_qids):\n",
    "                print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "                continue\n",
    "            \n",
    "            pred_entry = {'encounter_id': encounter_id}\n",
    "            \n",
    "            for base_qid, question_data in questions.items():\n",
    "                if base_qid not in qid_variants:\n",
    "                    continue\n",
    "                \n",
    "                final_answer = question_data['final_answer']\n",
    "                options = question_data['options']\n",
    "                \n",
    "                not_mentioned_index = self._find_not_mentioned_index(options)\n",
    "                \n",
    "                self._process_answers(\n",
    "                    pred_entry, \n",
    "                    base_qid, \n",
    "                    final_answer, \n",
    "                    options, \n",
    "                    qid_variants, \n",
    "                    not_mentioned_index\n",
    "                )\n",
    "            \n",
    "            formatted_predictions.append(pred_entry)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(formatted_predictions, f, indent=2)\n",
    "        \n",
    "        print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "        return formatted_predictions\n",
    "    \n",
    "    def _find_not_mentioned_index(self, options):\n",
    "        \"\"\"Find the index of 'Not mentioned' in options.\"\"\"\n",
    "        for i, opt in enumerate(options):\n",
    "            if opt.lower() == \"not mentioned\":\n",
    "                return i\n",
    "        return len(options) - 1\n",
    "    \n",
    "    def _process_answers(self, pred_entry, base_qid, final_answer, options, qid_variants, not_mentioned_index):\n",
    "        \"\"\"Process answers and add to prediction entry.\"\"\"\n",
    "        if ',' in final_answer:\n",
    "            answer_parts = [part.strip() for part in final_answer.split(',')]\n",
    "            answer_indices = []\n",
    "            \n",
    "            for part in answer_parts:\n",
    "                found = False\n",
    "                for i, opt in enumerate(options):\n",
    "                    if part.lower() == opt.lower():\n",
    "                        answer_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found:\n",
    "                    answer_indices.append(not_mentioned_index)\n",
    "            \n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            for i, idx in enumerate(answer_indices):\n",
    "                if i < len(available_variants):\n",
    "                    pred_entry[available_variants[i]] = idx\n",
    "            \n",
    "            for i in range(len(answer_indices), len(available_variants)):\n",
    "                pred_entry[available_variants[i]] = not_mentioned_index\n",
    "            \n",
    "        else:\n",
    "            answer_index = not_mentioned_index\n",
    "            \n",
    "            for i, opt in enumerate(options):\n",
    "                if final_answer.lower() == opt.lower():\n",
    "                    answer_index = i\n",
    "                    break\n",
    "            \n",
    "            pred_entry[qid_variants[base_qid][0]] = answer_index\n",
    "            \n",
    "            if len(qid_variants[base_qid]) > 1:\n",
    "                for i in range(1, len(qid_variants[base_qid])):\n",
    "                    pred_entry[qid_variants[base_qid][i]] = not_mentioned_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "275c6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diagnosis_based_pipeline_all_encounters(args=None):\n",
    "    \"\"\"Run the diagnosis-based pipeline for all available encounters.\"\"\"\n",
    "    # Load model predictions and validation dataset\n",
    "    if args is None:\n",
    "        args = Args(use_finetuning=True, use_test_dataset=True)\n",
    "        \n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(args)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    validation_df = DataLoader.load_validation_dataset(args)\n",
    "    \n",
    "    # Create agentic data and pipeline\n",
    "    agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "    pipeline = AgenticDermatologyPipeline(args=args)\n",
    "    \n",
    "    # Get all unique encounter IDs\n",
    "    all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "    unique_encounter_ids = sorted(list(set(pair[0] for pair in all_pairs)))\n",
    "    print(f\"Found {len(unique_encounter_ids)} unique encounters to process\")\n",
    "    \n",
    "    # Process each encounter\n",
    "    all_encounter_results = {}\n",
    "    for i, encounter_id in enumerate(unique_encounter_ids):\n",
    "        print(f\"Processing encounter {i+1}/{len(unique_encounter_ids)}: {encounter_id}...\")\n",
    "        \n",
    "        try:\n",
    "            encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "            if encounter_results:\n",
    "                all_encounter_results.update(encounter_results)\n",
    "                \n",
    "            # Save intermediate results periodically\n",
    "            if (i+1) % 5 == 0 or (i+1) == len(unique_encounter_ids):\n",
    "                timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                intermediate_output_file = os.path.join(\n",
    "                    args.output_dir, \n",
    "                    f\"intermediate_diagnosis_based_results_{i+1}_of_{len(unique_encounter_ids)}_{timestamp}.json\"\n",
    "                )\n",
    "                with open(intermediate_output_file, 'w') as f:\n",
    "                    json.dump(all_encounter_results, f, indent=2)\n",
    "                print(f\"Saved intermediate results after processing {i+1} encounters\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing encounter {encounter_id}: {str(e)}\")\n",
    "            # Save error information\n",
    "            error_file = os.path.join(\n",
    "                args.output_dir, \n",
    "                f\"error_encounter_{encounter_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "            )\n",
    "            with open(error_file, 'w') as f:\n",
    "                f.write(f\"Error processing encounter {encounter_id}: {str(e)}\\n\")\n",
    "                f.write(f\"Traceback:\\n{traceback.format_exc()}\")\n",
    "    \n",
    "    # Format and save final predictions\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(\n",
    "        args.output_dir, \n",
    "        f\"{args.dataset_name}_data_cvqa_sys_diagnosis_based_all_{timestamp}.json\"\n",
    "    )\n",
    "    \n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(all_encounter_results, output_file)\n",
    "    \n",
    "    print(f\"Processed {len(formatted_predictions)} encounters successfully\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30d826c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diagnosis_based_pipeline(encounter_id, args=None):\n",
    "    \"\"\"Run the diagnosis-based pipeline for a single encounter.\"\"\"\n",
    "    # Create args if not provided\n",
    "    if args is None:\n",
    "        args = Args(use_finetuning=True, use_test_dataset=True)\n",
    "        \n",
    "    # Load model predictions and validation dataset\n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(args)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    validation_df = DataLoader.load_validation_dataset(args)\n",
    "    \n",
    "    # Create agentic data and pipeline\n",
    "    agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "    pipeline = AgenticDermatologyPipeline(args=args)\n",
    "    \n",
    "    # Process the encounter\n",
    "    encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "    \n",
    "    # Format and save predictions\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(\n",
    "        args.output_dir, \n",
    "        f\"{args.dataset_name}_data_cvqa_sys_diagnosis_based_{encounter_id}_{timestamp}.json\"\n",
    "    )\n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(encounter_results, output_file)\n",
    "    \n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432ed21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_diagnosis_based_pipeline(encounter_id):\n",
    "#     \"\"\"Run the diagnosis-based pipeline for a single encounter.\"\"\"\n",
    "#     # Load model predictions and validation dataset\n",
    "#     model_predictions_dict = DataLoader.load_all_model_predictions(Config.MODEL_PREDICTIONS_DIR)\n",
    "#     all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "#     validation_df = DataLoader.load_validation_dataset(Config.VAL_DATASET_PATH)\n",
    "    \n",
    "#     # Create agentic data and pipeline\n",
    "#     agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "#     pipeline = AgenticDermatologyPipeline()\n",
    "    \n",
    "#     # Process the encounter\n",
    "#     encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "    \n",
    "#     # Format and save predictions\n",
    "#     output_file = os.path.join(\n",
    "#         Config.OUTPUT_DIR, \n",
    "#         f\"test_data_cvqa_sys_diagnosis_based_{encounter_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "#     )\n",
    "#     formatted_predictions = pipeline.format_results_for_evaluation(encounter_results, output_file)\n",
    "    \n",
    "#     return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb019882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration initialized:\n",
      "- Using test dataset\n",
      "- Looking for finetuned model predictions\n",
      "- Dataset path: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/test_dataset.csv\n",
      "- Images directory: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/2025_dataset/test/images_test\n",
      "- Prediction file prefix: aggregated_test_predictions_\n",
      "Using existing knowledge base at /storage/scratch1/2/kthakrar3/mediqa-magic-v2/knowledge_db\n",
      "Initializing BM25 index...\n",
      "BM25 index initialization complete.\n",
      "Initializing knowledge base...\n",
      "Using existing knowledge base at /storage/scratch1/2/kthakrar3/mediqa-magic-v2/knowledge_db\n",
      "Initializing BM25 index...\n",
      "BM25 index initialization complete.\n",
      "Processing 9 questions for encounter ENC00908\n",
      "Computing image analysis for ENC00908\n",
      "Analyzing image 1/2 for encounter ENC00908\n",
      "Analyzing image 2/2 for encounter ENC00908\n",
      "Extracting clinical context for ENC00908\n",
      "Processing question 1/9: CQID010\n",
      "Initial evidence integration for ENC00908, CQID010\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID010\n",
      "Integrating all evidence for ENC00908, CQID010\n",
      "Initial reasoning for ENC00908, CQID010\n",
      "Processing question 2/9: CQID011\n",
      "Initial evidence integration for ENC00908, CQID011\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID011\n",
      "Integrating all evidence for ENC00908, CQID011\n",
      "Initial reasoning for ENC00908, CQID011\n",
      "Processing question 3/9: CQID012\n",
      "Initial evidence integration for ENC00908, CQID012\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID012\n",
      "Integrating all evidence for ENC00908, CQID012\n",
      "Initial reasoning for ENC00908, CQID012\n",
      "Processing question 4/9: CQID015\n",
      "Initial evidence integration for ENC00908, CQID015\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID015\n",
      "Integrating all evidence for ENC00908, CQID015\n",
      "Initial reasoning for ENC00908, CQID015\n",
      "Confidence 0.7439661621450613 below threshold. Applying self-reflection.\n",
      "Reflection indicates revision needed. Performing deep analysis.\n",
      "Processing question 5/9: CQID020\n",
      "Initial evidence integration for ENC00908, CQID020\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID020\n",
      "Integrating all evidence for ENC00908, CQID020\n",
      "Initial reasoning for ENC00908, CQID020\n",
      "Processing question 6/9: CQID025\n",
      "Initial evidence integration for ENC00908, CQID025\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID025\n",
      "Integrating all evidence for ENC00908, CQID025\n",
      "Initial reasoning for ENC00908, CQID025\n",
      "Processing question 7/9: CQID034\n",
      "Initial evidence integration for ENC00908, CQID034\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID034\n",
      "Integrating all evidence for ENC00908, CQID034\n",
      "Initial reasoning for ENC00908, CQID034\n",
      "Processing question 8/9: CQID035\n",
      "Initial evidence integration for ENC00908, CQID035\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID035\n",
      "Integrating all evidence for ENC00908, CQID035\n",
      "Initial reasoning for ENC00908, CQID035\n",
      "Processing question 9/9: CQID036\n",
      "Initial evidence integration for ENC00908, CQID036\n",
      "Retrieving knowledge based on diagnoses for ENC00908, CQID036\n",
      "Integrating all evidence for ENC00908, CQID036\n",
      "Initial reasoning for ENC00908, CQID036\n",
      "Processed all 9 questions for encounter ENC00908\n",
      "Formatted predictions saved to /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/test_data_cvqa_sys_diagnosis_based_ENC00908_20250504_234123.json (1 complete encounters)\n",
      "Processed encounter ENC00908 with 1 prediction entries\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create args with desired configuration\n",
    "    args = Args(use_finetuning=True, use_test_dataset=True)\n",
    "    \n",
    "    # Initialize the knowledge base first (if needed)\n",
    "    kb_manager = KnowledgeBaseManager(args=args)\n",
    "    \n",
    "    # Run for a single encounter\n",
    "    encounter_id = \"ENC00908\"\n",
    "    formatted_predictions = run_diagnosis_based_pipeline(encounter_id, args)\n",
    "    print(f\"Processed encounter {encounter_id} with {len(formatted_predictions)} prediction entries\")\n",
    "    \n",
    "    # Or alternatively run for all encounters\n",
    "    # formatted_predictions = run_diagnosis_based_pipeline_all_encounters(args)\n",
    "    # print(f\"Total complete encounters processed: {len(formatted_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb72406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'encounter_id': 'ENC00908',\n",
       "  'CQID010-001': 1,\n",
       "  'CQID011-001': 3,\n",
       "  'CQID011-002': 2,\n",
       "  'CQID011-003': 7,\n",
       "  'CQID011-004': 7,\n",
       "  'CQID011-005': 7,\n",
       "  'CQID011-006': 7,\n",
       "  'CQID012-001': 0,\n",
       "  'CQID012-002': 1,\n",
       "  'CQID012-003': 3,\n",
       "  'CQID012-004': 3,\n",
       "  'CQID012-005': 3,\n",
       "  'CQID012-006': 3,\n",
       "  'CQID015-001': 6,\n",
       "  'CQID020-001': 0,\n",
       "  'CQID020-002': 1,\n",
       "  'CQID020-003': 2,\n",
       "  'CQID020-004': 3,\n",
       "  'CQID020-005': 6,\n",
       "  'CQID020-006': 9,\n",
       "  'CQID020-007': 9,\n",
       "  'CQID020-008': 9,\n",
       "  'CQID020-009': 9,\n",
       "  'CQID025-001': 2,\n",
       "  'CQID034-001': 8,\n",
       "  'CQID035-001': 1,\n",
       "  'CQID036-001': 1}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec110aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
