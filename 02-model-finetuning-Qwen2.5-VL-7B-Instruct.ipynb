{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd0225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing TRANSFORMERS_CACHE: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/hf_cache\n",
      "HF_HOME: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/.hf_cache\n"
     ]
    }
   ],
   "source": [
    "# At the very top of your script, before any imports\n",
    "import os\n",
    "\n",
    "# Clear out any existing cache variables that might cause conflicts\n",
    "if \"TRANSFORMERS_CACHE\" in os.environ:\n",
    "    print(f\"Removing existing TRANSFORMERS_CACHE: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "    del os.environ[\"TRANSFORMERS_CACHE\"]\n",
    "\n",
    "# Set up environment variables and cache directories\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "print(f\"HF_HOME: {os.getenv('HF_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da607113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import traceback\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine learning and deep learning\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, MllamaForConditionalGeneration, AutoModelForVision2Seq, Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# TensorBoard related\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# Others\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12fab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.10 (main, Apr 15 2024, 11:52:16) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# Display environment information\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3271e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /storage/scratch1/2/kthakrar3/mediqa-magic-v2\n",
      "Selected model: Qwen2.5-VL-7B-Instruct\n",
      "Model ID: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "Is Llama model: False\n",
      "Is Qwen model: True\n",
      "Model output directory: /storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/finetuned-model/Qwen2.5-VL-7B-Instruct_20250425_1821\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables, i.e. Hugging Face token\n",
    "load_dotenv()\n",
    "\n",
    "# Model configuration section\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama-3.2-11b-vision\": \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    \"gemma-3-4b-it\": \"google/gemma-3-4b-it\", # NEED TO RETRY\n",
    "    \"gemma-3-12b-it\": \"google/gemma-3-12b-it\",  # NEED TO RETRY\n",
    "    \"Qwen2-VL-2B-Instruct\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"Qwen2-VL-7B-Instruct\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"Qwen2.5-VL-3B-Instruct\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"Qwen2.5-VL-7B-Instruct\": \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "}\n",
    "\n",
    "# Set the model to use - you can change this to select different models\n",
    "SELECTED_MODEL = \"Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"2025_dataset\")\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"valid\")\n",
    "TRAIN_IMAGES_DIR = os.path.join(TRAIN_DIR, \"images_train\")\n",
    "VAL_IMAGES_DIR = os.path.join(VAL_DIR, \"images_valid\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "PROCESSED_DATA_DIR = os.path.join(OUTPUT_DIR, \"processed_data-Qwen2.5-VL-7B-Instruct\")\n",
    "PROCESSED_VAL_DATA_DIR = os.path.join(OUTPUT_DIR, \"processed_val_data-Qwen2.5-VL-7B-Instruct\")\n",
    "TRAIN_CSV_FILE = os.path.join(OUTPUT_DIR, \"multi_label_dataset.csv\")\n",
    "QUESTIONS_PATH = os.path.join(TRAIN_DIR, \"closedquestions_definitions_imageclef2025.json\")\n",
    "VAL_JSON_PATH = os.path.join(VAL_DIR, \"valid.json\")\n",
    "CVQA_PATH = os.path.join(VAL_DIR, \"valid_cvqa.json\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = AVAILABLE_MODELS[SELECTED_MODEL]\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]  # Gets the model name from the path\n",
    "IS_LLAMA = \"llama\" in MODEL_ID.lower()\n",
    "IS_QWEN = \"qwen\" in MODEL_ID.lower()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Timestamp for model directory\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "MODEL_SAVE_DIRECTORY = os.path.join(OUTPUT_DIR, \"finetuned-model\", f\"{MODEL_NAME}_{TIMESTAMP}\")\n",
    "os.makedirs(MODEL_SAVE_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Selected model: {SELECTED_MODEL}\")\n",
    "print(f\"Model ID: {MODEL_ID}\")\n",
    "print(f\"Is Llama model: {IS_LLAMA}\")\n",
    "print(f\"Is Qwen model: {IS_QWEN}\")\n",
    "print(f\"Model output directory: {MODEL_SAVE_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0747824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/kthakrar3/mediqa-magic-v2/outputs/multi_label_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_CSV_FILE)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6298484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['encounter_id', 'base_qid', 'image_id', 'image_path', 'valid_answers',\n",
       "       'valid_indices', 'question_text', 'query_title_en', 'query_content_en',\n",
       "       'author_id', 'options_en', 'question_type_en', 'question_category_en',\n",
       "       'is_multi_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791c01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7893 samples for training\n"
     ]
    }
   ],
   "source": [
    "# train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fe66fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>valid_answers</th>\n",
       "      <th>valid_indices</th>\n",
       "      <th>question_text</th>\n",
       "      <th>query_title_en</th>\n",
       "      <th>query_content_en</th>\n",
       "      <th>author_id</th>\n",
       "      <th>options_en</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "      <th>is_multi_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00001_00001.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>['limited area']</td>\n",
       "      <td>[1]</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>U04473</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00001_00002.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>['limited area']</td>\n",
       "      <td>[1]</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>U04473</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00001_00001.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>['back']</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1 Where is the affected area?</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>U04473</td>\n",
       "      <td>['head', 'neck', 'upper extremities', 'lower e...</td>\n",
       "      <td>Site Location</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00001_00002.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>['back']</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1 Where is the affected area?</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>U04473</td>\n",
       "      <td>['head', 'neck', 'upper extremities', 'lower e...</td>\n",
       "      <td>Site Location</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID012</td>\n",
       "      <td>IMG_ENC00001_00001.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>['size of palm']</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1 How large are the affected areas? Please spe...</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>U04473</td>\n",
       "      <td>['size of thumb nail', 'size of palm', 'larger...</td>\n",
       "      <td>Size</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid                image_id  \\\n",
       "0     ENC00001  CQID010  IMG_ENC00001_00001.jpg   \n",
       "1     ENC00001  CQID010  IMG_ENC00001_00002.jpg   \n",
       "2     ENC00001  CQID011  IMG_ENC00001_00001.jpg   \n",
       "3     ENC00001  CQID011  IMG_ENC00001_00002.jpg   \n",
       "4     ENC00001  CQID012  IMG_ENC00001_00001.jpg   \n",
       "\n",
       "                                          image_path     valid_answers  \\\n",
       "0  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...  ['limited area']   \n",
       "1  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...  ['limited area']   \n",
       "2  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...          ['back']   \n",
       "3  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...          ['back']   \n",
       "4  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...  ['size of palm']   \n",
       "\n",
       "  valid_indices                                      question_text  \\\n",
       "0           [1]                  How much of the body is affected?   \n",
       "1           [1]                  How much of the body is affected?   \n",
       "2           [5]                      1 Where is the affected area?   \n",
       "3           [5]                      1 Where is the affected area?   \n",
       "4           [1]  1 How large are the affected areas? Please spe...   \n",
       "\n",
       "                         query_title_en  \\\n",
       "0  Pleural effusion accompanied by rash   \n",
       "1  Pleural effusion accompanied by rash   \n",
       "2  Pleural effusion accompanied by rash   \n",
       "3  Pleural effusion accompanied by rash   \n",
       "4  Pleural effusion accompanied by rash   \n",
       "\n",
       "                                    query_content_en author_id  \\\n",
       "0  A patient with pleural effusion is accompanied...    U04473   \n",
       "1  A patient with pleural effusion is accompanied...    U04473   \n",
       "2  A patient with pleural effusion is accompanied...    U04473   \n",
       "3  A patient with pleural effusion is accompanied...    U04473   \n",
       "4  A patient with pleural effusion is accompanied...    U04473   \n",
       "\n",
       "                                          options_en question_type_en  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "1  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "2  ['head', 'neck', 'upper extremities', 'lower e...    Site Location   \n",
       "3  ['head', 'neck', 'upper extremities', 'lower e...    Site Location   \n",
       "4  ['size of thumb nail', 'size of palm', 'larger...             Size   \n",
       "\n",
       "  question_category_en  is_multi_label  \n",
       "0              General           False  \n",
       "1              General           False  \n",
       "2              General           False  \n",
       "3              General           False  \n",
       "4              General           False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c3123d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_options(options_str):\n",
    "    \"\"\"\n",
    "    Safely convert a string representation of a list to an actual list.\n",
    "    \"\"\"\n",
    "    if not isinstance(options_str, str):\n",
    "        return options_str\n",
    "        \n",
    "    try:\n",
    "        # Use ast.literal_eval which is safer than eval()\n",
    "        return ast.literal_eval(options_str)\n",
    "    except (SyntaxError, ValueError):\n",
    "        # Try common formats\n",
    "        if options_str.startswith('[') and options_str.endswith(']'):\n",
    "            # Strip brackets and split by commas\n",
    "            return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "        elif ',' in options_str:\n",
    "            # Just split by commas\n",
    "            return [opt.strip() for opt in options_str.split(',')]\n",
    "        else:\n",
    "            # Single option\n",
    "            return [options_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5072c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples and save them as a pickle file.\n",
    "    Includes query title and content as clinical context.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get image path - using image_id instead of image_ids\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            # Use the full image path if it's already in the dataframe\n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                # Otherwise construct from images_dir and image_id\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get options from options_en\n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "                \n",
    "                # Clean up options by removing \"(please specify)\" phrases\n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        # Remove \"(please specify)\" from option text\n",
    "                        cleaned_opt = opt.replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(opt)\n",
    "                options = cleaned_options\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                \n",
    "            options_text = \", \".join(options)\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            # Get question text and clean it\n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Remove \"Please specify which affected area for each selection.\" from CQID012\n",
    "            if \"Please specify which affected area for each selection\" in question:\n",
    "                question = question.replace(\" Please specify which affected area for each selection.\", \"\")\n",
    "            \n",
    "            # Remove leading numbers like \"1 \" from the beginning of questions\n",
    "            question = re.sub(r'^\\d+\\s+', '', question)\n",
    "            \n",
    "            # Get clinical context from query title and content\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "            \n",
    "            # Create the clinical context section\n",
    "            clinical_context = \"\"\n",
    "            if query_title or query_content:\n",
    "                clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "                if query_title:\n",
    "                    clinical_context += f\"{query_title}\\n\"\n",
    "                if query_content:\n",
    "                    clinical_context += f\"{query_content}\\n\"\n",
    "\n",
    "            # Create the full query text with clinical context\n",
    "            query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Available Options (choose from these): {options_text}\")\n",
    "            \n",
    "            # Get answer text - from valid_answers and clean it\n",
    "            if 'valid_answers' in row and row['valid_answers']:\n",
    "                # For multi-label, join all valid answers\n",
    "                answers = row['valid_answers']\n",
    "                if isinstance(answers, list):\n",
    "                    # Clean the answers by removing \"(please specify)\"\n",
    "                    cleaned_answers = []\n",
    "                    for ans in answers:\n",
    "                        if isinstance(ans, str):\n",
    "                            # Remove any quotes and trailing/leading spaces\n",
    "                            cleaned_ans = ans.strip(\"'\\\" \")\n",
    "                            # Remove \"(please specify)\"\n",
    "                            cleaned_ans = cleaned_ans.replace(\" (please specify)\", \"\")\n",
    "                            cleaned_answers.append(cleaned_ans)\n",
    "                        else:\n",
    "                            cleaned_answers.append(str(ans).strip(\"'\\\" \"))\n",
    "                    \n",
    "                    if len(cleaned_answers) > 1:\n",
    "                        # Join multiple answers with commas\n",
    "                        answer_text = \", \".join(cleaned_answers)\n",
    "                    elif len(cleaned_answers) == 1:\n",
    "                        answer_text = cleaned_answers[0]\n",
    "                    else:\n",
    "                        answer_text = \"Not mentioned\"\n",
    "                else:\n",
    "                    # Clean single answer\n",
    "                    if isinstance(answers, str):\n",
    "                        # Remove any quotes and leading/trailing spaces\n",
    "                        answer_text = answers.strip(\"'\\\" \")\n",
    "                        answer_text = answer_text.replace(\" (please specify)\", \"\")\n",
    "                    else:\n",
    "                        answer_text = str(answers).strip(\"'\\\" \")\n",
    "                        \n",
    "                # Make sure the answer is stored as plain text, not as a string representation of a list\n",
    "#                 if isinstance(answer_text, str) and answer_text.startswith(\"['\") and answer_text.endswith(\"']\"):\n",
    "                if isinstance(answer_text, str) and answer_text.startswith(\"[\") and answer_text.endswith(\"]\"):\n",
    "                    # Extract content from list representation\n",
    "                    clean_text = answer_text.strip(\"[]'\")\n",
    "                    # Split by quoted separators and rejoin with clean commas\n",
    "                    parts = [part.strip() for part in clean_text.split(\"', '\")]\n",
    "                    answer_text = \", \".join(parts)\n",
    "            \n",
    "            elif 'multi_label' in row:\n",
    "                answer_text = row['multi_label']\n",
    "            else:\n",
    "                answer_text = \"Not mentioned\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"answer_text\": answer_text,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"outputsprocessed_data\", images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the entire dataset in batches\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Use train_images_dir global variable if images_dir is not provided\n",
    "    if images_dir is None:\n",
    "        images_dir = TRAIN_IMAGES_DIR\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a6262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be31b8f2c814cd0a2cad3f1bea04045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples so far\n",
      "Processing batch 2/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd43014058545d7b48b34edb0a907ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 examples so far\n",
      "Processing batch 3/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14189c601bae4c729ff55e68db57049a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300 examples so far\n",
      "Processing batch 4/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3f15aa6cc04a29be3f67d47571925b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 examples so far\n",
      "Processing batch 5/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1ff82c4f83459fbd52c4df294293f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 examples so far\n",
      "Processing batch 6/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff074691a7524c75bd241be8a4676b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600 examples so far\n",
      "Processing batch 7/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6115e82b61a546d99c88d3a8cfd2bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 6:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 700 examples so far\n",
      "Processing batch 8/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcded83732b4f00be042c10ebadd16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 7:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 800 examples so far\n",
      "Processing batch 9/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf992c06d2d4628b51e4aeb7362d6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 8:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 900 examples so far\n",
      "Processing batch 10/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f1a8b75c574d278af522d6129036ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 9:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 examples so far\n",
      "Processing batch 11/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d5523c62b846149df6003d23008daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 10:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1100 examples so far\n",
      "Processing batch 12/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75f2e6fe0024ee5bd39883516ddbc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 11:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1200 examples so far\n",
      "Processing batch 13/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1362b17a7b7848f0b3e089f7c7dd69c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 12:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1300 examples so far\n",
      "Processing batch 14/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc7606b213a4c4e82c4fa8bc1cfca81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 13:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400 examples so far\n",
      "Processing batch 15/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce597e979d6a4530b94bd942a3184832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 14:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1500 examples so far\n",
      "Processing batch 16/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc68a705fc3b40ff869568a40c3f5328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 15:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1600 examples so far\n",
      "Processing batch 17/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d496b73f5cda4b95a85ce32564c55197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 16:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1700 examples so far\n",
      "Processing batch 18/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6a3961abb74a6db2809835dc1bb8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 17:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800 examples so far\n",
      "Processing batch 19/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2786a4498f4728826203043f68145d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 18:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1900 examples so far\n",
      "Processing batch 20/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61348df9d4b24dbfa992da65dce8a551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 19:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 examples so far\n",
      "Processing batch 21/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4181c086e9be4d5d8657bad286063e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 20:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2100 examples so far\n",
      "Processing batch 22/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061f6664ba684cf095019f7f7532c199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 21:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2200 examples so far\n",
      "Processing batch 23/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01d3267c3334e0cbdca51b2e3635ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 22:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2300 examples so far\n",
      "Processing batch 24/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dbefd11d4141c79045ca524dcdbf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 23:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2400 examples so far\n",
      "Processing batch 25/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0842352f3e4155ad6253c8811c3c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 24:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 examples so far\n",
      "Processing batch 26/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cd4f1191934eaab64c5d89dccf8c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 25:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2600 examples so far\n",
      "Processing batch 27/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c190d0f624b4138b9571bbeef652826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 26:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2700 examples so far\n",
      "Processing batch 28/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fe383ac6f344c2882f18ec400a4d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 27:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2800 examples so far\n",
      "Processing batch 29/79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7d98cffe1e4ef3aabdd79b30697762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 28:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear any existing processed data (optional)\n",
    "if os.path.exists(PROCESSED_DATA_DIR):\n",
    "    shutil.rmtree(PROCESSED_DATA_DIR)\n",
    "    \n",
    "# Process the dataset\n",
    "total_examples = preprocess_dataset(train_df, batch_size=100, save_dir=PROCESSED_DATA_DIR)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9268c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the processed data\n",
    "batch_file = os.path.join(PROCESSED_DATA_DIR, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[0]\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "print(\"\\nSample of processed data (second example):\")\n",
    "sample_data = batch_data[1]\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_llm_training_inputs(PROCESSED_DATA_DIR, qids=[\"CQID034\", \"CQID012\", \"CQID011\"], \n",
    "                               multi_label_examples=True, single_label_examples=True, \n",
    "                               num_samples_per_qid=1, show_images=True):\n",
    "    \"\"\"\n",
    "    Shows the exact inputs that would be sent to the LLM during training.\n",
    "    \n",
    "    Args:\n",
    "        PROCESSED_DATA_DIR: Directory containing processed batch files\n",
    "        qids: List of QIDs to look for\n",
    "        multi_label_examples: Whether to include multi-label examples\n",
    "        single_label_examples: Whether to include single-label examples\n",
    "        num_samples_per_qid: How many samples to show for each QID\n",
    "        show_images: Whether to display the images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Look in multiple batch files\n",
    "    batch_files = sorted([f for f in os.listdir(PROCESSED_DATA_DIR) \n",
    "                          if f.startswith(\"batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    # Dictionary to store examples by QID and label type\n",
    "    examples_by_qid = defaultdict(lambda: {\"multi\": [], \"single\": []})\n",
    "    \n",
    "    # Scan batch files to find examples\n",
    "    for batch_file in batch_files:\n",
    "        file_path = os.path.join(PROCESSED_DATA_DIR, batch_file)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            # Filter by requested QIDs\n",
    "            for sample in batch_data:\n",
    "                sample_qid = sample['qid']\n",
    "                if any(sample_qid.startswith(qid) for qid in qids):\n",
    "                    # Check if it's multi-label\n",
    "                    is_multi_label = ',' in sample['answer_text']\n",
    "                    \n",
    "                    # Store in appropriate category\n",
    "                    if is_multi_label and multi_label_examples:\n",
    "                        examples_by_qid[sample_qid][\"multi\"].append(sample)\n",
    "                    elif not is_multi_label and single_label_examples:\n",
    "                        examples_by_qid[sample_qid][\"single\"].append(sample)\n",
    "            \n",
    "            # Check if we've found enough examples for each QID\n",
    "            all_found = True\n",
    "            for qid in qids:\n",
    "                qid_examples = [key for key in examples_by_qid.keys() if key.startswith(qid)]\n",
    "                if not qid_examples:\n",
    "                    all_found = False\n",
    "                    break\n",
    "                    \n",
    "                for q in qid_examples:\n",
    "                    if multi_label_examples and len(examples_by_qid[q][\"multi\"]) < num_samples_per_qid:\n",
    "                        if not examples_by_qid[q][\"multi\"]:  # Only if we have no examples yet\n",
    "                            all_found = False\n",
    "                    if single_label_examples and len(examples_by_qid[q][\"single\"]) < num_samples_per_qid:\n",
    "                        if not examples_by_qid[q][\"single\"]:  # Only if we have no examples yet\n",
    "                            all_found = False\n",
    "            \n",
    "            if all_found:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {batch_file}: {e}\")\n",
    "    \n",
    "    # Display the examples\n",
    "    sample_count = 0\n",
    "    \n",
    "    for qid in qids:\n",
    "        matching_qids = [key for key in examples_by_qid.keys() if key.startswith(qid)]\n",
    "        \n",
    "        if not matching_qids:\n",
    "            print(f\"\\nNo examples found for QID {qid}\")\n",
    "            continue\n",
    "            \n",
    "        for matching_qid in matching_qids:\n",
    "            # Display multi-label examples\n",
    "            if multi_label_examples and examples_by_qid[matching_qid][\"multi\"]:\n",
    "                examples = examples_by_qid[matching_qid][\"multi\"][:num_samples_per_qid]\n",
    "                for i, sample in enumerate(examples):\n",
    "                    sample_count += 1\n",
    "                    display_llm_training_input(sample, sample_count, show_images)\n",
    "            \n",
    "            # Display single-label examples\n",
    "            if single_label_examples and examples_by_qid[matching_qid][\"single\"]:\n",
    "                examples = examples_by_qid[matching_qid][\"single\"][:num_samples_per_qid]\n",
    "                for i, sample in enumerate(examples):\n",
    "                    sample_count += 1\n",
    "                    display_llm_training_input(sample, sample_count, show_images)\n",
    "    \n",
    "    if sample_count == 0:\n",
    "        print(\"No matching examples found. Try different QIDs or check your data directory.\")\n",
    "\n",
    "def display_llm_training_input(sample, sample_num, show_images=True):\n",
    "    \"\"\"\n",
    "    Display a single example formatted exactly as it would be sent to the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"EXAMPLE #{sample_num}: {sample['qid']}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"Question ID: {sample['qid']}\")\n",
    "    print(f\"Type: {sample['question_type']}\")\n",
    "    print(f\"Category: {sample['question_category']}\")\n",
    "    print(f\"Image path: {sample['image_path']}\")\n",
    "    \n",
    "    # Determine if multi-label\n",
    "    is_multi_label = ',' in sample['answer_text']\n",
    "    print(f\"Multi-label: {is_multi_label}\")\n",
    "    \n",
    "    # Show the image if requested\n",
    "    if show_images and os.path.exists(sample['image_path']):\n",
    "        try:\n",
    "            img = Image.open(sample['image_path'])\n",
    "            width, height = img.size\n",
    "            print(f\"Image dimensions: {width}x{height}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image for {os.path.basename(sample['image_path'])}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image: {e}\")\n",
    "    \n",
    "    # Define the system message as in your MedicalImageDataset class   \n",
    "    system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "    IMPORTANT: \n",
    "    - Respond ONLY with the exact text of the option(s) that apply\n",
    "    - Do not provide any explanations\n",
    "    - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "    - Do not write \"Options:\" or similar prefixes\n",
    "    - Do not write \"Answer:\" or similar prefixes\n",
    "    - Multiple answers should be separated by commas\n",
    "    - If unsure, respond with \"Not mentioned\"\n",
    "    - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "    - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format as would be sent to the LLM\n",
    "    print(\"\\nACTUAL LLM INPUT (as formatted in your MedicalImageDataset):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\"System message:\")\n",
    "    print(system_message)\n",
    "    print(\"\\nUser message:\")\n",
    "    print(sample['query_text'])\n",
    "    print(\"[IMAGE WOULD BE INCLUDED HERE]\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Expected output\n",
    "    print(\"\\nEXPECTED LLM OUTPUT:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(sample['answer_text'])\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # If multi-label, show split answers\n",
    "    if is_multi_label:\n",
    "        answers = [ans.strip() for ans in sample['answer_text'].split(\",\")]\n",
    "#         print(f\"\\nSplit answers:\")\n",
    "#         for ans in answers:\n",
    "#             print(f\"- {ans}\")\n",
    "    \n",
    "    # Show what this would look like after chat template is applied\n",
    "    print(\"\\nCHAT TEMPLATE FORMAT (approximate):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\"<bos><begin_of_system>\\nYou are a medical image analysis assistant. Your task is to examine the provided clinical images along with clinical context, and select the option(s) that best describe what you see. \\nIMPORTANT: You must respond ONLY with the exact text of the option(s) that apply. \\n- Do not provide any explanations\\n- Do not include option numbers\\n- Do not write \\\"Options:\\\" or similar prefixes\\n- Do not write \\\"Answer:\\\" or similar prefixes\\n- Multiple answers should be separated by commas\\n- If unsure, respond with \\\"Not mentioned\\\"\\n<end_of_system>\\n\\n<begin_of_user>\\n\" + sample['query_text'] + \"\\n<boi>IMAGE_EMBEDDING_TOKENS<eoi>\\n<end_of_user>\\n\\n<begin_of_assistant>\\n\" + sample['answer_text'] + \"<end_of_assistant>\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Example usage\n",
    "inspect_llm_training_inputs(\n",
    "    PROCESSED_DATA_DIR,  # Update with your directory\n",
    "    qids=[\"CQID034\", \"CQID012\", \"CQID011\"],      # QIDs to look for\n",
    "    multi_label_examples=True,                    # Include multi-label examples\n",
    "    single_label_examples=True,                   # Include single-label examples\n",
    "    num_samples_per_qid=1,                        # Number of examples per QID\n",
    "    show_images=True                              # Show the images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(torch_dtype=None):\n",
    "    \"\"\"Create standardized model configuration dictionary\"\"\"\n",
    "    if torch_dtype is None:\n",
    "        torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        \n",
    "    # Configure base model parameters (common to all models)\n",
    "    model_kwargs = dict(\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    # Configure quantization for memory efficiency\n",
    "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "        bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"]\n",
    "    )\n",
    "    \n",
    "    return model_kwargs\n",
    "\n",
    "# Check if GPU can support bfloat16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8:\n",
    "    print(\"WARNING: GPU may not fully support bfloat16. Consider using float16 instead.\")\n",
    "\n",
    "# Get standardized model configuration\n",
    "model_kwargs = get_model_config(torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Load the model based on model type\n",
    "if IS_LLAMA:\n",
    "    # Use MllamaForConditionalGeneration for Llama models\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "    if hasattr(model, \"tie_weights\"):\n",
    "        model.tie_weights()\n",
    "elif IS_QWEN:\n",
    "    # Use Qwen2VLForConditionalGeneration for Qwen models\n",
    "    if \"2.5\" in MODEL_ID:\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "    else:\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "else:\n",
    "    # For non-Llama, non-Qwen models, use AutoModelForImageTextToText with eager attention\n",
    "    non_llama_kwargs = model_kwargs.copy()\n",
    "    non_llama_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "    model = AutoModelForImageTextToText.from_pretrained(MODEL_ID, **non_llama_kwargs)\n",
    "\n",
    "# Load processor first to use in the dataset class\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "# Print info about the chat template\n",
    "print(f\"Default chat template: {processor.tokenizer.chat_template}\")\n",
    "print(f\"Special tokens map: {processor.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Open image and convert to RGB\n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        # Define system message for medical image analysis\n",
    "        system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "        IMPORTANT: \n",
    "        - Respond ONLY with the exact text of the option(s) that apply\n",
    "        - Do not provide any explanations\n",
    "        - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\"\n",
    "        - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "        - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "        \"\"\"\n",
    "        \n",
    "        # Format as a conversation with system, user, and assistant messages\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function with specific handling for different model types\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function for batching examples for vision models.\"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Extract image from messages\n",
    "        image_input = None\n",
    "        for msg in example[\"messages\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                for content in msg[\"content\"]:\n",
    "                    if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                        image_input = content[\"image\"]\n",
    "                        break\n",
    "        \n",
    "        if image_input is None:\n",
    "            # Create a dummy image if needed\n",
    "            image_input = Image.new('RGB', (224, 224), color='black')\n",
    "            \n",
    "        # Process the conversation using the chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        texts.append(text.strip())\n",
    "        images.append([image_input])\n",
    "    \n",
    "    # Process both text and images\n",
    "    batch = processor(\n",
    "        text=texts, \n",
    "        images=images,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Create the labels for loss calculation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask padding tokens\n",
    "    if processor.tokenizer.pad_token_id is not None:\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Handle special tokens based on model type\n",
    "    if IS_LLAMA:\n",
    "        # Mask all special tokens in Llama models\n",
    "        for token_id in processor.tokenizer.all_special_ids:\n",
    "            labels[labels == token_id] = -100\n",
    "        \n",
    "        # Check if processor has image_token attribute and mask it\n",
    "        try:\n",
    "            if hasattr(processor, \"image_token\"):\n",
    "                image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\n",
    "                labels[labels == image_token_id] = -100\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # Specifically handle vision tokens for Llama\n",
    "        if hasattr(processor, \"image_token_index\"):\n",
    "            # If the processor exposes the image token directly\n",
    "            labels[labels == processor.image_token_index] = -100\n",
    "        else:\n",
    "            # Try to find image token via known token patterns\n",
    "            # Llama Vision uses special tokens for image embeddings\n",
    "            image_tokens = []\n",
    "            \n",
    "            # Try to get image token by name - including BOS and EOS explicitly\n",
    "            image_token_names = [\"<boi>\", \"<eoi>\", \"<image>\", \"<|image|>\", \"bos_token\", \"eos_token\"]\n",
    "            for token_name in image_token_names:\n",
    "                try:\n",
    "                    token_id = processor.tokenizer.convert_tokens_to_ids(token_name)\n",
    "                    if token_id != processor.tokenizer.unk_token_id:\n",
    "                        image_tokens.append(token_id)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            # Mask any found image tokens\n",
    "            for token_id in image_tokens:\n",
    "                labels[labels == token_id] = -100\n",
    "                \n",
    "        # Handle begin/end system tokens and other special tokens\n",
    "        special_token_patterns = [\"<|begin_of_\", \"<|end_of_\", \"<|start_header\", \"<|end_header\", \n",
    "                                \"<|eot_id|>\", \"<|begin_of_text|>\"]\n",
    "        for pattern in special_token_patterns:\n",
    "            for token, token_id in processor.tokenizer.get_vocab().items():\n",
    "                if pattern in token:\n",
    "                    labels[labels == token_id] = -100\n",
    "    \n",
    "    elif IS_QWEN:\n",
    "        # Qwen-specific token masking\n",
    "        # These are the tokens used for image embeddings in Qwen2-VL\n",
    "        image_tokens = [151652, 151653, 151655]  # Qwen2-VL image token IDs\n",
    "        for token_id in image_tokens:\n",
    "            labels[labels == token_id] = -100\n",
    "        \n",
    "        # Also mask all special tokens \n",
    "        for token_id in processor.tokenizer.all_special_ids:\n",
    "            labels[labels == token_id] = -100\n",
    "            \n",
    "    else:\n",
    "        # Gemma or other model special tokens\n",
    "        try:\n",
    "            for special_token in [\"boi_token\", \"eoi_token\"]:\n",
    "                if special_token in processor.tokenizer.special_tokens_map:\n",
    "                    token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "                        processor.tokenizer.special_tokens_map[special_token]\n",
    "                    )\n",
    "                    labels[labels == token_id] = -100\n",
    "            \n",
    "            # Handle Gemma's specific image token\n",
    "            labels[labels == 262144] = -100\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not mask tokens: {e}\")\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the dataset\n",
    "dataset = MedicalImageDataset(PROCESSED_DATA_DIR, processor)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Confirm we are correctly loading examples from the processed data\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    # Display the message structure for each sample\n",
    "    for i in range(sample_size):\n",
    "        example = dataset[i]\n",
    "        print(f\"\\nExample {i+1} message structure:\")\n",
    "        print(example)\n",
    "    \n",
    "    # Test the collate function\n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"\\nCollated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c64995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    # Process each batch. Note: in training, pass this to model.forward()\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32722ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA based on model type\n",
    "if IS_LLAMA or IS_QWEN:\n",
    "    # Use more limited target modules to reduce memory requirements\n",
    "    target_modules = [\"q_proj\", \"v_proj\"] # can adjust this eventually\n",
    "else:\n",
    "    target_modules = \"all-linear\"\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8, # try r8 to be more memory efficient eventually # was previously 16\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=None if IS_LLAMA else [\"lm_head\", \"embed_tokens\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c35280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training configuration\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=MODEL_SAVE_DIRECTORY,\n",
    "    num_train_epochs=3,  # Adjust based on dataset size\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=32,  # Accumulate gradients to simulate larger batch # was previously 8\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True, # NEW - hope this isnt an issue with gemma finetuning\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hub\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,  # Critical for custom datasets\n",
    "    label_names=[\"labels\"],  # Explicitly setting label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5229d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with components that vary by model type\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ec859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to find the latest training output directory\n",
    "def find_latest_training_dir():\n",
    "    # Find all finetuned model directories with the proper structure\n",
    "    model_dirs = glob.glob(os.path.join(OUTPUT_DIR, \"finetuned-model\", \"*\"))\n",
    "    \n",
    "    if not model_dirs:\n",
    "        raise FileNotFoundError(\"No finetuned model directories found\")\n",
    "    \n",
    "    # Get the most recent directory\n",
    "    latest_dir = max(model_dirs, key=os.path.getmtime)\n",
    "    return latest_dir\n",
    "\n",
    "def plot_training_loss(training_dir=None, window_size=10):\n",
    "    # Find the latest training directory if not provided\n",
    "    if training_dir is None:\n",
    "        training_dir = find_latest_training_dir()\n",
    "    \n",
    "    # Extract model name and timestamp from directory name\n",
    "    dir_name = os.path.basename(training_dir)\n",
    "    \n",
    "    # Find the tensorboard logs directory\n",
    "    log_dir = os.path.join(training_dir, \"runs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise FileNotFoundError(f\"TensorBoard logs not found in {log_dir}\")\n",
    "    \n",
    "    # Find all event files\n",
    "    event_files = []\n",
    "    for root, dirs, files in os.walk(log_dir):\n",
    "        for file in files:\n",
    "            if file.startswith(\"events.out.tfevents\"):\n",
    "                event_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(\"No TensorBoard event files found\")\n",
    "    \n",
    "    # Get the most recent event file\n",
    "    latest_event = max(event_files, key=os.path.getmtime)\n",
    "    \n",
    "    # Load the events using TensorBoard's event accumulator\n",
    "    ea = event_accumulator.EventAccumulator(os.path.dirname(latest_event))\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Check available tags\n",
    "    tags = ea.Tags()\n",
    "    loss_tag = None\n",
    "    for tag in tags['scalars']:\n",
    "        if 'loss' in tag.lower():\n",
    "            loss_tag = tag\n",
    "            break\n",
    "    \n",
    "    if not loss_tag:\n",
    "        raise ValueError(\"No loss data found in TensorBoard logs\")\n",
    "    \n",
    "    # Extract the training loss data\n",
    "    train_loss_steps = []\n",
    "    train_loss_values = []\n",
    "    for event in ea.Scalars(loss_tag):\n",
    "        train_loss_steps.append(event.step)\n",
    "        train_loss_values.append(event.value)\n",
    "    \n",
    "    # Create a DataFrame for easier manipulation\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Step': train_loss_steps,\n",
    "        'Training Loss': train_loss_values\n",
    "    })\n",
    "    \n",
    "    # Plot the training loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_df['Step'], loss_df['Training Loss'], label='Training Loss')\n",
    "    plt.title(f'Training Loss Curve for {dir_name}')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add moving average to smooth the curve if there are enough data points\n",
    "    if len(loss_df) > window_size:\n",
    "        loss_df['Moving Avg'] = loss_df['Training Loss'].rolling(window=window_size).mean()\n",
    "        plt.plot(loss_df['Step'], loss_df['Moving Avg'], 'r-', \n",
    "                 label=f'Moving Average (window={window_size})')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(OUTPUT_DIR, f\"training_loss_{dir_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training loss plot saved to {plot_path}\")\n",
    "    \n",
    "    # Print a summary table of the loss values\n",
    "    step_interval = max(1, len(loss_df) // 20)  # Show ~20 rows\n",
    "    print(\"\\nTraining Loss Summary:\")\n",
    "    print(\"Step\\tTraining Loss\\tMoving Avg\")\n",
    "    for i in range(0, len(loss_df), step_interval):\n",
    "        if 'Moving Avg' in loss_df.columns:\n",
    "            print(f\"{loss_df.iloc[i]['Step']:.0f}\\t{loss_df.iloc[i]['Training Loss']:.4f}\\t{loss_df.iloc[i]['Moving Avg']:.5f}\")\n",
    "        else:\n",
    "            print(f\"{loss_df.iloc[i]['Step']:.0f}\\t{loss_df.iloc[i]['Training Loss']:.4f}\\tNaN\")\n",
    "    \n",
    "    return loss_df\n",
    "\n",
    "# Example usage:\n",
    "plot_training_loss()  # Automatically find and plot the latest training run\n",
    "# Or specify a specific training directory:\n",
    "# plot_training_loss(os.path.join(OUTPUT_DIR, \"finetuned-model\", f\"{MODEL_NAME}_{TIMESTAMP}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b68d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just return the checkpoint path for both model types\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, use_finetuning=True):\n",
    "        \"\"\"\n",
    "        Initialize inference arguments with options for finetuning.\n",
    "        \n",
    "        Parameters:\n",
    "        - use_finetuning: Whether to use the fine-tuned model (True) or base model (False)\n",
    "        \"\"\"\n",
    "        self.test = True\n",
    "        self.skip_data_prep = False\n",
    "        self.batch_size = 100\n",
    "        self.max_samples = None\n",
    "        self.use_finetuning = use_finetuning\n",
    "        \n",
    "        # Find the latest checkpoint for reference (if we need it)\n",
    "        self.latest_checkpoint = None\n",
    "        if use_finetuning:\n",
    "            checkpoint_pattern = os.path.join(OUTPUT_DIR, \"finetuned-model\", f\"{MODEL_NAME}_*\", \"checkpoint-*\")\n",
    "            checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "            \n",
    "            if not checkpoint_dirs:\n",
    "                raise FileNotFoundError(f\"No checkpoints found for model {MODEL_NAME}\")\n",
    "            \n",
    "            # Sort by checkpoint number\n",
    "            checkpoint_dirs = sorted(checkpoint_dirs, \n",
    "                                    key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)), \n",
    "                                    reverse=True)\n",
    "            \n",
    "            self.latest_checkpoint = checkpoint_dirs[0]\n",
    "        \n",
    "        # Simplified handling - same for all model types\n",
    "        if use_finetuning:\n",
    "            # Use adapter approach for any model with fine-tuning\n",
    "            self.model_path = MODEL_ID\n",
    "            self.adapter_path = self.latest_checkpoint\n",
    "            print(f\"Using model with adapter fine-tuning.\")\n",
    "            print(f\"Base model: {self.model_path}\")\n",
    "            print(f\"Adapter path: {self.adapter_path}\")\n",
    "        else:\n",
    "            # Base model only (no finetuning)\n",
    "            self.model_path = MODEL_ID\n",
    "            self.adapter_path = None\n",
    "            print(f\"Using base model WITHOUT fine-tuning.\")\n",
    "            print(f\"Base model: {self.model_path}\")\n",
    "        \n",
    "        # Print inference model type\n",
    "        print(\"\\nModel for inference:\", \n",
    "              \"FINE-TUNED\" if self.use_finetuning else \"BASE MODEL (NO FINE-TUNING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Args and see which model is being used\n",
    "args = Args()\n",
    "print(f\"Selected model path: {args.model_path}\")\n",
    "print(f\"Using fine-tuned model: {args.use_finetuning}\")\n",
    "print(f\"Using test dataset: {args.test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare validation data\n",
    "def prepare_validation_data():\n",
    "    \"\"\"\n",
    "    Create a validation dataframe similar to the training dataframe.\n",
    "    \"\"\"\n",
    "    print(\"Preparing validation data...\")\n",
    "    \n",
    "    # Load question definitions\n",
    "    with open(QUESTIONS_PATH, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "        \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    questions_df = pd.json_normalize(questions)[[\"qid\", \"question_en\", \"options_en\", \"question_type_en\", \"question_category_en\"]]\n",
    "    \n",
    "    # Load validation data with query information\n",
    "    val_df = pd.read_json(VAL_JSON_PATH)\n",
    "    \n",
    "    # Extract relevant columns including query content and title\n",
    "    query_info_df = val_df[[\"encounter_id\", \"image_ids\", \"query_title_en\", \"query_content_en\", \"author_id\"]]\n",
    "    \n",
    "    # Load CVQA data (ground truth answers)\n",
    "    with open(CVQA_PATH, 'r') as f:\n",
    "        cvqa_data = json.load(f)\n",
    "    cvqa_df = pd.json_normalize(cvqa_data)\n",
    "    \n",
    "    # Melt to get one row per questionS\n",
    "    cvqa_long = cvqa_df.melt(id_vars=[\"encounter_id\"], \n",
    "                             var_name=\"qid\", \n",
    "                             value_name=\"answer_index\")\n",
    "    \n",
    "    # Filter out encounter_id rows\n",
    "    cvqa_long = cvqa_long[cvqa_long[\"qid\"] != \"encounter_id\"]\n",
    "    \n",
    "    # Merge CVQA with questions\n",
    "    cvqa_merged = cvqa_long.merge(questions_df, on=\"qid\", how=\"left\")\n",
    "    \n",
    "    # Get answer text\n",
    "    def get_answer_text(row):\n",
    "        try:\n",
    "            return row[\"options_en\"][row[\"answer_index\"]]\n",
    "        except (IndexError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    cvqa_merged[\"answer_text\"] = cvqa_merged.apply(get_answer_text, axis=1)\n",
    "    \n",
    "    # Merge with validation data\n",
    "    final_df = cvqa_merged.merge(query_info_df, on=\"encounter_id\", how=\"left\")\n",
    "    \n",
    "    # Extract the base CQID code\n",
    "    final_df['base_qid'] = final_df['qid'].str.extract(r'(CQID\\d+)')\n",
    "    \n",
    "    # Group by encounter_id and base_qid to see all answers for each question family\n",
    "    grouped_by_family = final_df.groupby(['encounter_id', 'base_qid']).agg({\n",
    "        'qid': list,\n",
    "        'question_en': list,\n",
    "        'answer_text': list,\n",
    "        'answer_index': list,\n",
    "        'image_ids': 'first',\n",
    "        'options_en': 'first',\n",
    "        'question_type_en': 'first',\n",
    "        'question_category_en': 'first',\n",
    "        'query_title_en': 'first',\n",
    "        'query_content_en': 'first',\n",
    "        'author_id': 'first'\n",
    "    })\n",
    "    \n",
    "    # Reset index for easier manipulation\n",
    "    grouped_by_family = grouped_by_family.reset_index()\n",
    "       \n",
    "    def get_valid_answers(row):\n",
    "        \"\"\"\n",
    "        Extract all valid answers, with special handling for \"Not mentioned\".\n",
    "        If \"Not mentioned\" is the only answer for all slots, we keep it.\n",
    "        Otherwise, we collect all non-\"Not mentioned\" answers.\n",
    "        \"\"\"\n",
    "        answers = row['answer_text']\n",
    "        answer_indices = row['answer_index']\n",
    "\n",
    "        if all(ans == \"Not mentioned\" for ans in answers):\n",
    "            return [\"Not mentioned\"], [answer_indices[0]]  # If all are \"Not mentioned\", return it as valid\n",
    "\n",
    "        valid_answers = []\n",
    "        valid_indices = []\n",
    "\n",
    "        for i, ans in enumerate(answers):\n",
    "            if ans != \"Not mentioned\":\n",
    "                # Clean the answer string by removing quotes and extra whitespace\n",
    "                if isinstance(ans, str):\n",
    "                    cleaned_ans = ans.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                    # Only add if not already in valid_answers (after cleaning)\n",
    "                    if cleaned_ans not in valid_answers:\n",
    "                        valid_answers.append(cleaned_ans)\n",
    "                        valid_indices.append(answer_indices[i])\n",
    "                else:\n",
    "                    # Handle non-string answers\n",
    "                    str_ans = str(ans).strip(\"'\\\" \")\n",
    "                    if str_ans not in valid_answers:\n",
    "                        valid_answers.append(str_ans)\n",
    "                        valid_indices.append(answer_indices[i])\n",
    "\n",
    "        return valid_answers, valid_indices\n",
    "    \n",
    "    # Apply to all question families\n",
    "    grouped_by_family[['valid_answers', 'valid_indices']] = grouped_by_family.apply(\n",
    "        lambda row: pd.Series(get_valid_answers(row)), axis=1)\n",
    "    \n",
    "    # Create the multi-label validation dataset\n",
    "    multi_label_data = []\n",
    "    \n",
    "    # Process all validation encounters\n",
    "    for _, row in tqdm(grouped_by_family.iterrows(), desc=\"Creating validation dataset\"):\n",
    "        encounter_id = row['encounter_id']\n",
    "        base_qid = row['base_qid']\n",
    "        valid_answers = row['valid_answers']\n",
    "        valid_indices = row['valid_indices']\n",
    "        image_ids = row['image_ids']\n",
    "        question_text = row['question_en'][0]  # Taking the first question as reference\n",
    "        query_title = row['query_title_en']\n",
    "        query_content = row['query_content_en']\n",
    "        author_id = row['author_id']\n",
    "        options_en = row['options_en']\n",
    "        question_type_en = row['question_type_en']\n",
    "        question_category_en = row['question_category_en']\n",
    "        \n",
    "        # For each image in the encounter\n",
    "        for img_id in image_ids:\n",
    "            img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n",
    "            \n",
    "            # Skip if image doesn't exist\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image {img_id} not found at {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            multi_label_data.append({\n",
    "                'encounter_id': encounter_id,\n",
    "                'base_qid': base_qid,\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path,\n",
    "                'valid_answers': valid_answers,\n",
    "                'valid_indices': valid_indices,\n",
    "                'question_text': question_text,\n",
    "                'query_title_en': query_title,\n",
    "                'query_content_en': query_content,\n",
    "                'author_id': author_id,\n",
    "                'options_en': options_en,\n",
    "                'question_type_en': question_type_en, \n",
    "                'question_category_en': question_category_en,\n",
    "                'is_multi_label': len(valid_answers) > 1\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    val_dataset = pd.DataFrame(multi_label_data)\n",
    "    \n",
    "    # Save the dataset\n",
    "    val_dataset.to_csv(os.path.join(OUTPUT_DIR, \"val_dataset.csv\"), index=False)\n",
    "    \n",
    "    print(f\"Validation dataset created with {len(val_dataset)} entries\")\n",
    "    \n",
    "    return val_dataset\n",
    "\n",
    "# Function to process a batch for inference\n",
    "def process_inference_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples for inference and save them as a pickle file.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get image path\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            # Use the full image path if it's already in the dataframe\n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                # Otherwise construct from images_dir and image_id\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get options from options_en\n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "                \n",
    "                # Clean up options by removing \"(please specify)\" phrases\n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        # Remove \"(please specify)\" from option text\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                options = cleaned_options\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                print(\"Error with parsing options\")\n",
    "                \n",
    "            options_text = \", \".join(options)\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            # Get question text and clean it\n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Remove \"Please specify which affected area for each selection.\" from CQID012\n",
    "            if \"Please specify which affected area for each selection\" in question:\n",
    "                question = question.replace(\" Please specify which affected area for each selection.\", \"\")\n",
    "            \n",
    "            # Remove leading numbers like \"1 \" from the beginning of questions\n",
    "            question = re.sub(r'^\\d+\\s+', '', question)\n",
    "            \n",
    "            # Get clinical context from query title and content\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "                       \n",
    "            # Create the clinical context section\n",
    "            clinical_context = \"\"\n",
    "            if query_title or query_content:\n",
    "                clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "                if query_title:\n",
    "                    clinical_context += f\"{query_title}\\n\"\n",
    "                if query_content:\n",
    "                    clinical_context += f\"{query_content}\\n\"\n",
    "\n",
    "            # Create the full query text with clinical context\n",
    "            query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Available Options (choose from these): {options_text}\")\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"val_batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_validation_dataset(df, batch_size=50, save_dir=None, images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the entire validation dataset in batches\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Use global PROCESSED_VAL_DATA_DIR if save_dir is not provided\n",
    "    if save_dir is None:\n",
    "        save_dir = PROCESSED_VAL_DATA_DIR\n",
    "    \n",
    "    # Use VAL_IMAGES_DIR global variable if images_dir is not provided\n",
    "    if images_dir is None:\n",
    "        images_dir = VAL_IMAGES_DIR\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_inference_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageInference:\n",
    "    def __init__(self, model_path, token=None, adapter_path=None, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the inference class for medical image analysis.\n",
    "        \n",
    "        Parameters:\n",
    "        - model_path: Path to the base model\n",
    "        - token: HF token for downloading models\n",
    "        - adapter_path: Path to adapter weights (used for any model type)\n",
    "        - device: Computing device (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        print(f\"Loading processor from {model_path}...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path, token=token)\n",
    "               \n",
    "        # Standard kwargs for all models\n",
    "        base_kwargs = dict(\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            low_cpu_mem_usage=True,\n",
    "            token=token\n",
    "        )\n",
    "        \n",
    "        # Handle different model types\n",
    "        if IS_LLAMA:\n",
    "            # Load Llama-specific model\n",
    "            print(\"Loading Llama model...\")\n",
    "            self.model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                model_path,\n",
    "                **base_kwargs\n",
    "            )\n",
    "        elif IS_QWEN:\n",
    "            # Load Qwen-specific model with flash attention\n",
    "            print(\"Loading Qwen model...\")\n",
    "            if \"Qwen2.5-VL\" in model_path:\n",
    "                # Add flash attention for A100 GPUs\n",
    "                base_kwargs[\"attn_implementation\"] = \"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    "                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                    model_path,\n",
    "                    **base_kwargs\n",
    "                )\n",
    "            elif \"Qwen2-VL\" in model_path:\n",
    "                # Add flash attention for A100 GPUs\n",
    "                base_kwargs[\"attn_implementation\"] = \"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    "                self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                    model_path,\n",
    "                    **base_kwargs\n",
    "                )\n",
    "        else:\n",
    "            # For other models (Gemma, etc.) use AutoModelForImageTextToText\n",
    "            print(\"Loading non-Llama, non-Qwen model...\")\n",
    "            non_llama_kwargs = base_kwargs.copy()\n",
    "            non_llama_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                model_path,\n",
    "                **non_llama_kwargs\n",
    "            )\n",
    "        \n",
    "        # Load and apply the adapter if provided\n",
    "        if adapter_path:\n",
    "            print(f\"Loading adapter from {adapter_path}...\")\n",
    "            from peft import PeftModel\n",
    "            self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.IS_QWEN = IS_QWEN  # Save for use in prediction\n",
    "        print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Rest of the methods remain unchanged\n",
    "    def predict(self, query_text, image_path, max_new_tokens=100):\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Create the system message\n",
    "            system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "            IMPORTANT: \n",
    "            - Respond ONLY with the exact text of the option(s) that apply\n",
    "            - Do not provide any explanations\n",
    "            - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "            - Do not write \"Options:\" or similar prefixes\n",
    "            - Do not write \"Answer:\" or similar prefixes\n",
    "            - Multiple answers should be separated by commas\n",
    "            - If unsure, respond with \"Not mentioned\"\n",
    "            - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "            - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "            \"\"\"\n",
    "            \n",
    "            # Format as a conversation with system and user messages\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query_text},\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Create model inputs\n",
    "            inputs = self.processor(\n",
    "                text=self.processor.apply_chat_template(messages, tokenize=False),\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate prediction               \n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.0,\n",
    "                    top_p=0.95,\n",
    "                    top_k=64\n",
    "                )\n",
    "\n",
    "            # Get only the new tokens (the model's answer)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = generated_ids[0][input_length:]\n",
    "\n",
    "            # After decoding the prediction, add this code to clean it:\n",
    "            prediction = self.processor.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # Clean the prediction - remove any remaining template artifacts\n",
    "            prediction = prediction.strip()\n",
    "\n",
    "            # Add this new code to remove \"assistant\\n\\n\" prefix\n",
    "            \n",
    "            # Add this new code to remove \"assistant\\n\\n\" prefix\n",
    "            if prediction.startswith(\"assistant\\n\\n\"):\n",
    "                prediction = prediction[len(\"assistant\\n\\n\"):]\n",
    "                \n",
    "            if prediction.startswith(\"assistant\\n\"):\n",
    "                prediction = prediction[len(\"assistant\\n\"):]\n",
    "                \n",
    "            if prediction.startswith(\"system\\n\"):\n",
    "                prediction = prediction[len(\"system\\n\"):]\n",
    "\n",
    "            # Continue with existing cleaning code\n",
    "            if prediction.startswith(\"model\\n\"):\n",
    "                prediction = prediction[len(\"model\\n\"):]\n",
    "\n",
    "            # Remove asterisk formatting (for Gemma model)\n",
    "            prediction = re.sub(r'^\\*+\\s*', '', prediction)  # Remove leading asterisks and spaces\n",
    "            prediction = re.sub(r'\\n\\*+\\s*', ' ', prediction)  # Replace newline + asterisks with space\n",
    "            prediction = re.sub(r'\\*\\s*', '', prediction)  # Remove standalone asterisks with spaces\n",
    "\n",
    "            # Remove disclaimers and notes\n",
    "            if prediction.startswith(\"Note:\") or prediction.startswith(\"Disclaimer:\") or prediction.startswith(\"*Note:\"):\n",
    "                if \"\\n\" in prediction:\n",
    "                    prediction = prediction.split(\"\\n\", 1)[1].strip()\n",
    "                \n",
    "            # Extract just the answer text\n",
    "            if \"Answer:\" in prediction:\n",
    "                parts = prediction.split(\"Answer:\")\n",
    "                if len(parts) > 1:\n",
    "                    prediction = parts[1].strip()\n",
    "                    \n",
    "            # NEW: Remove trailing period at the end of answers\n",
    "            if prediction.endswith(\".\"):\n",
    "                prediction = prediction[:-1]\n",
    "                \n",
    "            if prediction.startswith(\"<start_of_turn>model\") or prediction.startswith(\"<start_of_turn>assistant\"):\n",
    "                prediction = prediction.split(\"\\n\", 1)[1] if \"\\n\" in prediction else \"\"\n",
    "            if prediction.endswith(\"<end_of_turn>\"):\n",
    "                prediction = prediction[:-len(\"<end_of_turn>\")]\n",
    "\n",
    "            return prediction.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {image_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return \"Not mentioned\"  # Default to not mentioned in case of errors\n",
    "    \n",
    "    def batch_predict(self, processed_data_dir=None, output_file=None, max_samples=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of preprocessed data\n",
    "        \"\"\"\n",
    "        # Use global constants if not provided\n",
    "        if processed_data_dir is None:\n",
    "            processed_data_dir = PROCESSED_VAL_DATA_DIR\n",
    "            \n",
    "        if output_file is None:\n",
    "            output_file = os.path.join(OUTPUT_DIR, \"predictions.csv\")\n",
    "            \n",
    "        results = []\n",
    "        sample_count = 0\n",
    "        \n",
    "        # Load all batch files\n",
    "        batch_files = sorted([f for f in os.listdir(processed_data_dir) if f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "        \n",
    "        # Process each batch file\n",
    "        for batch_file in tqdm(batch_files, desc=\"Processing batches\"):\n",
    "            with open(os.path.join(processed_data_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for sample in tqdm(batch_data, desc=f\"Predicting {batch_file}\", leave=False):\n",
    "                # Get prediction\n",
    "                prediction = self.predict(sample[\"query_text\"], sample[\"image_path\"])\n",
    "                \n",
    "                # Save results\n",
    "                results.append({\n",
    "                    \"encounter_id\": sample[\"id\"],\n",
    "                    \"base_qid\": sample[\"qid\"],\n",
    "                    \"image_id\": os.path.basename(sample[\"image_path\"]),\n",
    "                    \"prediction\": prediction\n",
    "                })\n",
    "                \n",
    "                sample_count += 1\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "                \n",
    "            if max_samples and sample_count >= max_samples:\n",
    "                break\n",
    "        \n",
    "        # Convert to DataFrame and save\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    # Update the aggregate_predictions method in the MedicalImageInference class\n",
    "    def aggregate_predictions(self, predictions_df, validation_df=None):\n",
    "        \"\"\"\n",
    "        Aggregate predictions for each encounter and question ID\n",
    "        For each encounter-question pair, collect unique predictions across all images,\n",
    "        respecting the maximum allowed answers for each question type.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions_df: DataFrame with prediction results\n",
    "        - validation_df: Optional DataFrame containing validation data with options_en\n",
    "        \"\"\"\n",
    "        # Define maximum allowed answers for each question type\n",
    "        max_answers = {\n",
    "            'CQID010': 1,  # Single answer\n",
    "            'CQID011': 6,  # Up to 6 answers\n",
    "            'CQID012': 6,  # Up to 6 answers\n",
    "            'CQID015': 1,  # Single answer\n",
    "            'CQID020': 9,  # Up to 9 answers\n",
    "            'CQID025': 1,  # Single answer\n",
    "            'CQID034': 1,  # Single answer\n",
    "            'CQID035': 1,  # Single answer\n",
    "            'CQID036': 1   # Single answer\n",
    "        }\n",
    "\n",
    "        # Set default max_answers for any question type not explicitly listed\n",
    "        default_max_answers = 1\n",
    "\n",
    "        # Group by encounter_id and base_qid\n",
    "        grouped = predictions_df.groupby(['encounter_id', 'base_qid'])\n",
    "\n",
    "        aggregated_results = []\n",
    "\n",
    "        for (encounter_id, base_qid), group in tqdm(grouped, desc=\"Aggregating predictions\"):\n",
    "            # Extract all predictions for this group\n",
    "            predictions = group['prediction'].tolist()\n",
    "            image_ids = group['image_id'].tolist()\n",
    "\n",
    "            # Process predictions to standardize format\n",
    "            cleaned_predictions = []\n",
    "            for pred in predictions:\n",
    "                # Handle predictions that might be in a list format\n",
    "                if isinstance(pred, str):\n",
    "                    # Remove \"(please specify)\" from prediction\n",
    "                    pred = pred.replace(\" (please specify)\", \"\")\n",
    "\n",
    "                    if pred.startswith('[') and pred.endswith(']'):\n",
    "                        try:\n",
    "                            # Try to evaluate as a Python list\n",
    "                            pred_list = safe_convert_options(pred)\n",
    "                            if isinstance(pred_list, list):\n",
    "                                # Clean each item in the list\n",
    "                                pred_list = [p.replace(\" (please specify)\", \"\") if isinstance(p, str) else p for p in pred_list]\n",
    "                                cleaned_predictions.extend(pred_list)\n",
    "                                continue\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    # Handle comma-separated values\n",
    "                    if ',' in pred:\n",
    "                        # Clean each comma-separated item\n",
    "                        items = [p.strip().replace(\" (please specify)\", \"\") for p in pred.split(',')]\n",
    "                        cleaned_predictions.extend(items)\n",
    "                    else:\n",
    "                        cleaned_predictions.append(pred.strip())\n",
    "                else:\n",
    "                    cleaned_predictions.append(str(pred).strip())\n",
    "\n",
    "            all_cleaned_predictions = cleaned_predictions.copy()\n",
    "\n",
    "            # Count frequencies of each prediction\n",
    "            cleaned_predictions = [p.lower() if isinstance(p, str) else str(p).lower() for p in cleaned_predictions]\n",
    "            prediction_counts = Counter(cleaned_predictions)\n",
    "\n",
    "            # Get question type for determining max allowed answers\n",
    "            question_type = base_qid.split('-')[0] if '-' in base_qid else base_qid\n",
    "\n",
    "            # Determine max allowed answers for this question type\n",
    "            allowed_max = max_answers.get(question_type, default_max_answers)\n",
    "\n",
    "            # Sort predictions by frequency (most common first)\n",
    "            sorted_predictions = sorted(prediction_counts.items(), \n",
    "                                       key=lambda x: x[1], \n",
    "                                       reverse=True)\n",
    "\n",
    "            all_sorted_predictions = sorted_predictions.copy()\n",
    "            \n",
    "            # Get top N predictions where N is the max allowed\n",
    "            top_predictions = [p[0] for p in sorted_predictions[:allowed_max]]\n",
    "\n",
    "            # If there are ties at the cutoff point, randomly select to meet the max limit\n",
    "            if len(sorted_predictions) > allowed_max:\n",
    "                # Check if there's a tie at the cutoff\n",
    "                cutoff_count = sorted_predictions[allowed_max-1][1]\n",
    "                tied_predictions = [p[0] for p in sorted_predictions if p[1] == cutoff_count]\n",
    "\n",
    "                # If we have more tied predictions than slots available\n",
    "                if len(tied_predictions) > 1 and len(top_predictions) > allowed_max - len(tied_predictions):\n",
    "                    # Remove all tied predictions from top_predictions\n",
    "                    top_predictions = [p for p in top_predictions if p not in tied_predictions]\n",
    "\n",
    "                    # Randomly select from tied predictions to fill remaining slots\n",
    "                    random.seed(42)  # For reproducibility\n",
    "                    slots_remaining = allowed_max - len(top_predictions)\n",
    "                    selected_tied = random.sample(tied_predictions, slots_remaining)\n",
    "\n",
    "                    # Add the randomly selected tied predictions\n",
    "                    top_predictions.extend(selected_tied)\n",
    "\n",
    "            # If \"Not mentioned\" is in predictions but there are other predictions,\n",
    "            # remove \"Not mentioned\" (unless it's the only prediction)\n",
    "            if len(top_predictions) > 1 and \"not mentioned\" in top_predictions:\n",
    "                top_predictions.remove(\"not mentioned\")\n",
    "\n",
    "            # Create a single, combined prediction\n",
    "            combined_prediction = \", \".join(top_predictions)\n",
    "\n",
    "            # Initialize options_en as None\n",
    "            options_en = None\n",
    "\n",
    "            # If validation_df is provided, try to get options_en from it\n",
    "            if validation_df is not None:\n",
    "                # Find matching rows in validation_df\n",
    "                matching_rows = validation_df[(validation_df['encounter_id'] == encounter_id) & \n",
    "                                             (validation_df['base_qid'] == base_qid)]\n",
    "                if not matching_rows.empty:\n",
    "                    # Get options_en from the first matching row\n",
    "                    options_en = matching_rows.iloc[0].get('options_en')\n",
    "\n",
    "            result_dict = {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"base_qid\": base_qid,\n",
    "                \"image_ids\": image_ids,\n",
    "                \"unique_predictions\": top_predictions,  # Now limited to max allowed\n",
    "                \"combined_prediction\": combined_prediction,\n",
    "                \"all_raw_predictions\": all_cleaned_predictions,\n",
    "                \"all_sorted_predictions\": all_sorted_predictions\n",
    "            }\n",
    "\n",
    "            # Add options_en only if it's available\n",
    "            if options_en is not None:\n",
    "                result_dict[\"options_en\"] = options_en\n",
    "\n",
    "            aggregated_results.append(result_dict)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42442c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.skip_data_prep:\n",
    "    # Prepare data for validation\n",
    "    print(\"Preparing validation dataset...\")\n",
    "    val_df = prepare_validation_data()\n",
    "    \n",
    "    # Subset for testing if requested\n",
    "    if args.test:\n",
    "        print(\"Running in test mode with a small subset of data...\")\n",
    "        test_size = min(100, len(val_df))\n",
    "        val_df = val_df.head(test_size)\n",
    "    \n",
    "    # Process validation data\n",
    "    # Clear any existing processed data\n",
    "    if os.path.exists(PROCESSED_VAL_DATA_DIR):\n",
    "        shutil.rmtree(PROCESSED_VAL_DATA_DIR)\n",
    "        os.makedirs(PROCESSED_VAL_DATA_DIR)\n",
    "    \n",
    "    total_examples = preprocess_validation_dataset(\n",
    "        val_df, \n",
    "        batch_size=args.batch_size,\n",
    "        save_dir=PROCESSED_VAL_DATA_DIR\n",
    "    )\n",
    "    print(f\"Total processed validation examples: {total_examples}\")\n",
    "else:\n",
    "    print(\"Skipping data preparation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7711cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_inference_inputs(processed_val_dir=None, num_samples=3):\n",
    "    \"\"\"\n",
    "    Inspect the actual inputs being used during inference by examining the\n",
    "    processed validation data files.\n",
    "    \n",
    "    Args:\n",
    "        processed_val_dir: Directory containing processed validation data\n",
    "        num_samples: Number of samples to display\n",
    "    \"\"\"\n",
    "    # Use global variable if no directory is provided\n",
    "    if processed_val_dir is None:\n",
    "        processed_val_dir = PROCESSED_VAL_DATA_DIR\n",
    "        \n",
    "    # Find all batch files\n",
    "    batch_files = sorted([f for f in os.listdir(processed_val_dir) \n",
    "                         if f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No batch files found in {processed_val_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(batch_files)} batch files in {processed_val_dir}\")\n",
    "    \n",
    "    # Load the first batch file\n",
    "    with open(os.path.join(processed_val_dir, batch_files[0]), 'rb') as f:\n",
    "        batch_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"Batch contains {len(batch_data)} samples\")\n",
    "    \n",
    "    # Display the requested number of samples\n",
    "    for i, sample in enumerate(batch_data[:num_samples]):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SAMPLE {i+1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Display metadata\n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(f\"Question ID: {sample['qid']}\")\n",
    "        \n",
    "        # Display the data that would be fed to the model\n",
    "        print(\"\\nSYSTEM MESSAGE (Used during inference):\")\n",
    "        print(\"-\" * 80)\n",
    "        # Create the system message\n",
    "        system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "        \n",
    "        IMPORTANT: \n",
    "        - Respond ONLY with the exact text of the option(s) that apply\n",
    "        - Do not provide any explanations\n",
    "        - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\"\n",
    "        - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "        - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "        \"\"\"\n",
    "        \n",
    "        print(system_message)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display the query text\n",
    "        print(\"\\nUSER MESSAGE TEXT (Used during inference):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(sample['query_text'])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display image information\n",
    "        print(\"\\nIMAGE (Used during inference):\")\n",
    "        print(f\"Image path: {sample['image_path']}\")\n",
    "        \n",
    "        # Try to display image dimensions if PIL is available\n",
    "        try:\n",
    "            img = Image.open(sample['image_path'])\n",
    "            print(f\"Image dimensions: {img.size[0]}x{img.size[1]}, Format: {img.format}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open image: {e}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Option to stop after showing samples\n",
    "        if i >= num_samples - 1:\n",
    "            break\n",
    "\n",
    "# Run the inspection function\n",
    "inspect_inference_inputs(num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for running inference\n",
    "def run_inference():\n",
    "    # Use the global args that was already created\n",
    "    # No need to create a new Args instance\n",
    "    \n",
    "    # Add a timestamp for unique filenames\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_type = \"finetuned\" if args.use_finetuning else \"base\"\n",
    "    inference_id = f\"{MODEL_NAME}_{model_type}_{timestamp}\"\n",
    "    \n",
    "    # Create the inference object - simpler now with consistent approach\n",
    "    inference = MedicalImageInference(\n",
    "        model_path=args.model_path,\n",
    "        token=HF_TOKEN,\n",
    "        adapter_path=args.adapter_path\n",
    "    )\n",
    "    \n",
    "    # Run inference\n",
    "    predictions_file = os.path.join(OUTPUT_DIR, \n",
    "                              f\"val_predictions_{inference_id}_{timestamp}{'_test' if args.test else ''}.csv\")\n",
    "    \n",
    "    print(f\"Running inference (max_samples={args.max_samples if args.max_samples else 'all'})...\")\n",
    "    predictions_df = inference.batch_predict(PROCESSED_VAL_DATA_DIR, predictions_file, max_samples=args.max_samples)\n",
    "    return predictions_df, inference, inference_id, timestamp, predictions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd849093",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df, inference, INFERENCE_MODEL_ID, INFERENCE_TIMESTAMP, predictions_file = run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate predictions\n",
    "print(\"Aggregating predictions...\")\n",
    "\n",
    "# Load the validation dataset\n",
    "val_dataset_path = os.path.join(OUTPUT_DIR, \"val_dataset.csv\")\n",
    "val_dataset = pd.read_csv(val_dataset_path)\n",
    "\n",
    "# Aggregate predictions with options_en\n",
    "aggregated_df = inference.aggregate_predictions(predictions_df, validation_df=val_dataset)\n",
    "\n",
    "# Save aggregated results\n",
    "aggregated_file = os.path.join(OUTPUT_DIR, \n",
    "                             f\"aggregated_predictions_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{'_test' if args.test else ''}.csv\")\n",
    "aggregated_df.to_csv(aggregated_file, index=False)\n",
    "\n",
    "print(f\"Inference complete. Results saved to {predictions_file} and {aggregated_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271687d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample of predictions for inspection\n",
    "print(\"\\nSample of raw predictions:\")\n",
    "predictions_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39598c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test suffix\n",
    "TEST_SUFFIX = '_test' if args.test else ''\n",
    "\n",
    "# Find the latest prediction file\n",
    "prediction_pattern = os.path.join(OUTPUT_DIR, f\"val_predictions_*{TEST_SUFFIX}.csv\")\n",
    "prediction_files = sorted(glob.glob(prediction_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not prediction_files:\n",
    "    raise FileNotFoundError(f\"No prediction files found matching pattern {prediction_pattern}\")\n",
    "LATEST_PREDICTION_FILE = prediction_files[0]\n",
    "print(f\"Loading latest prediction file: {LATEST_PREDICTION_FILE}\")\n",
    "predictions = pd.read_csv(LATEST_PREDICTION_FILE)\n",
    "\n",
    "# Find the latest aggregated file\n",
    "aggregated_pattern = os.path.join(OUTPUT_DIR, f\"aggregated_predictions_*{TEST_SUFFIX}.csv\")\n",
    "aggregated_files = sorted(glob.glob(aggregated_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not aggregated_files:\n",
    "    raise FileNotFoundError(f\"No aggregated files found matching pattern {aggregated_pattern}\")\n",
    "LATEST_AGGREGATED_FILE = aggregated_files[0]\n",
    "print(f\"Loading latest aggregated file: {LATEST_AGGREGATED_FILE}\")\n",
    "aggregated = pd.read_csv(LATEST_AGGREGATED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07978e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter predictions that have commas (indicating multiple answers)\n",
    "multi_answer_preds = predictions[predictions['prediction'].str.contains(',', na=False)]\n",
    "\n",
    "# Display sample of multi-answer predictions\n",
    "print(\"Sample of predictions with multiple answers:\")\n",
    "multi_answer_preds[[\"encounter_id\", \"base_qid\", \"image_id\", \"prediction\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many multi-answer predictions we have\n",
    "print(f\"\\nTotal multi-answer predictions: {len(multi_answer_preds)}\")\n",
    "\n",
    "# See which questions tend to have multiple answers\n",
    "print(\"\\nMulti-answer predictions by question type:\")\n",
    "multi_answer_preds['base_qid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932db705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up the aggregated result for this question\n",
    "agg_result = aggregated[\n",
    "    (aggregated['encounter_id'] == 'ENC00852') & \n",
    "    (aggregated['base_qid'] == 'CQID034')\n",
    "]\n",
    "agg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of different answers\n",
    "answer_counts = predictions[\"prediction\"].value_counts().head(10)\n",
    "print(\"\\nMost common predictions:\")\n",
    "print(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_for_official_eval_with_display(aggregated_df, output_file):\n",
    "    \"\"\"\n",
    "    Format predictions as expected by the official evaluation script,\n",
    "    mapping text answers to indices and distributing multiple answers\n",
    "    across question variants when appropriate.\n",
    "    Also displays the text values alongside their indices for verification.\n",
    "    \"\"\"\n",
    "    # Define the question IDs and their allowed variants\n",
    "    QIDS = [\n",
    "        \"CQID010-001\",  # how much of body is affected (single answer)\n",
    "        \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",  # multiple answers allowed\n",
    "        \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",  # multiple answers allowed\n",
    "        \"CQID015-001\",  # single answer\n",
    "        \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "        \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",  # multiple answers allowed\n",
    "        \"CQID025-001\",  # single answer\n",
    "        \"CQID034-001\",  # single answer\n",
    "        \"CQID035-001\",  # single answer\n",
    "        \"CQID036-001\",  # single answer\n",
    "    ]\n",
    "    \n",
    "    # Create a mapping of question base IDs to their allowed variants\n",
    "    qid_variants = {}\n",
    "    for qid in QIDS:\n",
    "        base_qid, variant = qid.split('-')\n",
    "        if base_qid not in qid_variants:\n",
    "            qid_variants[base_qid] = []\n",
    "        qid_variants[base_qid].append(qid)\n",
    "    \n",
    "    # Get all required base QIDs for a complete encounter\n",
    "    required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "    \n",
    "    formatted_predictions = []\n",
    "    display_info = []\n",
    "    \n",
    "    # Group by encounter_id\n",
    "    for encounter_id, group in aggregated_df.groupby('encounter_id'):\n",
    "        # Get all base_qids for this encounter\n",
    "        encounter_base_qids = set(group['base_qid'].unique())\n",
    "        \n",
    "        # Skip encounters that don't have all required questions\n",
    "        if not required_base_qids.issubset(encounter_base_qids):\n",
    "            print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "            continue\n",
    "        \n",
    "        # Create a prediction entry for this encounter\n",
    "        pred_entry = {'encounter_id': encounter_id}\n",
    "        encounter_display = {'encounter_id': encounter_id, 'questions': []}\n",
    "        \n",
    "        # Process each question for this encounter\n",
    "        for _, row in group.iterrows():\n",
    "            base_qid = row['base_qid']\n",
    "            \n",
    "            # Skip if we don't have variants defined for this question\n",
    "            if base_qid not in qid_variants:\n",
    "                continue\n",
    "            \n",
    "            # Get the options list for this question\n",
    "            options = safe_convert_options(row['options_en'])\n",
    "            \n",
    "            # Find the index of \"Not mentioned\" in the options\n",
    "            not_mentioned_index = None\n",
    "            for i, opt in enumerate(options):\n",
    "                if opt == \"Not mentioned\":\n",
    "                    not_mentioned_index = i\n",
    "                    break\n",
    "            \n",
    "            # If \"Not mentioned\" is not in the options, default to the last option\n",
    "            if not_mentioned_index is None:\n",
    "                not_mentioned_index = len(options) - 1\n",
    "            \n",
    "            # Get predictions\n",
    "            if isinstance(row['unique_predictions'], list):\n",
    "                predictions = row['unique_predictions']\n",
    "            else:\n",
    "                try:\n",
    "                    predictions = eval(row['unique_predictions'])\n",
    "                except:\n",
    "                    predictions = [row['unique_predictions']]\n",
    "            \n",
    "            # Map text predictions to indices\n",
    "            prediction_indices = []\n",
    "            prediction_texts = []\n",
    "            \n",
    "            for pred in predictions:\n",
    "                pred_text = str(pred).strip()\n",
    "                prediction_texts.append(pred_text)\n",
    "                        \n",
    "                # Find index of the prediction in options\n",
    "                found = False\n",
    "                for i, option in enumerate(options):\n",
    "                    # Clean option text by removing \"(please specify)\"\n",
    "                    clean_option = option.replace(\" (please specify)\", \"\").lower()\n",
    "\n",
    "                    # Compare with cleaned option text\n",
    "                    if pred_text.lower() == clean_option:\n",
    "                        prediction_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                # If prediction not found in options, use index 100\n",
    "                if not found:\n",
    "                    prediction_indices.append(100)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            unique_indices = []\n",
    "            unique_texts = []\n",
    "            for idx, text in zip(prediction_indices, prediction_texts):\n",
    "                if idx not in unique_indices:\n",
    "                    unique_indices.append(idx)\n",
    "                    unique_texts.append(text)\n",
    "            \n",
    "            # If 100 is in the list along with valid indices, remove 100\n",
    "            if len(unique_indices) > 1 and 100 in unique_indices:\n",
    "                idx_to_remove = unique_indices.index(100)\n",
    "                unique_indices.remove(100)\n",
    "                unique_texts.pop(idx_to_remove)\n",
    "            \n",
    "            # Get the available variants for this question\n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            # Store info for display\n",
    "            question_display = {\n",
    "                'base_qid': base_qid,\n",
    "                'predicted_texts': unique_texts,\n",
    "                'predicted_indices': unique_indices,\n",
    "                'options': options,\n",
    "                'not_mentioned_index': not_mentioned_index,\n",
    "                'variant_assignments': {}\n",
    "            }\n",
    "            \n",
    "            # For single-answer questions (with only one variant)\n",
    "            if len(available_variants) == 1:\n",
    "                if unique_indices:\n",
    "                    # Store as a single integer, not a list\n",
    "                    pred_entry[available_variants[0]] = unique_indices[0]\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': unique_indices[0],\n",
    "                        'text': unique_texts[0] if unique_texts else \"None\"\n",
    "                    }\n",
    "                else:\n",
    "                    # Default to \"Not mentioned\" if no prediction\n",
    "                    pred_entry[available_variants[0]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            # For multi-answer questions\n",
    "            else:\n",
    "                # Distribute answers across available variants\n",
    "                for i, idx in enumerate(unique_indices):\n",
    "                    if i < len(available_variants):\n",
    "                        # Store each answer as a single integer, not a list\n",
    "                        pred_entry[available_variants[i]] = idx\n",
    "                        question_display['variant_assignments'][available_variants[i]] = {\n",
    "                            'index': idx,\n",
    "                            'text': unique_texts[i] if i < len(unique_texts) else \"None\"\n",
    "                        }\n",
    "                \n",
    "                # Fill remaining variants with a default value (usually \"Not mentioned\")\n",
    "                for i in range(len(unique_indices), len(available_variants)):\n",
    "                    # Use correct \"Not mentioned\" index for this question\n",
    "                    pred_entry[available_variants[i]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[i]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            encounter_display['questions'].append(question_display)\n",
    "        \n",
    "        formatted_predictions.append(pred_entry)\n",
    "        display_info.append(encounter_display)\n",
    "    \n",
    "    if not formatted_predictions:\n",
    "        print(\"Warning: No complete encounters found in the data!\")\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_predictions, f, indent=2)\n",
    "    \n",
    "    # Display information about the predictions\n",
    "    for encounter in display_info:\n",
    "        print(f\"\\nEncounter: {encounter['encounter_id']}\")\n",
    "        for question in encounter['questions']:\n",
    "            print(f\"  Question: {question['base_qid']}\")\n",
    "            print(f\"  Predicted texts: {question['predicted_texts']}\")\n",
    "            print(f\"  Predicted indices: {question['predicted_indices']}\")\n",
    "            print(f\"  'Not mentioned' index: {question['not_mentioned_index']}\")\n",
    "            print(\"  Variant assignments:\")\n",
    "            for variant, assignment in question['variant_assignments'].items():\n",
    "                print(f\"    {variant}: index={assignment['index']} ({assignment['text']})\")\n",
    "            print(f\"  Available options: {question['options']}\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and save predictions for official evaluation\n",
    "predictions_json = os.path.join(OUTPUT_DIR, \n",
    "                              f\"data_cvqa_sys_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.json\")\n",
    "format_predictions_for_official_eval_with_display(aggregated, predictions_json)\n",
    "print(f\"Formatted predictions saved to {predictions_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter validation DataFrame to get the specific question and encounter\n",
    "specific_question = val_dataset[(val_dataset['encounter_id'] == 'ENC00853') & \n",
    "                               (val_dataset['base_qid'] == 'CQID012')]\n",
    "\n",
    "# Display all relevant columns\n",
    "print(\"Question Information:\")\n",
    "print(f\"Question text: {specific_question['question_text'].values[0]}\")\n",
    "print(f\"Question type: {specific_question['question_type_en'].values[0]}\")\n",
    "print(f\"Question category: {specific_question['question_category_en'].values[0]}\")\n",
    "print(f\"Options: {specific_question['options_en'].values[0]}\")\n",
    "print(f\"Multi-label: {specific_question['is_multi_label'].values[0]}\")\n",
    "print(\"\\nClinical Context:\")\n",
    "print(f\"Query title: {specific_question['query_title_en'].values[0]}\")\n",
    "print(f\"Query content: {specific_question['query_content_en'].values[0]}\")\n",
    "print(\"\\nImage information:\")\n",
    "print(f\"Image ID: {specific_question['image_id'].values[0]}\")\n",
    "print(f\"Image path: {specific_question['image_path'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482db9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to double check the saved file\n",
    "# Find the most recent data_cvqa_sys file for this model\n",
    "json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.json\")\n",
    "json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No JSON prediction files found matching pattern {json_pattern}\")\n",
    "    \n",
    "predictions_file_path = json_files[0]\n",
    "print(f\"Using most recent prediction JSON: {predictions_file_path}\")\n",
    "\n",
    "with open(predictions_file_path, 'r') as f:\n",
    "    formatted_preds = json.load(f)\n",
    "    \n",
    "# Display the first 3 entries\n",
    "print(\"First 3 prediction entries:\")\n",
    "for i in range(min(3, len(formatted_preds))):\n",
    "    print(f\"\\nPrediction {i+1}:\")\n",
    "    pprint(formatted_preds[i])\n",
    "\n",
    "# Show an example of answers not in options (if any)\n",
    "print(\"\\nLooking for predictions with index 100 (not in options):\")\n",
    "found = False\n",
    "for entry in formatted_preds:\n",
    "    for key, value in entry.items():\n",
    "        if key != 'encounter_id':  # Skip the encounter_id\n",
    "            if (isinstance(value, list) and 100 in value) or value == 100:\n",
    "                print(f\"\\nFound prediction not in options:\")\n",
    "                print(f\"Encounter: {entry['encounter_id']}\")\n",
    "                print(f\"Question: {key}\")\n",
    "                print(f\"Prediction indices: {value}\")\n",
    "                \n",
    "                # Load original predictions for this encounter\n",
    "                # Use the correct aggregated file with the model ID and timestamp\n",
    "                aggregated_file_path = os.path.join(OUTPUT_DIR, \n",
    "                                                  f\"aggregated_predictions_{INFERENCE_MODEL_ID}_{INFERENCE_TIMESTAMP}{TEST_SUFFIX}.csv\")\n",
    "                \n",
    "                # Check if the file exists before trying to open it\n",
    "                if not os.path.exists(aggregated_file_path):\n",
    "                    print(f\"Warning: Could not find aggregated predictions file at {aggregated_file_path}\")\n",
    "                    # Try to find it using glob\n",
    "                    agg_pattern = os.path.join(OUTPUT_DIR, f\"aggregated_predictions_*{TEST_SUFFIX}.csv\")\n",
    "                    agg_files = sorted(glob.glob(agg_pattern), key=os.path.getmtime, reverse=True)\n",
    "                    if agg_files:\n",
    "                        aggregated_file_path = agg_files[0]\n",
    "                        print(f\"Using most recent aggregated file instead: {aggregated_file_path}\")\n",
    "                    else:\n",
    "                        print(\"No aggregated prediction files found. Skipping detailed analysis.\")\n",
    "                        continue\n",
    "                \n",
    "                agg_df = pd.read_csv(aggregated_file_path)\n",
    "                base_qid = key.split('-')[0]\n",
    "                encounter = entry['encounter_id']\n",
    "                match = agg_df[(agg_df['encounter_id'] == encounter) & (agg_df['base_qid'] == base_qid)]\n",
    "                if not match.empty:\n",
    "                    print(f\"Original prediction text: {match['combined_prediction'].values[0]}\")\n",
    "                    print(f\"Available options: {match['options_en'].values[0]}\")\n",
    "                found = True\n",
    "                break\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print(\"No predictions with index 100 found in the first few entries.\")\n",
    "\n",
    "# Show statistics\n",
    "question_counts = {}\n",
    "for entry in formatted_preds:\n",
    "    qid_count = len(entry) - 1  # Subtract 1 for encounter_id\n",
    "    if qid_count in question_counts:\n",
    "        question_counts[qid_count] += 1\n",
    "    else:\n",
    "        question_counts[qid_count] = 1\n",
    "\n",
    "print(\"\\nNumber of questions per encounter:\")\n",
    "for count, num_entries in sorted(question_counts.items()):\n",
    "    print(f\"{count} questions: {num_entries} encounters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35843cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use INFERENCE_MODEL_ID for consistency\n",
    "SUBMISSION_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Find the most recent data_cvqa_sys file for this model\n",
    "json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{INFERENCE_MODEL_ID}*{TEST_SUFFIX}.json\")\n",
    "json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No JSON prediction files found matching pattern {json_pattern}\")\n",
    "most_recent_json = json_files[0]\n",
    "print(f\"Using most recent prediction JSON: {most_recent_json}\")\n",
    "\n",
    "# Create directory for empty masks_preds if it doesn't exist\n",
    "masks_preds_dir = os.path.join(OUTPUT_DIR, \"masks_preds\")\n",
    "os.makedirs(masks_preds_dir, exist_ok=True)\n",
    "\n",
    "# Create submission folder with timestamp\n",
    "submission_dir = os.path.join(OUTPUT_DIR, f\"submission_{INFERENCE_MODEL_ID}_{SUBMISSION_TIMESTAMP}\")\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# Copy the most recent JSON with the exact required filename\n",
    "dest_json = os.path.join(submission_dir, \"data_cvqa_sys.json\")\n",
    "shutil.copy2(most_recent_json, dest_json)\n",
    "\n",
    "# Create empty masks_preds directory in the submission folder\n",
    "submission_masks_dir = os.path.join(submission_dir, \"masks_preds\")\n",
    "os.makedirs(submission_masks_dir, exist_ok=True)\n",
    "\n",
    "# Create the zip file with both components\n",
    "zip_path = os.path.join(OUTPUT_DIR, f\"mysubmission_{INFERENCE_MODEL_ID}_{SUBMISSION_TIMESTAMP}.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    # Add the json file\n",
    "    zipf.write(dest_json, arcname=\"data_cvqa_sys.json\")\n",
    "    \n",
    "    # Add the masks_preds directory (it will be empty)\n",
    "    zipf.write(submission_masks_dir, arcname=\"masks_preds\")\n",
    "\n",
    "print(f\"Submission package created at: {zip_path}\")\n",
    "print(f\"Files included:\")\n",
    "print(f\" - data_cvqa_sys.json (copied from {most_recent_json})\")\n",
    "print(f\" - masks_preds/ (empty directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f4bfd",
   "metadata": {},
   "source": [
    "Measuring input context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f882d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_tokens(dataset_dir=None, processor=None, num_samples=None):\n",
    "    \"\"\"\n",
    "    Analyze token counts in the dataset without running training or inference\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to the processed dataset directory\n",
    "        processor: The processor from the model\n",
    "        num_samples: Optional limit on number of samples to process\n",
    "    \"\"\"\n",
    "    # Check if processor is provided\n",
    "    if processor is None:\n",
    "        raise ValueError(\"Processor must be provided for tokenization\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Directory not found: {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    token_stats = {\n",
    "        \"samples\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    # Find batch files\n",
    "    batch_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"batch_\") and f.endswith(\".pkl\") or\n",
    "                          f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No batch files found in {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    total_tokens = 0\n",
    "    max_tokens = 0\n",
    "    min_tokens = float('inf')\n",
    "    sample_count = 0\n",
    "    all_token_counts = []\n",
    "    \n",
    "    for batch_file in tqdm(batch_files, desc=\"Analyzing batches\"):\n",
    "        try:\n",
    "            with open(os.path.join(dataset_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            for sample in tqdm(batch_data, desc=f\"Analyzing {batch_file}\", leave=False):\n",
    "                if not isinstance(sample, dict) or \"query_text\" not in sample:\n",
    "                    print(f\"Warning: Unexpected sample format in {batch_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Create the input as it would be during inference                \n",
    "                \n",
    "                system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "                IMPORTANT: \n",
    "                - Respond ONLY with the exact text of the option(s) that apply\n",
    "                - Do not provide any explanations\n",
    "                - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "                - Do not write \"Options:\" or similar prefixes\n",
    "                - Do not write \"Answer:\" or similar prefixes\n",
    "                - Multiple answers should be separated by commas\n",
    "                - If unsure, respond with \"Not mentioned\"\n",
    "                - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "                - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "                \"\"\"\n",
    "                \n",
    "                # Format messages similar to during inference\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": sample[\"query_text\"]},\n",
    "                            # Don't need to actually load the image for token counting\n",
    "                            {\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                # Get formatted text for tokenization\n",
    "                text = processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "                \n",
    "                # Count tokens\n",
    "                tokens = processor.tokenizer.encode(text)\n",
    "                token_count = len(tokens)\n",
    "                all_token_counts.append(token_count)\n",
    "                \n",
    "                # Update statistics\n",
    "                total_tokens += token_count\n",
    "                max_tokens = max(max_tokens, token_count)\n",
    "                min_tokens = min(min_tokens, token_count)\n",
    "                \n",
    "                # Store sample info\n",
    "                token_stats[\"samples\"].append({\n",
    "                    \"id\": sample.get(\"id\", \"unknown\"),\n",
    "                    \"qid\": sample.get(\"qid\", \"unknown\"),\n",
    "                    \"image\": os.path.basename(sample.get(\"image_path\", \"unknown\")),\n",
    "                    \"token_count\": token_count,\n",
    "                    \"text_length\": len(sample[\"query_text\"])\n",
    "                })\n",
    "                \n",
    "                sample_count += 1\n",
    "                if num_samples and sample_count >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if num_samples and sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if sample_count == 0:\n",
    "        print(\"No samples were successfully processed\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    token_stats[\"summary\"] = {\n",
    "        \"total_samples\": sample_count,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_tokens_per_sample\": total_tokens / sample_count,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"min_tokens\": min_tokens,\n",
    "        \"median_tokens\": np.median(all_token_counts),\n",
    "        \"percentile_90\": np.percentile(all_token_counts, 90),\n",
    "        \"percentile_99\": np.percentile(all_token_counts, 99)\n",
    "    }\n",
    "    \n",
    "    # Save the analysis\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(dataset_dir)}_token_analysis.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(token_stats, f, indent=2)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nToken Usage Analysis:\")\n",
    "    print(f\"Total samples analyzed: {sample_count}\")\n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Average tokens per sample: {total_tokens/sample_count:.2f}\")\n",
    "    print(f\"Median tokens per sample: {np.median(all_token_counts):.2f}\")\n",
    "    print(f\"90th percentile: {np.percentile(all_token_counts, 90):.2f}\")\n",
    "    print(f\"99th percentile: {np.percentile(all_token_counts, 99):.2f}\")\n",
    "    print(f\"Max tokens in a sample: {max_tokens}\")\n",
    "    print(f\"Min tokens in a sample: {min_tokens}\")\n",
    "    print(f\"Percentage of 128K context window used (max): {(max_tokens/128000)*100:.2f}%\")\n",
    "    \n",
    "    # Create better histogram\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Use bins based on data range\n",
    "    bin_count = min(50, len(set(all_token_counts)))\n",
    "    \n",
    "    # Plot histogram with actual counts\n",
    "    n, bins, patches = plt.hist(all_token_counts, bins=bin_count, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add mean line\n",
    "    plt.axvline(x=np.mean(all_token_counts), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_token_counts):.1f}')\n",
    "    \n",
    "    # Add median line\n",
    "    plt.axvline(x=np.median(all_token_counts), color='green', linestyle='-', linewidth=2, label=f'Median: {np.median(all_token_counts):.1f}')\n",
    "    \n",
    "    # Add 90th percentile line\n",
    "    plt.axvline(x=np.percentile(all_token_counts, 90), color='orange', linestyle='-.', linewidth=2, label=f'90th percentile: {np.percentile(all_token_counts, 90):.1f}')\n",
    "    \n",
    "    plt.title(f\"Distribution of Token Counts (n={sample_count})\", fontsize=16)\n",
    "    plt.xlabel(\"Token Count\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Samples\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text with stats\n",
    "    stats_text = (\n",
    "        f\"Min: {min_tokens}\\n\"\n",
    "        f\"Max: {max_tokens}\\n\"\n",
    "        f\"Mean: {np.mean(all_token_counts):.1f}\\n\"\n",
    "        f\"Median: {np.median(all_token_counts):.1f}\\n\"\n",
    "        f\"Std Dev: {np.std(all_token_counts):.1f}\\n\"\n",
    "        f\"90th %ile: {np.percentile(all_token_counts, 90):.1f}\\n\"\n",
    "        f\"% of 128K used: {(max_tokens/128000)*100:.2f}%\"\n",
    "    )\n",
    "    \n",
    "    # Position text in the upper right\n",
    "    plt.text(0.95, 0.95, stats_text, \n",
    "             transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', \n",
    "             horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(dataset_dir)}_token_distribution.png\")\n",
    "    plt.savefig(plt_path, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return token_stats\n",
    "\n",
    "# Now run for both datasets using global constants\n",
    "try:\n",
    "    print(\"Analyzing training data tokens...\")\n",
    "    train_token_stats = analyze_dataset_tokens(PROCESSED_DATA_DIR, processor)\n",
    "    \n",
    "    print(\"\\nAnalyzing validation data tokens...\")\n",
    "    val_token_stats = analyze_dataset_tokens(PROCESSED_VAL_DATA_DIR, processor)\n",
    "    \n",
    "    print(\"\\nToken analysis complete! Files saved to the outputs directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running token analysis: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_other_please_specify(df, column_name='valid_answers'):\n",
    "    \"\"\"\n",
    "    Check if \"other (please specify)\" appears as a ground truth answer\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check (train or validation)\n",
    "        column_name: Column containing the answers (default: 'valid_answers')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics about occurrences\n",
    "    \"\"\"\n",
    "    # Convert string representations to lists if needed\n",
    "    if df[column_name].dtype == 'object':\n",
    "        \n",
    "        def safe_eval(x):\n",
    "            try:\n",
    "                if isinstance(x, list):\n",
    "                    return x\n",
    "                if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                    return ast.literal_eval(x)\n",
    "                return [x]  # Return single item as list\n",
    "            except:\n",
    "                return [str(x)]  # Return as single-item list if eval fails\n",
    "        \n",
    "        # Apply the conversion\n",
    "        valid_answers_lists = df[column_name].apply(safe_eval)\n",
    "    else:\n",
    "        valid_answers_lists = df[column_name]\n",
    "    \n",
    "    # Check for matches\n",
    "    matches = []\n",
    "    for idx, answers in enumerate(valid_answers_lists):\n",
    "        for answer in answers:\n",
    "            if isinstance(answer, str) and \"other (please specify)\" in answer.lower():\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"encounter_id\": df.iloc[idx].get('encounter_id', 'unknown'),\n",
    "                    \"base_qid\": df.iloc[idx].get('base_qid', 'unknown'),\n",
    "                    \"answer\": answer,\n",
    "                    \"all_answers\": answers\n",
    "                })\n",
    "    \n",
    "    # Summarize findings\n",
    "    results = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"match_count\": len(matches),\n",
    "        \"percentage\": (len(matches) / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Results for {column_name} in dataset with {len(df)} rows:\")\n",
    "    print(f\"Found {len(matches)} occurrences of 'other (please specify)' ({results['percentage']:.2f}%)\")\n",
    "    \n",
    "    if matches:\n",
    "        print(\"\\nSample matches:\")\n",
    "        for i, match in enumerate(matches[:5]):  # Show up to 5 examples\n",
    "            print(f\"{i+1}. Index {match['index']}, Encounter: {match['encounter_id']}, QID: {match['base_qid']}\")\n",
    "            print(f\"   Answer: {match['answer']}\")\n",
    "            print(f\"   All answers: {match['all_answers']}\")\n",
    "    \n",
    "    return results\n",
    "def check_for_other_please_specify(df, column_name='valid_answers'):\n",
    "    \"\"\"\n",
    "    Check if \"other (please specify)\" appears as a ground truth answer\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check (train or validation)\n",
    "        column_name: Column containing the answers (default: 'valid_answers')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics about occurrences\n",
    "    \"\"\"\n",
    "    # Convert string representations to lists if needed\n",
    "    if df[column_name].dtype == 'object':\n",
    "        \n",
    "        def safe_eval(x):\n",
    "            try:\n",
    "                if isinstance(x, list):\n",
    "                    return x\n",
    "                if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                    return ast.literal_eval(x)\n",
    "                return [x]  # Return single item as list\n",
    "            except:\n",
    "                return [str(x)]  # Return as single-item list if eval fails\n",
    "        \n",
    "        # Apply the conversion\n",
    "        valid_answers_lists = df[column_name].apply(safe_eval)\n",
    "    else:\n",
    "        valid_answers_lists = df[column_name]\n",
    "    \n",
    "    # Check for matches\n",
    "    matches = []\n",
    "    for idx, answers in enumerate(valid_answers_lists):\n",
    "        for answer in answers:\n",
    "            if isinstance(answer, str) and \"other (please specify)\" in answer.lower():\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"encounter_id\": df.iloc[idx].get('encounter_id', 'unknown'),\n",
    "                    \"base_qid\": df.iloc[idx].get('base_qid', 'unknown'),\n",
    "                    \"answer\": answer,\n",
    "                    \"all_answers\": answers\n",
    "                })\n",
    "    \n",
    "    # Summarize findings\n",
    "    results = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"match_count\": len(matches),\n",
    "        \"percentage\": (len(matches) / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Results for {column_name} in dataset with {len(df)} rows:\")\n",
    "    print(f\"Found {len(matches)} occurrences of 'other (please specify)' ({results['percentage']:.2f}%)\")\n",
    "    \n",
    "    if matches:\n",
    "        print(\"\\nSample matches:\")\n",
    "        for i, match in enumerate(matches[:5]):  # Show up to 5 examples\n",
    "            print(f\"{i+1}. Index {match['index']}, Encounter: {match['encounter_id']}, QID: {match['base_qid']}\")\n",
    "            print(f\"   Answer: {match['answer']}\")\n",
    "            print(f\"   All answers: {match['all_answers']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Check training data\n",
    "print(\"CHECKING TRAINING DATA:\")\n",
    "train_results = check_for_other_please_specify(train_df)\n",
    "\n",
    "# Check validation data\n",
    "print(\"\\nCHECKING VALIDATION DATA:\")\n",
    "val_results = check_for_other_please_specify(val_df)\n",
    "\n",
    "# Check specifically for CQID011 (where \"other (please specify)\" is likely)\n",
    "print(\"\\nCHECKING CQID011 IN TRAINING DATA:\")\n",
    "train_cqid011 = train_df[train_df['base_qid'] == 'CQID011']\n",
    "train_cqid011_results = check_for_other_please_specify(train_cqid011)\n",
    "\n",
    "print(\"\\nCHECKING CQID011 IN VALIDATION DATA:\")\n",
    "val_cqid011 = val_df[val_df['base_qid'] == 'CQID011']\n",
    "val_cqid011_results = check_for_other_please_specify(val_cqid011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e98618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCHECKING CQID013 IN VALIDATION DATA:\")\n",
    "val_cqid013 = val_df[val_df['base_qid'] == 'CQID013']\n",
    "val_cqid013_results = check_for_other_please_specify(val_cqid013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20972fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
