{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Retrieve captions + images for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata for training data\n",
    "json_file = os.path.join(\"2024_dataset\", \"train_downloaded.json\")\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access image and caption data for first entry\n",
    "if data:\n",
    "    first_entry = data[0]\n",
    "    image_id = first_entry[\"encounter_id\"]\n",
    "    caption = first_entry[\"responses\"][0][\"content_en\"]\n",
    "    print(f\"Image ID: {image_id}\")\n",
    "    print(f\"Caption: {caption}\")\n",
    "else:\n",
    "    print(\"The dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "images_dir = os.path.join(\"2024_dataset\", \"images\", \"train\")\n",
    "script_path = os.path.join(\"2024_dataset\", \"download_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image paths with captions\n",
    "data_combined = [\n",
    "    {\n",
    "        \"image_path\": os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"),\n",
    "        \"caption\": entry[\"responses\"][0][\"content_en\"],\n",
    "        \"query\": entry[\"query_title_en\"]\n",
    "    }\n",
    "    for entry in data if os.path.exists(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "]\n",
    "\n",
    "# Convert to df\n",
    "df = pd.DataFrame(data_combined)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image and its caption\n",
    "first_row = df.iloc[0]\n",
    "image_path = first_row[\"image_path\"]\n",
    "caption = first_row[\"caption\"]\n",
    "query = first_row[\"query\"]\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open(image_path)\n",
    "img.thumbnail((300, 300))  # Resize to a maximum width and height of 300 pixels\n",
    "display(img)\n",
    "\n",
    "# Print the caption\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Pass it through an initial baseline architecture\n",
    "\n",
    "<img src=\"image.png\" alt=\"image.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load environment variables and configure Gemini\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: is there a med-gemini we can use? https://research.google/blog/advancing-medical-ai-with-med-gemini/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to process the image with Gemini\n",
    "def process_image_with_gemini(image_path, model):\n",
    "    image_prompt = f\"\"\"\n",
    "    Describe the following image in detail for a medical context. Provide a comprehensive description including any visible abnormalities, patterns, or other notable observations.\n",
    "\n",
    "    Image Path: {image_path}\n",
    "\n",
    "    Output the description in plain text without any additional formatting.\n",
    "    \"\"\"\n",
    "    response = model.generate_content(image_prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_description = process_image_with_gemini(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the query, caption, and Gemini description\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Gemini Description: {gemini_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine Gemini's output with the query and generate a response\n",
    "def generate_response(query, image_description, model):\n",
    "    response_prompt = f\"\"\"\n",
    "    Based on the following query and image description, provide a detailed and helpful medical response:\n",
    "\n",
    "    Query: {query}\n",
    "    Image Description: {image_description}\n",
    "\n",
    "    Output the response in plain text without any additional formatting.\n",
    "    \"\"\"\n",
    "    response = model.generate_content(response_prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = generate_response(query, gemini_description, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the query, caption, Gemini description, and generated response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Gemini Description: {gemini_description}\")\n",
    "print(f\"Generated Response: {generated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: How does the above process compare to simply just having a single prompt where both the image and query are provided to retrieve image description? It seems less confident in its understanding of what it could be from these outputs. What else can we do to improve the quality of the repsonse in addition to changing the process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.5: Filter the dataset for valid image paths\n",
    "def filter_valid_entries(data, images_dir):\n",
    "    valid_entries = []\n",
    "    for entry in data:\n",
    "        image_path = os.path.normpath(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "        if os.path.exists(image_path):  # Check if the image file exists\n",
    "            valid_entries.append(entry)\n",
    "        else:\n",
    "            print(f\"Skipping entry with missing image: {entry['encounter_id']}\")\n",
    "    return valid_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Workflow to process an image, query, and caption\n",
    "def process_entry(entry, images_dir, model):\n",
    "    # Construct the full image path\n",
    "    image_path = os.path.normpath(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "    \n",
    "    # Debugging: Print the constructed image path\n",
    "    print(f\"Constructed image path: {image_path}\")\n",
    "    \n",
    "    # Check if the image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image does not exist: {image_path}\")\n",
    "        return None  # Skip entries without an image\n",
    "\n",
    "    # Extract query and caption\n",
    "    query = entry.get(\"query_title_en\", \"No query provided.\")\n",
    "    original_caption = entry[\"responses\"][0][\"content_en\"]\n",
    "\n",
    "    # Process the image with Gemini\n",
    "    image_description = process_image_with_gemini(image_path, model)\n",
    "\n",
    "    # Generate a response\n",
    "    response = generate_response(query, image_description, model)\n",
    "\n",
    "    # Return the combined data\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"query\": query,\n",
    "        \"original_caption\": original_caption,\n",
    "        \"image_description\": image_description,\n",
    "        \"response\": response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "json_file = os.path.join(\"2024_dataset\", \"train_downloaded.json\")\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter the dataset to only include entries with valid image paths\n",
    "filtered_data = filter_valid_entries(data, images_dir)\n",
    "\n",
    "# Process the filtered dataset\n",
    "if filtered_data:\n",
    "    # Example: Process only the first valid entry\n",
    "    first_entry = filtered_data[0]\n",
    "    processed_entry = process_entry(first_entry, images_dir, model)\n",
    "\n",
    "    # Debugging: Check the processed entry\n",
    "    if processed_entry:\n",
    "        print(\"Processed Entry:\")\n",
    "        print(processed_entry)\n",
    "    else:\n",
    "        print(\"Processing failed for the first valid entry.\")\n",
    "else:\n",
    "    print(\"No valid entries with images were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: there seem to be guardrails in place - how to prompt engineer this so the output is as meaningful for the patient + similiar to the conference results needed (aka without the guardrails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Example Data Processing for all entires in the dataset\n",
    "def process_dataset(data, images_dir, model):\n",
    "    results = []\n",
    "    for entry in data:\n",
    "        result = process_entry(entry, images_dir, model)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code when we are ready to process the entire dataset\n",
    "\n",
    "# # Load the dataset\n",
    "# with open(os.path.join('2024_dataset', 'train_downloaded.json'), 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Define the images directory\n",
    "# images_dir = os.path.join(\"2024_dataset\", \"images\", \"train\")\n",
    "\n",
    "# # Configure the Gemini model\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv('API_KEY')\n",
    "# genai.configure(api_key=api_key)\n",
    "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# # Filter valid entries (optional but recommended to avoid missing images)\n",
    "# data_filtered = [entry for entry in data if os.path.exists(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))]\n",
    "\n",
    "# # Process the dataset\n",
    "# processed_df = process_dataset(data_filtered, images_dir, model)\n",
    "\n",
    "# # Inspect the output\n",
    "# print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Display an example in the dataset with side-by-side comparison\n",
    "def display_example(row):\n",
    "    # Load and display the image\n",
    "    img = Image.open(row[\"image_path\"])\n",
    "    img.thumbnail((300, 300))\n",
    "    display(img)\n",
    "\n",
    "    # Print the details\n",
    "    print(f\"Query: {row['query']}\\n\")\n",
    "    print(f\"Original Caption: {row['original_caption']}\\n\")\n",
    "    print(f\"Gemini Image Description: {row['image_description']}\\n\")\n",
    "    print(f\"Generated Response: {row['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code when we are ready to process the entire dataset\n",
    "\n",
    "# # Ensure the DataFrame has been processed\n",
    "# processed_df = process_dataset(filtered_data, images_dir, model)\n",
    "\n",
    "# # Check if the DataFrame is not empty\n",
    "# if not processed_df.empty:\n",
    "#     # Display the first example in the dataset\n",
    "#     display_example(processed_df.iloc[0])\n",
    "# else:\n",
    "#     print(\"No valid entries found in the dataset to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS: How can we add in the following: \n",
    "- Medical chain-of-thought - see https://arxiv.org/abs/2412.13736v1\n",
    "- Figure out to map generative text to multiple choice \n",
    "- Test out augmenting the queries? or image descriptions? with ShareCaptioner\n",
    "- Is there a way to extract high- and low-level image features? Taking inspo from Flickr30k dataset..\n",
    "- What would LLM finetuning look like here? Can we finetune multiple LLMs (region specific) and employ weight-merging or multitask learning?\n",
    "- Could we leverage multimodal explainability here to provide transparency in model's reasoning? https://jayneelparekh.github.io/LMM_Concept_Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
