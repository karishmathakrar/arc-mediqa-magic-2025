{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Import Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON datasets\n",
    "train_images_dir = os.path.join(\"2024_dataset\", \"images\", \"train\")\n",
    "val_images_dir = os.path.join(\"2024_dataset\", \"images\", \"valid\")\n",
    "test_images_dir = os.path.join(\"2024_dataset\", \"images\", \"test\")\n",
    "\n",
    "train_json_file = os.path.join(\"2024_dataset\", \"train_downloaded.json\")\n",
    "with open(train_json_file, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "val_json_file = os.path.join(\"2024_dataset\", \"valid_downloaded.json\")\n",
    "with open(val_json_file, 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "test_json_file = os.path.join(\"2024_dataset\", \"test_downloaded.json\")\n",
    "with open(test_json_file, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for valid image paths\n",
    "def filter_valid_entries(data, images_dir, verbose=False):\n",
    "    valid_entries = []\n",
    "    for entry in data:\n",
    "        image_path = os.path.normpath(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "        if os.path.exists(image_path):  # Check if the image file exists\n",
    "            valid_entries.append(entry)\n",
    "        elif verbose:\n",
    "            print(f\"Skipping entry with missing image: {entry['encounter_id']}\")\n",
    "    return valid_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets that have been filtered for valid image paths\n",
    "train_valid_data = filter_valid_entries(train_data, train_images_dir)\n",
    "val_valid_data = filter_valid_entries(val_data, val_images_dir)\n",
    "test_valid_data = filter_valid_entries(test_data, test_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Retrieve Captions + Images for Single Training Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access image and caption data for first entry in validation set\n",
    "if val_valid_data:\n",
    "    first_entry = val_valid_data[0]\n",
    "    image_id = first_entry[\"encounter_id\"]\n",
    "    caption = first_entry[\"responses\"][0][\"content_en\"]\n",
    "    print(f\"Image ID: {image_id}\")\n",
    "    print(f\"Caption: {caption}\")\n",
    "else:\n",
    "    print(\"The dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image paths with captions\n",
    "data_combined = [\n",
    "    {\n",
    "        \"image_path\": os.path.join(val_images_dir, f\"{entry['encounter_id']}.jpg\"),\n",
    "        \"caption\": entry[\"responses\"][0][\"content_en\"],\n",
    "        \"query\": entry[\"query_title_en\"]\n",
    "    }\n",
    "    for entry in val_valid_data if os.path.exists(os.path.join(val_images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "]\n",
    "\n",
    "# Convert to df\n",
    "df = pd.DataFrame(data_combined)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image and its caption\n",
    "first_row = df.iloc[0]\n",
    "image_path = first_row[\"image_path\"]\n",
    "caption = first_row[\"caption\"]\n",
    "query = first_row[\"query\"]\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open(image_path)\n",
    "img.thumbnail((300, 300))  # Resize to a maximum width and height of 300 pixels\n",
    "display(img)\n",
    "\n",
    "# Print the caption\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Pass it through an initial baseline architecture\n",
    "\n",
    "<img src=\"image.png\" alt=\"image.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load environment variables and configure Gemini\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "genai.configure(api_key=api_key)\n",
    "model_name = 'gemini-1.5-flash'\n",
    "model = genai.GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: is there a med-gemini we can use? https://research.google/blog/advancing-medical-ai-with-med-gemini/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to process the image with Gemini\n",
    "def process_image_with_gemini(image_path, model):\n",
    "    image_prompt = f\"\"\"\n",
    "    Describe the following image in detail for a medical context.\n",
    "    Provide a comprehensive description including any visible abnormalities, patterns, or other notable observations.\n",
    "    Output the description in plain text without any additional formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content([image_prompt, Image.open(image_path)])\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_description = process_image_with_gemini(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the query, caption, and Gemini description\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Gemini Description: {gemini_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine Gemini's output with the query and generate a response\n",
    "def generate_prompt(query, image_description):\n",
    "    return f\"\"\"\n",
    "    Based on the following query and image description, provide a detailed and helpful medical response:\n",
    "\n",
    "    Query: {query}\n",
    "    Image Description: {image_description}\n",
    "\n",
    "    Output the response in plain text without any additional formatting.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate a fine-tuned dataset and fine-tune the Gemini model\n",
    "# Source: https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python\n",
    "def generate_fine_tuning_dataset():\n",
    "    train_finetuning_data = []\n",
    "    for entry in tqdm(train_valid_data):\n",
    "        image_path = os.path.normpath(os.path.join(train_images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "    \n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "    \n",
    "        # Extract query and caption\n",
    "        query = entry.get(\"query_title_en\", \"No query provided.\")\n",
    "        response = entry[\"responses\"][0][\"content_en\"]\n",
    "    \n",
    "        try:\n",
    "            # Process the image with Gemini\n",
    "            image_description = process_image_with_gemini(image_path, model)\n",
    "            prompt = generate_prompt(query, image_description)\n",
    "    \n",
    "            train_finetuning_data.append({\"text_input\": prompt, \"output\": response})\n",
    "        except Exception as e:\n",
    "            print(\"Skipping entry due to following error: \", e)\n",
    "\n",
    "        # This is required to keep the requests less than 15 RPM\n",
    "        time.sleep(5)\n",
    "    return train_finetuning_data\n",
    "\n",
    "print(\"Generating fine-tuning dataset...\")\n",
    "train_finetuning_data = generate_fine_tuning_dataset()\n",
    "\n",
    "print(\"Fine-tuning model...\")\n",
    "# The hyperaparameters are \n",
    "operation = genai.create_tuned_model(\n",
    "    display_name=\"mediqa\",\n",
    "    source_model=\"models/gemini-1.5-flash-001-tuning\",\n",
    "    epoch_count=20,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.001,\n",
    "    training_data=train_finetuning_data,\n",
    ")\n",
    "\n",
    "for status in operation.wait_bar():\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Learning Curve from Fine-Tuning\n",
    "result = operation.result()\n",
    "snapshots = pd.DataFrame(result.tuning_task.snapshots)\n",
    "sns.lineplot(data=snapshots, x='epoch', y='mean_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Get the mediqa fine-tuned model\n",
    "fine_tuned_model = None\n",
    "for m in genai.list_tuned_models():\n",
    "    if \"tunedModels\" in m.name and \"mediqa\" in m.name:\n",
    "        fine_tuned_model = genai.GenerativeModel(model_name=m.name)\n",
    "        break\n",
    "\n",
    "if fine_tuned_model is None:\n",
    "    print(\"ERROR: Unable to find fine-tuned model\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Generate the response from the fine-tuned model\n",
    "def generate_response(query, image_description, model):\n",
    "    response_prompt = generate_prompt(query, image_description)\n",
    "    response = model.generate_content(response_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "generated_response = generate_response(query, gemini_description, fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Print the query, caption, Gemini description, and generated response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Gemini Description: {gemini_description}\")\n",
    "print(f\"Generated Response: {generated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Process Entire Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: How does the above process compare to simply just having a single prompt where both the image and query are provided to retrieve image description? It seems less confident in its understanding of what it could be from these outputs. What else can we do to improve the quality of the repsonse in addition to changing the process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Workflow to process an image, query, and caption\n",
    "def process_entry(entry, images_dir, model):\n",
    "    # Construct the full image path\n",
    "    image_path = os.path.normpath(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))\n",
    "    \n",
    "    # Debugging: Print the constructed image path\n",
    "    print(f\"Constructed image path: {image_path}\")\n",
    "    \n",
    "    # Check if the image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image does not exist: {image_path}\")\n",
    "        return None  # Skip entries without an image\n",
    "\n",
    "    # Extract query and caption\n",
    "    query = entry.get(\"query_title_en\", \"No query provided.\")\n",
    "    original_caption = entry[\"responses\"][0][\"content_en\"]\n",
    "\n",
    "    # Process the image with Gemini\n",
    "    image_description = process_image_with_gemini(image_path, model)\n",
    "\n",
    "    # Generate a response\n",
    "    response = generate_response(query, image_description, model)\n",
    "\n",
    "    # Return the combined data\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"query\": query,\n",
    "        \"original_caption\": original_caption,\n",
    "        \"image_description\": image_description,\n",
    "        \"response\": response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "json_file = os.path.join(\"2024_dataset\", \"train_downloaded.json\")\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter the dataset to only include entries with valid image paths\n",
    "filtered_data = filter_valid_entries(data, images_dir)\n",
    "\n",
    "# Process the filtered dataset\n",
    "if filtered_data:\n",
    "    # Example: Process only the first valid entry\n",
    "    first_entry = filtered_data[0]\n",
    "    processed_entry = process_entry(first_entry, images_dir, model)\n",
    "\n",
    "    # Debugging: Check the processed entry\n",
    "    if processed_entry:\n",
    "        print(\"Processed Entry:\")\n",
    "        print(processed_entry)\n",
    "    else:\n",
    "        print(\"Processing failed for the first valid entry.\")\n",
    "else:\n",
    "    print(\"No valid entries with images were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: there seem to be guardrails in place - how to prompt engineer this so the output is as meaningful for the patient + similiar to the conference results needed (aka without the guardrails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Example Data Processing for all entires in the dataset\n",
    "def process_dataset(data, images_dir, model):\n",
    "    results = []\n",
    "    for entry in data:\n",
    "        result = process_entry(entry, images_dir, model)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code when we are ready to process the entire dataset\n",
    "\n",
    "# # Load the dataset\n",
    "# with open(os.path.join('2024_dataset', 'train_downloaded.json'), 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Define the images directory\n",
    "# images_dir = os.path.join(\"2024_dataset\", \"images\", \"train\")\n",
    "\n",
    "# # Configure the Gemini model\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv('API_KEY')\n",
    "# genai.configure(api_key=api_key)\n",
    "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# # Filter valid entries (optional but recommended to avoid missing images)\n",
    "# data_filtered = [entry for entry in data if os.path.exists(os.path.join(images_dir, f\"{entry['encounter_id']}.jpg\"))]\n",
    "\n",
    "# # Process the dataset\n",
    "# processed_df = process_dataset(data_filtered, images_dir, model)\n",
    "\n",
    "# # Inspect the output\n",
    "# print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Display an example in the dataset with side-by-side comparison\n",
    "def display_example(row):\n",
    "    # Load and display the image\n",
    "    img = Image.open(row[\"image_path\"])\n",
    "    img.thumbnail((300, 300))\n",
    "    display(img)\n",
    "\n",
    "    # Print the details\n",
    "    print(f\"Query: {row['query']}\\n\")\n",
    "    print(f\"Original Caption: {row['original_caption']}\\n\")\n",
    "    print(f\"Gemini Image Description: {row['image_description']}\\n\")\n",
    "    print(f\"Generated Response: {row['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code when we are ready to process the entire dataset\n",
    "\n",
    "# # Ensure the DataFrame has been processed\n",
    "# processed_df = process_dataset(filtered_data, images_dir, model)\n",
    "\n",
    "# # Check if the DataFrame is not empty\n",
    "# if not processed_df.empty:\n",
    "#     # Display the first example in the dataset\n",
    "#     display_example(processed_df.iloc[0])\n",
    "# else:\n",
    "#     print(\"No valid entries found in the dataset to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS: How can we add in the following: \n",
    "- Medical chain-of-thought - see https://arxiv.org/abs/2412.13736v1\n",
    "- Figure out to map generative text to multiple choice \n",
    "- Test out augmenting the queries? or image descriptions? with ShareCaptioner\n",
    "- Is there a way to extract high- and low-level image features? Taking inspo from Flickr30k dataset..\n",
    "- What would LLM finetuning look like here? Can we finetune multiple LLMs (region specific) and employ weight-merging or multitask learning?\n",
    "- Could we leverage multimodal explainability here to provide transparency in model's reasoning? https://jayneelparekh.github.io/LMM_Concept_Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
