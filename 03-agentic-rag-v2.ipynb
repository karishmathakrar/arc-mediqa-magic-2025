{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad691db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MODEL_PREDICTIONS_DIR = os.path.join(os.getcwd(), \"outputs\", \"outputs-akshay-2\", \"outputs\")\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n",
    "    VAL_DATASET_PATH = os.path.join(OUTPUT_DIR, \"val_dataset.csv\")\n",
    "    IMAGES_DIR = os.path.join(os.getcwd(), \"2025_dataset\", \"valid\", \"images_valid\")\n",
    "    GEMINI_MODEL = \"gemini-2.5-flash-preview-04-17\"\n",
    "    MAX_REFLECTION_CYCLES = 2\n",
    "    CONFIDENCE_THRESHOLD = 0.8  # Threshold for accepting an answer without reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19496960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def get_latest_aggregated_files(model_predictions_dir):\n",
    "        \"\"\"Get the latest aggregated prediction files for each model.\"\"\"\n",
    "        pattern = os.path.join(model_predictions_dir, \"aggregated_predictions_*.csv\")\n",
    "        \n",
    "        agg_files = glob.glob(pattern)\n",
    "        \n",
    "        if len(agg_files) == 0:\n",
    "            return []\n",
    "        \n",
    "        latest_files = {}\n",
    "        \n",
    "        for file_path in agg_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(\"_base_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            model_part = parts[0].replace(\"aggregated_predictions_\", \"\")\n",
    "            model_name = model_part\n",
    "            \n",
    "            timestamps = re.findall(r'(\\d+)', parts[1])\n",
    "            if len(timestamps) < 2:\n",
    "                print(f\"Warning: Could not find timestamps in {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            timestamp = int(timestamps[1])\n",
    "            \n",
    "            if model_name not in latest_files or timestamp > latest_files[model_name]['timestamp']:\n",
    "                latest_files[model_name] = {\n",
    "                    'file_path': file_path,\n",
    "                    'timestamp': timestamp\n",
    "                }\n",
    "        \n",
    "        return [info['file_path'] for _, info in latest_files.items()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_model_predictions(model_predictions_dir):\n",
    "        \"\"\"Load all model predictions from aggregated files.\"\"\"\n",
    "        latest_files = DataLoader.get_latest_aggregated_files(model_predictions_dir)\n",
    "        \n",
    "        if not latest_files:\n",
    "            print(\"No aggregated prediction files found. Cannot proceed.\")\n",
    "            return {}\n",
    "        \n",
    "        model_predictions = {}\n",
    "        \n",
    "        for file_path in latest_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            parts = file_name.split(\"_base_\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Warning: Unexpected filename format: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            model_name = parts[0].replace(\"aggregated_predictions_\", \"\")\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                model_predictions[model_name] = df\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        return model_predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def load_validation_dataset(val_dataset_path):\n",
    "        \"\"\"Load the validation dataset.\"\"\"\n",
    "        val_df = pd.read_csv(val_dataset_path)\n",
    "        \n",
    "        val_df = DataLoader.process_validation_dataset(val_df)\n",
    "        \n",
    "        encounter_question_data = defaultdict(lambda: {\n",
    "            'images': [],\n",
    "            'data': None\n",
    "        })\n",
    "        \n",
    "        for _, row in val_df.iterrows():\n",
    "            encounter_id = row['encounter_id']\n",
    "            base_qid = row['base_qid']\n",
    "            key = (encounter_id, base_qid)\n",
    "            \n",
    "            if 'image_path' in row and row['image_path']:\n",
    "                encounter_question_data[key]['images'].append(row['image_path'])\n",
    "            elif 'image_id' in row and row['image_id']:\n",
    "                image_path = os.path.join(Config.IMAGES_DIR, row['image_id'])\n",
    "                encounter_question_data[key]['images'].append(image_path)\n",
    "            \n",
    "            if encounter_question_data[key]['data'] is None:\n",
    "                encounter_question_data[key]['data'] = row.to_dict()\n",
    "        \n",
    "        grouped_data = []\n",
    "        for (encounter_id, base_qid), data in encounter_question_data.items():\n",
    "            entry = data['data'].copy()\n",
    "            entry['all_images'] = data['images']\n",
    "            entry['encounter_id'] = encounter_id\n",
    "            entry['base_qid'] = base_qid\n",
    "            grouped_data.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(grouped_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_convert_options(options_str):\n",
    "        \"\"\"Safely convert a string representation of a list to an actual list.\"\"\"\n",
    "        if not isinstance(options_str, str):\n",
    "            return options_str\n",
    "            \n",
    "        try:\n",
    "            return ast.literal_eval(options_str)\n",
    "        except (SyntaxError, ValueError):\n",
    "            if options_str.startswith('[') and options_str.endswith(']'):\n",
    "                return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "            elif ',' in options_str:\n",
    "                return [opt.strip() for opt in options_str.split(',')]\n",
    "            else:\n",
    "                return [options_str]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_validation_dataset(val_df):\n",
    "        \"\"\"Process and clean the validation dataset.\"\"\"\n",
    "        if 'options_en' in val_df.columns:\n",
    "            val_df['options_en'] = val_df['options_en'].apply(DataLoader.safe_convert_options)\n",
    "            \n",
    "            def clean_options(options):\n",
    "                if not isinstance(options, list):\n",
    "                    return options\n",
    "                    \n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                return cleaned_options\n",
    "                \n",
    "            val_df['options_en_cleaned'] = val_df['options_en'].apply(clean_options)\n",
    "        \n",
    "        if 'question_text' in val_df.columns:\n",
    "            val_df['question_text_cleaned'] = val_df['question_text'].apply(\n",
    "                lambda q: q.replace(\" Please specify which affected area for each selection.\", \"\") \n",
    "                          if isinstance(q, str) and \"Please specify which affected area for each selection\" in q \n",
    "                          else q\n",
    "            )\n",
    "            \n",
    "            val_df['question_text_cleaned'] = val_df['question_text_cleaned'].apply(\n",
    "                lambda q: re.sub(r'^\\d+\\s+', '', q) if isinstance(q, str) else q\n",
    "            )\n",
    "        \n",
    "        if 'base_qid' not in val_df.columns and 'qid' in val_df.columns:\n",
    "            val_df['base_qid'] = val_df['qid'].apply(\n",
    "                lambda q: q.split('-')[0] if isinstance(q, str) and '-' in q else q\n",
    "            )\n",
    "        \n",
    "        return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d325cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def create_query_context(row):\n",
    "        \"\"\"Create query context from validation data similar to the inference process.\"\"\"\n",
    "        question = row.get('question_text_cleaned', row.get('question_text', 'What do you see in this image?'))\n",
    "        \n",
    "        metadata = \"\"\n",
    "        if 'question_type_en' in row:\n",
    "            metadata += f\"Type: {row['question_type_en']}\"\n",
    "            \n",
    "        if 'question_category_en' in row:\n",
    "            metadata += f\", Category: {row['question_category_en']}\"\n",
    "        \n",
    "        query_title = row.get('query_title_en', '')\n",
    "        query_content = row.get('query_content_en', '')\n",
    "        \n",
    "        clinical_context = \"\"\n",
    "        if query_title or query_content:\n",
    "            clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "            if query_title:\n",
    "                clinical_context += f\"{query_title}\\n\"\n",
    "            if query_content:\n",
    "                clinical_context += f\"{query_content}\\n\"\n",
    "        \n",
    "        options = row.get('options_en_cleaned', row.get('options_en', ['Yes', 'No', 'Not mentioned']))\n",
    "        if isinstance(options, list):\n",
    "            options_text = \", \".join(options)\n",
    "        else:\n",
    "            options_text = str(options)\n",
    "        \n",
    "        query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                     f\"Question Metadata: {metadata}\\n\"\n",
    "                     f\"{clinical_context}\"\n",
    "                     f\"Available Options (choose from these): {options_text}\")\n",
    "        \n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGData:\n",
    "    def __init__(self, all_models_df, validation_df):\n",
    "        self.all_models_df = all_models_df\n",
    "        self.validation_df = validation_df\n",
    "        \n",
    "        self.model_predictions = {}\n",
    "        for (encounter_id, base_qid), group in all_models_df.groupby(['encounter_id', 'base_qid']):\n",
    "            self.model_predictions[(encounter_id, base_qid)] = group\n",
    "        \n",
    "        self.validation_data = {}\n",
    "        for _, row in validation_df.iterrows():\n",
    "            self.validation_data[(row['encounter_id'], row['base_qid'])] = row\n",
    "    \n",
    "    def get_combined_data(self, encounter_id, base_qid):\n",
    "        \"\"\"Retrieve combined data for a specific encounter and question.\"\"\"\n",
    "        model_preds = self.model_predictions.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        val_data = self.validation_data.get((encounter_id, base_qid), None)\n",
    "        \n",
    "        if model_preds is None:\n",
    "            print(f\"No model predictions found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "            \n",
    "        if val_data is None:\n",
    "            print(f\"No validation data found for encounter {encounter_id}, question {base_qid}\")\n",
    "            return None\n",
    "        \n",
    "        if 'query_context' not in val_data:\n",
    "            val_data['query_context'] = DataProcessor.create_query_context(val_data)\n",
    "        \n",
    "        model_predictions_dict = {}\n",
    "        for _, row in model_preds.iterrows():\n",
    "            model_name = row['model_name']\n",
    "            \n",
    "            model_predictions_dict[model_name] = self._process_model_predictions(row)\n",
    "        \n",
    "        return {\n",
    "            'encounter_id': encounter_id,\n",
    "            'base_qid': base_qid,\n",
    "            'query_context': val_data['query_context'],\n",
    "            'images': val_data.get('all_images', []),\n",
    "            'options': val_data.get('options_en_cleaned', val_data.get('options_en', [])),\n",
    "            'question_type': val_data.get('question_type_en', ''),\n",
    "            'question_category': val_data.get('question_category_en', ''),\n",
    "            'model_predictions': model_predictions_dict\n",
    "        }\n",
    "    \n",
    "    def _process_model_predictions(self, row):\n",
    "        \"\"\"Process model predictions from row data.\"\"\"\n",
    "#         unique_preds = row.get('unique_predictions', [])\n",
    "#         if isinstance(unique_preds, str):\n",
    "#             try:\n",
    "#                 unique_preds = ast.literal_eval(unique_preds)\n",
    "#             except:\n",
    "#                 unique_preds = [unique_preds]\n",
    "                \n",
    "#         raw_preds = row.get('all_raw_predictions', [])\n",
    "#         if isinstance(raw_preds, str):\n",
    "#             try:\n",
    "#                 raw_preds = ast.literal_eval(raw_preds)\n",
    "#             except:\n",
    "#                 raw_preds = [raw_preds]\n",
    "                \n",
    "#         sorted_preds = row.get('all_sorted_predictions', [])\n",
    "#         if isinstance(sorted_preds, str):\n",
    "#             try:\n",
    "#                 sorted_preds = ast.literal_eval(sorted_preds)\n",
    "#             except:\n",
    "#                 sorted_preds = [(str(raw_preds[0]), 1)] if raw_preds else []\n",
    "        \n",
    "        return {\n",
    "            'model_prediction': row.get('combined_prediction', '')\n",
    "#             'unique_predictions': unique_preds,\n",
    "#             'all_raw_predictions': raw_preds,\n",
    "#             'all_sorted_predictions': sorted_preds\n",
    "        }\n",
    "    \n",
    "    def get_all_encounter_question_pairs(self):\n",
    "        \"\"\"Return a list of all unique encounter_id, base_qid pairs.\"\"\"\n",
    "        return list(self.validation_data.keys())\n",
    "    \n",
    "    def get_sample_data(self, n=5):\n",
    "        \"\"\"Get a sample of combined data for n random encounter-question pairs.\"\"\"\n",
    "        import random\n",
    "        \n",
    "        all_pairs = self.get_all_encounter_question_pairs()\n",
    "        sample_pairs = random.sample(all_pairs, min(n, len(all_pairs)))\n",
    "        \n",
    "        return [self.get_combined_data(encounter_id, base_qid) for encounter_id, base_qid in sample_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAnalysisService:\n",
    "    \"\"\"Service for analyzing dermatological images.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def analyze_images(self, image_paths, encounter_id):\n",
    "        \"\"\"\n",
    "        Analyze multiple dermatological images for an encounter.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of paths to images\n",
    "            encounter_id: Encounter identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with individual and aggregated analyses\n",
    "        \"\"\"\n",
    "        image_analyses = []\n",
    "        \n",
    "        structured_prompt = self._create_dermatology_prompt()\n",
    "        \n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            analysis = self._analyze_single_image(\n",
    "                img_path, \n",
    "                structured_prompt, \n",
    "                encounter_id, \n",
    "                idx, \n",
    "                len(image_paths)\n",
    "            )\n",
    "            image_analyses.append(analysis)\n",
    "        \n",
    "        aggregated_analysis = self._aggregate_analyses(image_analyses, encounter_id)\n",
    "        \n",
    "        return {\n",
    "            \"encounter_id\": encounter_id,\n",
    "            \"image_count\": len(image_paths),\n",
    "            \"individual_analyses\": image_analyses,\n",
    "            \"aggregated_analysis\": aggregated_analysis\n",
    "        }\n",
    "    \n",
    "    def _create_dermatology_prompt(self):\n",
    "        \"\"\"Create the structured dermatology analysis prompt.\"\"\"\n",
    "        return \"\"\"As dermatology specialist analyzing skin images, extract and structure all clinically relevant information from this dermatological image.\n",
    "\n",
    "Organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is cannot be determined, state \"Cannot determine from image\".\n",
    "\"\"\"\n",
    "    \n",
    "    def _analyze_single_image(self, img_path, prompt, encounter_id, idx, total_images):\n",
    "        \"\"\"Analyze a single dermatological image.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            print(f\"Analyzing image {idx+1}/{total_images} for encounter {encounter_id}\")\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt, image]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "            \n",
    "            structured_analysis = self._parse_json_response(analysis_text)\n",
    "            \n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"structured_analysis\": structured_analysis\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image {img_path}: {str(e)}\")\n",
    "            return {\n",
    "                \"image_index\": idx + 1,\n",
    "                \"image_path\": os.path.basename(img_path),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}\n",
    "    \n",
    "    def _aggregate_analyses(self, image_analyses, encounter_id):\n",
    "        \"\"\"Aggregate structured analyses from multiple images.\"\"\"\n",
    "        valid_analyses = [a for a in image_analyses if \"error\" not in a and \"structured_analysis\" in a]\n",
    "        \n",
    "        if not valid_analyses:\n",
    "            return {\n",
    "                \"error\": \"No valid analyses to aggregate\",\n",
    "                \"message\": \"Unable to generate aggregated analysis due to errors in individual analyses.\"\n",
    "            }\n",
    "        \n",
    "        if len(valid_analyses) == 1:\n",
    "            return valid_analyses[0][\"structured_analysis\"]\n",
    "        \n",
    "        analysis_jsons = []\n",
    "        for analysis in valid_analyses:\n",
    "            analysis_json = json.dumps(analysis[\"structured_analysis\"])\n",
    "            analysis_jsons.append(f\"Image {analysis['image_index']} ({analysis['image_path']}): {analysis_json}\")\n",
    "        \n",
    "        aggregation_prompt = self._create_aggregation_prompt(analysis_jsons)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[aggregation_prompt]\n",
    "            )\n",
    "            \n",
    "            aggregation_text = response.text\n",
    "            \n",
    "            aggregated_analysis = self._parse_json_response(aggregation_text)\n",
    "            \n",
    "            return aggregated_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating aggregated analysis for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"aggregation_error\": \"Failed to generate aggregated analysis\"\n",
    "            }\n",
    "    \n",
    "    def _create_aggregation_prompt(self, analysis_jsons):\n",
    "        \"\"\"Create a prompt for aggregating multiple image analyses.\"\"\"\n",
    "        return f\"\"\"As dermatology specialist reviewing multiple skin image analyses for the same patient, combine these analyses and organize your response in a JSON dictionary:\n",
    "\n",
    "1. SIZE: Approximate dimensions of lesions/affected areas, size comparison (thumbnail, palm, larger), Relative size comparisons for multiple lesions\n",
    "2. SITE_LOCATION: Visible body parts in the image, body areas showing lesions/abnormalities, Specific anatomical locations affected\n",
    "3. SKIN_DESCRIPTION: Lesion morphology (flat, raised, depressed), Texture of affected areas, Surface characteristics (scales, crust, fluid), Appearance of lesion boundaries\n",
    "4. LESION_COLOR: Predominant color(s) of affected areas, Color variations within lesions, Color comparison to normal skin, Color distribution patterns\n",
    "5. LESION_COUNT: Number of distinct lesions/affected areas, Single vs multiple presentation, Distribution pattern if multiple, Any counting limitations\n",
    "6. EXTENT: How widespread the condition appears, Localized vs widespread assessment, Approximate percentage of visible skin affected, Limitations in determining full extent\n",
    "7. TEXTURE: Expected tactile qualities, Smooth vs rough assessment, Notable textural features, Texture consistency across affected areas\n",
    "8. ONSET_INDICATORS: Visual clues about condition duration, Acute vs chronic presentation features, Healing/progression/chronicity signs, Note: precise timing cannot be determined from images\n",
    "9. ITCH_INDICATORS: Scratch marks/excoriations/trauma signs, Features associated with itchy conditions, Pruritic vs non-pruritic visual indicators, Note: sensation cannot be directly observed\n",
    "10. OVERALL_IMPRESSION: Brief description (1-2 sentences), Key diagnostic features, Potential diagnoses (2-3)\n",
    "    \n",
    "{' '.join(analysis_jsons)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250979dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalContextAnalyzer:\n",
    "    \"\"\"Service for analyzing clinical context.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def extract_clinical_context(self, query_context, encounter_id):\n",
    "        \"\"\"\n",
    "        Extract structured clinical information from an encounter's query context.\n",
    "        \n",
    "        Args:\n",
    "            query_context: The query context text\n",
    "            encounter_id: Encounter identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with structured clinical information\n",
    "        \"\"\"\n",
    "        clinical_text = self._extract_clinical_text(query_context)\n",
    "        \n",
    "        if not clinical_text:\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"clinical_summary\": \"No clinical information available\"\n",
    "            }\n",
    "        \n",
    "        prompt = self._create_clinical_context_prompt(clinical_text)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            structured_context = self._parse_json_response(response.text)\n",
    "            \n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"structured_clinical_context\": structured_context\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting clinical context for encounter {encounter_id}: {str(e)}\")\n",
    "            return {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"raw_clinical_text\": clinical_text,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_clinical_text(self, query_context):\n",
    "        \"\"\"Extract clinical text from query context.\"\"\"\n",
    "        clinical_lines = []\n",
    "        capturing = False\n",
    "        for line in query_context.split('\\n'):\n",
    "            if \"Background Clinical Information\" in line:\n",
    "                capturing = True\n",
    "                continue\n",
    "            elif \"Available Options\" in line:\n",
    "                capturing = False\n",
    "            elif capturing:\n",
    "                clinical_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(clinical_lines).strip()\n",
    "    \n",
    "    def _create_clinical_context_prompt(self, clinical_text):\n",
    "        \"\"\"Create prompt for extracting structured clinical information.\"\"\"\n",
    "        return f\"\"\"You are a dermatology specialist analyzing patient information. \n",
    "Extract and structure all clinically relevant information from this patient description:\n",
    "\n",
    "{clinical_text}\n",
    "\n",
    "Organize your response in the following JSON structure:\n",
    "\n",
    "1. DEMOGRAPHICS: Age, sex, and any other demographic data\n",
    "2. SITE_LOCATION: Body parts affected by the condition as described in the text\n",
    "3. SKIN_DESCRIPTION: Any mention of lesion morphology (flat, raised, depressed), texture, surface characteristics (scales, crust, fluid), appearance of lesion boundaries\n",
    "4. LESION_COLOR: Any description of color(s) of affected areas, color variations, comparison to normal skin\n",
    "5. LESION_COUNT: Any information about number of lesions, single vs multiple presentation, distribution pattern\n",
    "6. EXTENT: How widespread the condition appears based on the description, localized vs widespread\n",
    "7. TEXTURE: Any description of tactile qualities, smooth vs rough, notable textural features\n",
    "8. ONSET_INDICATORS: Information about onset, duration, progression, or evolution of symptoms\n",
    "9. ITCH_INDICATORS: Mentions of scratching, itchiness, or other sensory symptoms\n",
    "10. OTHER_SYMPTOMS: Any additional symptoms mentioned (pain, burning, etc.)\n",
    "11. TRIGGERS: Identified factors that worsen/improve the condition\n",
    "12. HISTORY: Relevant past medical history or previous treatments\n",
    "13. DIAGNOSTIC_CONSIDERATIONS: Any mentioned or suggested diagnoses in the text\n",
    "\n",
    "Be concise and use medical terminology where appropriate. If information for a section is \n",
    "not available, indicate \"Not mentioned\".\n",
    "\"\"\"\n",
    "\n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceIntegrator:\n",
    "    \"\"\"Integrates visual and clinical evidence.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def integrate_evidence(self, image_analysis, clinical_context, question_type):\n",
    "        \"\"\"\n",
    "        Integrate image analysis with clinical context.\n",
    "        \n",
    "        Args:\n",
    "            image_analysis: Structured image analysis\n",
    "            clinical_context: Structured clinical context\n",
    "            question_type: Type of question being asked\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with integrated evidence\n",
    "        \"\"\"\n",
    "        # Determine weighting based on question type\n",
    "        weights = self._get_weights_for_question(question_type)\n",
    "        \n",
    "        # Create prompt for integration\n",
    "        prompt = self._create_integration_prompt(\n",
    "            image_analysis,\n",
    "            clinical_context,\n",
    "            question_type,\n",
    "            weights\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            integration_text = response.text\n",
    "            \n",
    "            integrated_evidence = self._parse_json_response(integration_text)\n",
    "            \n",
    "            return integrated_evidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error integrating evidence: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"message\": \"Failed to integrate evidence\"\n",
    "            }\n",
    "    \n",
    "    def _get_weights_for_question(self, question_type):\n",
    "        \"\"\"\n",
    "        Determine evidence weighting based on question type.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with weights for each evidence type\n",
    "        \"\"\"\n",
    "        weights = {\n",
    "            \"Site Location\": {\"image\": 0.8, \"clinical\": 0.2},\n",
    "            \"Lesion Color\": {\"image\": 0.9, \"clinical\": 0.1},\n",
    "            \"Size\": {\"image\": 0.8, \"clinical\": 0.2},\n",
    "            \"Skin Description\": {\"image\": 0.7, \"clinical\": 0.3},\n",
    "            \"Duration of Symptoms\": {\"image\": 0.3, \"clinical\": 0.7},\n",
    "            \"Itch\": {\"image\": 0.4, \"clinical\": 0.6},\n",
    "            \"Extent\": {\"image\": 0.7, \"clinical\": 0.3},\n",
    "            \"Treatment\": {\"image\": 0.1, \"clinical\": 0.9},\n",
    "            \"Lesion Evolution\": {\"image\": 0.3, \"clinical\": 0.7},\n",
    "            \"Texture\": {\"image\": 0.6, \"clinical\": 0.4},\n",
    "            \"Specific Diagnosis\": {\"image\": 0.5, \"clinical\": 0.5},\n",
    "            \"Count\": {\"image\": 0.8, \"clinical\": 0.2},\n",
    "            \"Differential\": {\"image\": 0.5, \"clinical\": 0.5},\n",
    "        }\n",
    "        \n",
    "        # Default weights if question type not found\n",
    "        return weights.get(question_type, {\"image\": 0.5, \"clinical\": 0.5})\n",
    "    \n",
    "    def _create_integration_prompt(self, image_analysis, clinical_context, question_type, weights):\n",
    "        \"\"\"Create prompt for evidence integration.\"\"\"\n",
    "        return f\"\"\"As a dermatology specialist, integrate the visual findings from images with the clinical history.\n",
    "\n",
    "IMAGE ANALYSIS:\n",
    "{json.dumps(image_analysis.get(\"aggregated_analysis\", {}), indent=2)}\n",
    "\n",
    "CLINICAL CONTEXT:\n",
    "{json.dumps(clinical_context.get(\"structured_clinical_context\", {}), indent=2)}\n",
    "\n",
    "For this {question_type} question, image evidence has {weights['image']*100}% weight and clinical evidence has {weights['clinical']*100}% weight.\n",
    "\n",
    "Pay special attention to potential contradictions between visual findings and clinical history. Even minor inconsistencies should be noted as contradictions. Look for cases where clinical context suggests features not visible in images or where visual findings seem to contradict patient-reported symptoms or history.\n",
    "\n",
    "Organize your response in a JSON structure with the following elements:\n",
    "\n",
    "1. INTEGRATED_FINDINGS: For each key dermatological feature, combine visual and clinical evidence\n",
    "   - SIZE\n",
    "   - SITE_LOCATION\n",
    "   - SKIN_DESCRIPTION\n",
    "   - LESION_COLOR\n",
    "   - LESION_COUNT\n",
    "   - EXTENT\n",
    "   - TEXTURE\n",
    "   - ONSET_DURATION\n",
    "   - SYMPTOMS\n",
    "\n",
    "2. CONCORDANCE_ASSESSMENT: For each feature, assess if visual and clinical evidence are:\n",
    "   - CONCORDANT: Visual and clinical evidence agree\n",
    "   - DISCORDANT: Visual and clinical evidence conflict (explain the conflict)\n",
    "   - COMPLEMENTARY: Evidence sources provide different but non-conflicting information\n",
    "   - MISSING_VISUAL: Clinical description present but not visible in images\n",
    "   - MISSING_CLINICAL: Visible in images but not mentioned in clinical context\n",
    "\n",
    "3. CONTRADICTIONS: List any specific contradictions between visual and clinical evidence\n",
    "   - For each contradiction, explain what the conflict is and assess which source is more reliable\n",
    "\n",
    "4. WEIGHTED_EVIDENCE_PROFILE: Synthesize the most reliable information for each category\n",
    "   - Apply the provided weights to determine the most reliable facts for each feature\n",
    "   - Explain where you've prioritized one source over another\n",
    "\n",
    "5. CONFIDENCE_SCORES: Score the confidence (0.0-1.0) in the integrated evidence for each feature\n",
    "\n",
    "Be specific, concise, and use medical terminology where appropriate.\n",
    "\"\"\"\n",
    "\n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningEngine:\n",
    "    \"\"\"Applies reasoning to determine the best answer.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "    def apply_initial_reasoning(self, question_text, question_type, options, integrated_evidence, model_predictions):\n",
    "        \"\"\"\n",
    "        Apply initial reasoning to determine the most likely answer.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence from images and clinical context\n",
    "            model_predictions: Model predictions to consider\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with reasoning and answer\n",
    "        \"\"\"\n",
    "        model_prediction_text = self._format_model_predictions(model_predictions)\n",
    "        \n",
    "        multiple_answers_allowed = question_type in [\"Site Location\", \"Size\", \"Skin Description\"]\n",
    "        \n",
    "        prompt = self._create_reasoning_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            model_prediction_text,\n",
    "            multiple_answers_allowed\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reasoning_text = response.text\n",
    "            \n",
    "            reasoning_result = self._parse_json_response(reasoning_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(reasoning_result.get('answer', ''), options)\n",
    "            reasoning_result['validated_answer'] = validated_answer\n",
    "            \n",
    "            return reasoning_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying initial reasoning: {str(e)}\")\n",
    "            return {\n",
    "                \"reasoning\": f\"Error: {str(e)}\",\n",
    "                \"answer\": \"Not mentioned\",\n",
    "                \"validated_answer\": \"Not mentioned\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _format_model_predictions(self, model_predictions):\n",
    "        \"\"\"Format model predictions for the prompt.\"\"\"\n",
    "        model_prediction_text = \"\"\n",
    "        for model_name, predictions in model_predictions.items():\n",
    "            combined_pred = predictions.get('model_prediction', '')\n",
    "            if isinstance(combined_pred, float) and pd.isna(combined_pred):\n",
    "                combined_pred = \"No prediction\"\n",
    "            model_prediction_text += f\"- {model_name}: {combined_pred}\\n\"\n",
    "        return model_prediction_text\n",
    "\n",
    "    def _create_reasoning_prompt(self, question_text, question_type, options, integrated_evidence, model_prediction_text, multiple_answers_allowed):\n",
    "        \"\"\"Create a prompt for the reasoning layer.\"\"\"\n",
    "        specialized_guidance = \"\"\n",
    "        \n",
    "        if question_type == \"Size\" and all(option in \", \".join(options) for option in [\"size of thumb nail\", \"size of palm\", \"larger area\"]):\n",
    "            specialized_guidance = \"\"\"\n",
    "SPECIALIZED GUIDANCE FOR SIZE ASSESSMENT:\n",
    "When answering this size-related question, interpret the options as follows:\n",
    "- \"size of thumb nail\": Individual lesions or affected areas approximately 1-2 cm in diameter\n",
    "- \"size of palm\": Affected areas larger than the size of a thumb nail and roughly the size of a palm (approximately 1% of body surface area), which may include multiple smaller lesions across a region\n",
    "- \"larger area\": Widespread involvement significantly larger than a palm, affecting a substantial portion(s) of the body\n",
    "\n",
    "IMPORTANT: For cases with multiple small lesions that are visible in the images, but without extensive widespread involvement across large body regions, \"size of palm\" is likely the most appropriate answer.\n",
    "\"\"\"\n",
    "        elif question_type == \"Lesion Color\" and \"combination\" in \", \".join(options):\n",
    "            specialized_guidance = \"\"\"\n",
    "SPECIALIZED GUIDANCE FOR LESION COLOR:\n",
    "When answering color-related questions, pay careful attention to whether there are multiple distinct colors present across the affected areas. \"Combination\" would be appropriate when different lesions display different colors (e.g., some lesions appear red while others appear white), or when individual lesions show mixed or varied coloration patterns.\n",
    "\"\"\"\n",
    "\n",
    "        if multiple_answers_allowed:\n",
    "            task_description = \"\"\"\n",
    "Based on all the evidence above, determine the most accurate answer(s) to the question. Your task is to:\n",
    "1. Analyze the integrated evidence\n",
    "2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "3. Provide a detailed reasoning for your conclusion\n",
    "4. Select the final answer(s) from the available options\n",
    "5. Provide a confidence score from 0.0 to 1.0 for your answer. Be conservative in your confidence assessment. Consider all possible sources of uncertainty, including image quality limitations, interpretation ambiguity, and potential contradictions. Confidence scores should rarely exceed 0.8 unless evidence is absolutely conclusive and unambiguous.\n",
    "\n",
    "If selecting multiple answers is appropriate, provide them in a comma-separated list. If no answer can be determined, select \"Not mentioned\".\n",
    "\"\"\"\n",
    "        else:\n",
    "            task_description = \"\"\"\n",
    "Based on all the evidence above, determine the SINGLE most accurate answer to the question. Your task is to:\n",
    "1. Analyze the integrated evidence\n",
    "2. Consider the model predictions, noting any consensus or disagreement, but maintain your critical judgment\n",
    "3. Provide a detailed reasoning for your conclusion\n",
    "4. Select ONLY ONE answer option that is most accurate\n",
    "5. Provide a confidence score from 0.0 to 1.0 for your answer. Be conservative in your confidence assessment. Consider all possible sources of uncertainty, including image quality limitations, interpretation ambiguity, and potential contradictions. Confidence scores should rarely exceed 0.8 unless evidence is absolutely conclusive and unambiguous.\n",
    "\n",
    "For this question type, you must select ONLY ONE option as your answer. If no answer can be determined, select \"Not mentioned\".\n",
    "\"\"\"\n",
    "\n",
    "        return f\"\"\"You are a medical expert analyzing dermatological findings. Use the provided evidence to determine the most accurate answer(s) for the following question:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "MODEL PREDICTIONS:\n",
    "{model_prediction_text}\n",
    "\n",
    "{specialized_guidance}\n",
    "\n",
    "IMPORTANT: While multiple model predictions are provided, be aware that these predictions can be inaccurate or inconsistent. Do not assume majority agreement equals correctness. Evaluate the evidence critically and independently from these predictions. Your job is to determine the correct answer based primarily on the integrated evidence, treating model predictions as secondary suggestions that may contain errors.\n",
    "\n",
    "{task_description}\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"reasoning\": Your step-by-step reasoning process\n",
    "2. \"answer\": Your final answer(s) as a single string or comma-separated list of options\n",
    "3. \"confidence\": A score from 0.0 to 1.0 representing your confidence level in this answer\n",
    "4. \"evidence_used\": The key evidence that supports your answer\n",
    "5. \"uncertainty_factors\": Any factors that reduce your confidence\n",
    "6. \"counterfactual\": What evidence would make you choose a different answer\n",
    "\n",
    "When providing your answer, strictly adhere to the available options and only select from them.\n",
    "\"\"\"\n",
    "\n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}\n",
    "    \n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfReflectionEngine:\n",
    "    \"\"\"Applies self-reflection to the reasoning process.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def apply_reflection(self, question_text, question_type, options, integrated_evidence, reasoning_result):\n",
    "        \"\"\"\n",
    "        Apply self-reflection to the initial reasoning result.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence\n",
    "            reasoning_result: Initial reasoning result\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with reflection results\n",
    "        \"\"\"\n",
    "        prompt = self._create_reflection_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            reasoning_result\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            reflection_text = response.text\n",
    "            \n",
    "            reflection_result = self._parse_json_response(reflection_text)\n",
    "            \n",
    "            if 'revised_answer' in reflection_result:\n",
    "                validated_answer = self._validate_answer(reflection_result.get('revised_answer', ''), options)\n",
    "                reflection_result['validated_revised_answer'] = validated_answer\n",
    "            \n",
    "            return reflection_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying reflection: {str(e)}\")\n",
    "            return {\n",
    "                \"reflection\": f\"Error: {str(e)}\",\n",
    "                \"requires_revision\": False,\n",
    "                \"confidence\": reasoning_result.get('confidence', 0.0),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _create_reflection_prompt(self, question_text, question_type, options, integrated_evidence, reasoning_result):\n",
    "        \"\"\"Create a prompt for the self-reflection layer.\"\"\"\n",
    "        return f\"\"\"You are a medical expert critically reviewing your own reasoning about a dermatological question. \n",
    "Carefully examine the initial reasoning and check for errors, biases, and inconsistencies:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "INITIAL REASONING:\n",
    "{json.dumps(reasoning_result, indent=2)}\n",
    "\n",
    "Your task is to:\n",
    "1. Critically examine the initial reasoning for errors, biases, or incomplete analysis\n",
    "2. Identify any evidence that was overlooked or misinterpreted\n",
    "3. Evaluate whether the confidence level was appropriate\n",
    "4. Determine if a different answer would be more accurate\n",
    "5. Check if the evidence truly supports the chosen answer\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"reflection\": Your critical review of the initial reasoning\n",
    "2. \"overlooked_evidence\": Any important evidence that was missed or undervalued\n",
    "3. \"misinterpreted_evidence\": Any evidence that was incorrectly interpreted\n",
    "4. \"reasoning_gaps\": Logical gaps or assumptions in the initial reasoning\n",
    "5. \"confidence_assessment\": Was the confidence level appropriate? Why or why not?\n",
    "6. \"requires_revision\": Boolean indicating if the answer needs to be revised (true/false)\n",
    "7. \"revised_answer\": If revision is needed, the corrected answer\n",
    "8. \"revised_confidence\": If revision is needed, the corrected confidence level (0.0-1.0)\n",
    "9. \"revision_explanation\": If revision is needed, the explanation for the change\n",
    "\n",
    "Be particularly careful to identify:\n",
    "- Cherry-picking: Did the initial reasoning focus only on evidence supporting its conclusion?\n",
    "- Overconfidence: Was the confidence level too high given the available evidence?\n",
    "- Alternative explanations: Are there valid alternative interpretations of the evidence?\n",
    "- Implicit assumptions: Were there unstated assumptions in the reasoning process?\n",
    "\n",
    "Be honest and thorough in your self-reflection, even if it means acknowledging errors in the initial reasoning.\n",
    "\"\"\"\n",
    "\n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}\n",
    "    \n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReAnalysisEngine:\n",
    "    \"\"\"Handles re-analysis when initial reasoning is insufficient.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def deep_analysis(self, question_text, question_type, options, integrated_evidence, reasoning_result, reflection_result):\n",
    "        \"\"\"\n",
    "        Perform a deeper analysis based on reflection results.\n",
    "        \n",
    "        Args:\n",
    "            question_text: The question text\n",
    "            question_type: The type of question\n",
    "            options: Available answer options\n",
    "            integrated_evidence: Integrated evidence\n",
    "            reasoning_result: Initial reasoning result\n",
    "            reflection_result: Self-reflection result\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with deep analysis result\n",
    "        \"\"\"\n",
    "        prompt = self._create_deep_analysis_prompt(\n",
    "            question_text,\n",
    "            question_type,\n",
    "            options,\n",
    "            integrated_evidence,\n",
    "            reasoning_result,\n",
    "            reflection_result\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=Config.GEMINI_MODEL,\n",
    "                contents=[prompt]\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.text\n",
    "            \n",
    "            deep_analysis = self._parse_json_response(analysis_text)\n",
    "            \n",
    "            validated_answer = self._validate_answer(deep_analysis.get('final_answer', ''), options)\n",
    "            deep_analysis['validated_final_answer'] = validated_answer\n",
    "            \n",
    "            return deep_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error performing deep analysis: {str(e)}\")\n",
    "            return {\n",
    "                \"deep_reasoning\": f\"Error: {str(e)}\",\n",
    "                \"final_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                \"validated_final_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                \"final_confidence\": reasoning_result.get('confidence', 0.0),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _create_deep_analysis_prompt(self, question_text, question_type, options, integrated_evidence, reasoning_result, reflection_result):\n",
    "        \"\"\"Create a prompt for deep analysis.\"\"\"\n",
    "        return f\"\"\"You are a medical expert performing a deep analysis for a dermatological question after identifying issues with initial reasoning.\n",
    "Review all evidence and reasoning paths comprehensively:\n",
    "\n",
    "QUESTION: {question_text}\n",
    "QUESTION TYPE: {question_type}\n",
    "OPTIONS: {\", \".join(options)}\n",
    "\n",
    "INTEGRATED EVIDENCE:\n",
    "{json.dumps(integrated_evidence, indent=2)}\n",
    "\n",
    "INITIAL REASONING:\n",
    "{json.dumps(reasoning_result, indent=2)}\n",
    "\n",
    "REFLECTION:\n",
    "{json.dumps(reflection_result, indent=2)}\n",
    "\n",
    "Your task is to:\n",
    "1. Re-examine ALL available evidence with fresh eyes\n",
    "2. Address the specific issues highlighted in the reflection\n",
    "3. Consider each answer option systematically\n",
    "4. Weigh evidence for and against each potential answer\n",
    "5. Determine the most accurate answer based on comprehensive analysis\n",
    "\n",
    "For issues identified in reflection:\n",
    "- Overlooked evidence: {reflection_result.get('overlooked_evidence', 'None identified')}\n",
    "- Misinterpreted evidence: {reflection_result.get('misinterpreted_evidence', 'None identified')}\n",
    "- Reasoning gaps: {reflection_result.get('reasoning_gaps', 'None identified')}\n",
    "\n",
    "Format your response as a JSON object with these fields:\n",
    "1. \"deep_reasoning\": Your comprehensive analysis considering all evidence and perspectives\n",
    "2. \"systematic_assessment\": Assessment of evidence for EACH possible answer option\n",
    "3. \"final_answer\": Your conclusion after deep analysis\n",
    "4. \"final_confidence\": Your confidence level after deep analysis (0.0-1.0)\n",
    "5. \"key_determinants\": The most important factors that determined your final answer\n",
    "6. \"remaining_uncertainties\": Any unresolved questions or limitations\n",
    "\n",
    "Be thorough, balanced, and precise in your analysis. Consider the evidence holistically and avoid the pitfalls identified in the reflection phase.\n",
    "\"\"\"\n",
    "\n",
    "    def _parse_json_response(self, text):\n",
    "        \"\"\"Parse JSON from LLM response.\"\"\"\n",
    "        cleaned_text = text\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```json\")[1]\n",
    "        if \"```\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.split(\"```\")[0]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse as JSON\")\n",
    "            return {\"parse_error\": \"Could not parse as JSON\", \"raw_text\": text}\n",
    "    \n",
    "    def _validate_answer(self, answer, options):\n",
    "        \"\"\"Validate the answer against available options.\"\"\"\n",
    "        if not answer:\n",
    "            return \"Not mentioned\"\n",
    "            \n",
    "        answer = answer.lower()\n",
    "        valid_answers = []\n",
    "        \n",
    "        if ',' in answer:\n",
    "            answer_parts = [part.strip() for part in answer.split(',')]\n",
    "            for part in answer_parts:\n",
    "                for option in options:\n",
    "                    if part == option.lower():\n",
    "                        valid_answers.append(option)\n",
    "        else:\n",
    "            for option in options:\n",
    "                if answer == option.lower():\n",
    "                    valid_answers.append(option)\n",
    "        \n",
    "        if not valid_answers:\n",
    "            if \"not mentioned\" in answer:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "            else:\n",
    "                valid_answers = [\"Not mentioned\"]\n",
    "        \n",
    "        return \", \".join(valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f02960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticDermatologyPipeline:\n",
    "    \"\"\"Main pipeline for agentic dermatology analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        if api_key is None:\n",
    "            load_dotenv()\n",
    "            api_key = os.getenv(\"API_KEY\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        \n",
    "        self.image_analyzer = ImageAnalysisService(self.client)\n",
    "        self.clinical_analyzer = ClinicalContextAnalyzer(self.client)\n",
    "        self.evidence_integrator = EvidenceIntegrator(self.client)\n",
    "        self.reasoning_engine = ReasoningEngine(self.client)\n",
    "        self.reflection_engine = SelfReflectionEngine(self.client)\n",
    "        self.reanalysis_engine = ReAnalysisEngine(self.client)\n",
    "    \n",
    "    def process_single_encounter(self, agentic_data, encounter_id):\n",
    "        \"\"\"\n",
    "        Process a single encounter with all its questions using the agentic pipeline.\n",
    "\n",
    "        Args:\n",
    "            agentic_data: AgenticRAGData instance containing all encounter data\n",
    "            encounter_id: The specific encounter ID to process\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with all questions processed with agentic reasoning for this encounter\n",
    "        \"\"\"\n",
    "        all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "        encounter_pairs = [pair for pair in all_pairs if pair[0] == encounter_id]\n",
    "\n",
    "        if not encounter_pairs:\n",
    "            print(f\"No data found for encounter {encounter_id}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Processing {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "\n",
    "        encounter_results = {encounter_id: {}}\n",
    "\n",
    "        # Extract image analysis once per encounter\n",
    "        print(f\"Computing image analysis for {encounter_id}\")\n",
    "        sample_data = agentic_data.get_combined_data(encounter_pairs[0][0], encounter_pairs[0][1])\n",
    "        image_analysis = self.image_analyzer.analyze_images(sample_data['images'], encounter_id)\n",
    "\n",
    "        # Extract clinical context once per encounter\n",
    "        print(f\"Extracting clinical context for {encounter_id}\")\n",
    "        clinical_context = self.clinical_analyzer.extract_clinical_context(\n",
    "            sample_data['query_context'], \n",
    "            encounter_id\n",
    "        )\n",
    "\n",
    "        for i, (encounter_id, base_qid) in enumerate(encounter_pairs):\n",
    "            print(f\"Processing question {i+1}/{len(encounter_pairs)}: {base_qid}\")\n",
    "\n",
    "            sample_data = agentic_data.get_combined_data(encounter_id, base_qid)\n",
    "            if not sample_data:\n",
    "                print(f\"Warning: No data found for {encounter_id}, {base_qid}\")\n",
    "                continue\n",
    "\n",
    "            # Extract question details\n",
    "            question_text = sample_data['query_context'].split(\"MAIN QUESTION TO ANSWER:\")[1].split(\"\\n\")[0].strip()\n",
    "            question_type = sample_data['question_type']\n",
    "            options = sample_data['options']\n",
    "            model_predictions = sample_data['model_predictions']\n",
    "\n",
    "            # Integrate evidence (reuse previously extracted clinical context)\n",
    "            print(f\"Integrating evidence for {encounter_id}, {base_qid}\")\n",
    "            integrated_evidence = self.evidence_integrator.integrate_evidence(\n",
    "                image_analysis,\n",
    "                clinical_context,\n",
    "                question_type\n",
    "            )\n",
    "\n",
    "            # Initial reasoning\n",
    "            print(f\"Initial reasoning for {encounter_id}, {base_qid}\")\n",
    "            reasoning_result = self.reasoning_engine.apply_initial_reasoning(\n",
    "                question_text,\n",
    "                question_type,\n",
    "                options,\n",
    "                integrated_evidence,\n",
    "                model_predictions\n",
    "            )\n",
    "\n",
    "            # Determine if self-reflection is needed based on confidence\n",
    "            confidence = reasoning_result.get('confidence', 0.0)\n",
    "            if isinstance(confidence, str):\n",
    "                try:\n",
    "                    confidence = float(confidence)\n",
    "                except:\n",
    "                    confidence = 0.0\n",
    "\n",
    "            final_result = reasoning_result\n",
    "            reflection_path = []\n",
    "\n",
    "            # Apply self-reflection if confidence is below threshold\n",
    "            if confidence < Config.CONFIDENCE_THRESHOLD:\n",
    "                print(f\"Confidence {confidence} below threshold. Applying self-reflection.\")\n",
    "\n",
    "                reflection_result = self.reflection_engine.apply_reflection(\n",
    "                    question_text,\n",
    "                    question_type,\n",
    "                    options,\n",
    "                    integrated_evidence,\n",
    "                    reasoning_result\n",
    "                )\n",
    "                reflection_path.append(reflection_result)\n",
    "\n",
    "                # Determine if re-analysis is needed based on reflection\n",
    "                requires_revision = reflection_result.get('requires_revision', False)\n",
    "                if requires_revision:\n",
    "                    print(f\"Reflection indicates revision needed. Performing deep analysis.\")\n",
    "\n",
    "                    deep_analysis = self.reanalysis_engine.deep_analysis(\n",
    "                        question_text,\n",
    "                        question_type,\n",
    "                        options,\n",
    "                        integrated_evidence,\n",
    "                        reasoning_result,\n",
    "                        reflection_result\n",
    "                    )\n",
    "                    reflection_path.append(deep_analysis)\n",
    "\n",
    "                    final_result = {\n",
    "                        \"reasoning\": deep_analysis.get('deep_reasoning', ''),\n",
    "                        \"answer\": deep_analysis.get('final_answer', 'Not mentioned'),\n",
    "                        \"validated_answer\": deep_analysis.get('validated_final_answer', 'Not mentioned'),\n",
    "                        \"confidence\": deep_analysis.get('final_confidence', 0.0)\n",
    "                    }\n",
    "                else:\n",
    "                    # Use original answer but with updated confidence if available\n",
    "                    revised_confidence = reflection_result.get('revised_confidence', reasoning_result.get('confidence', 0.0))\n",
    "                    final_result = {\n",
    "                        \"reasoning\": reasoning_result.get('reasoning', ''),\n",
    "                        \"answer\": reasoning_result.get('answer', 'Not mentioned'),\n",
    "                        \"validated_answer\": reasoning_result.get('validated_answer', 'Not mentioned'),\n",
    "                        \"confidence\": revised_confidence\n",
    "                    }\n",
    "\n",
    "            encounter_results[encounter_id][base_qid] = {\n",
    "                \"query_context\": sample_data['query_context'],\n",
    "                \"options\": sample_data['options'],\n",
    "                \"model_predictions\": sample_data['model_predictions'],\n",
    "                \"integrated_evidence\": integrated_evidence,\n",
    "                \"reasoning_result\": reasoning_result,\n",
    "                \"reflection_path\": reflection_path,\n",
    "                \"final_result\": final_result,\n",
    "                \"final_answer\": final_result.get('validated_answer', 'Not mentioned')\n",
    "            }\n",
    "\n",
    "        output_file = os.path.join(Config.OUTPUT_DIR, f\"agentic_reasoning_results_{encounter_id}.json\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(encounter_results, f, indent=2)\n",
    "\n",
    "        print(f\"Processed all {len(encounter_pairs)} questions for encounter {encounter_id}\")\n",
    "        return encounter_results\n",
    "    \n",
    "    def format_results_for_evaluation(self, encounter_results, output_file):\n",
    "        \"\"\"Format results for official evaluation.\"\"\"\n",
    "        QIDS = [\n",
    "            \"CQID010-001\",\n",
    "            \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",\n",
    "            \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",\n",
    "            \"CQID015-001\",\n",
    "            \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "            \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",\n",
    "            \"CQID025-001\",\n",
    "            \"CQID034-001\",\n",
    "            \"CQID035-001\",\n",
    "            \"CQID036-001\",\n",
    "        ]\n",
    "        \n",
    "        qid_variants = {}\n",
    "        for qid in QIDS:\n",
    "            base_qid, variant = qid.split('-')\n",
    "            if base_qid not in qid_variants:\n",
    "                qid_variants[base_qid] = []\n",
    "            qid_variants[base_qid].append(qid)\n",
    "        \n",
    "        required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "        \n",
    "        formatted_predictions = []\n",
    "        for encounter_id, questions in encounter_results.items():\n",
    "            encounter_base_qids = set(questions.keys())\n",
    "            if not required_base_qids.issubset(encounter_base_qids):\n",
    "                print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "                continue\n",
    "            \n",
    "            pred_entry = {'encounter_id': encounter_id}\n",
    "            \n",
    "            for base_qid, question_data in questions.items():\n",
    "                if base_qid not in qid_variants:\n",
    "                    continue\n",
    "                \n",
    "                final_answer = question_data['final_answer']\n",
    "                options = question_data['options']\n",
    "                \n",
    "                not_mentioned_index = self._find_not_mentioned_index(options)\n",
    "                \n",
    "                self._process_answers(\n",
    "                    pred_entry, \n",
    "                    base_qid, \n",
    "                    final_answer, \n",
    "                    options, \n",
    "                    qid_variants, \n",
    "                    not_mentioned_index\n",
    "                )\n",
    "            \n",
    "            formatted_predictions.append(pred_entry)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(formatted_predictions, f, indent=2)\n",
    "        \n",
    "        print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "        return formatted_predictions\n",
    "    \n",
    "    def _find_not_mentioned_index(self, options):\n",
    "        \"\"\"Find the index of 'Not mentioned' in options.\"\"\"\n",
    "        for i, opt in enumerate(options):\n",
    "            if opt.lower() == \"not mentioned\":\n",
    "                return i\n",
    "        return len(options) - 1\n",
    "    \n",
    "    def _process_answers(self, pred_entry, base_qid, final_answer, options, qid_variants, not_mentioned_index):\n",
    "        \"\"\"Process answers and add to prediction entry.\"\"\"\n",
    "        if ',' in final_answer:\n",
    "            answer_parts = [part.strip() for part in final_answer.split(',')]\n",
    "            answer_indices = []\n",
    "            \n",
    "            for part in answer_parts:\n",
    "                found = False\n",
    "                for i, opt in enumerate(options):\n",
    "                    if part.lower() == opt.lower():\n",
    "                        answer_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found:\n",
    "                    answer_indices.append(not_mentioned_index)\n",
    "            \n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            for i, idx in enumerate(answer_indices):\n",
    "                if i < len(available_variants):\n",
    "                    pred_entry[available_variants[i]] = idx\n",
    "            \n",
    "            for i in range(len(answer_indices), len(available_variants)):\n",
    "                pred_entry[available_variants[i]] = not_mentioned_index\n",
    "            \n",
    "        else:\n",
    "            answer_index = not_mentioned_index\n",
    "            \n",
    "            for i, opt in enumerate(options):\n",
    "                if final_answer.lower() == opt.lower():\n",
    "                    answer_index = i\n",
    "                    break\n",
    "            \n",
    "            pred_entry[qid_variants[base_qid][0]] = answer_index\n",
    "            \n",
    "            if len(qid_variants[base_qid]) > 1:\n",
    "                for i in range(1, len(qid_variants[base_qid])):\n",
    "                    pred_entry[qid_variants[base_qid][i]] = not_mentioned_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76103fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_all_encounters_pipeline():\n",
    "#     \"\"\"Run the agentic pipeline for all available encounters.\"\"\"\n",
    "#     # Load model predictions and validation dataset\n",
    "#     model_predictions_dict = DataLoader.load_all_model_predictions(Config.MODEL_PREDICTIONS_DIR)\n",
    "#     all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "#     validation_df = DataLoader.load_validation_dataset(Config.VAL_DATASET_PATH)\n",
    "    \n",
    "#     # Create agentic data and pipeline\n",
    "#     agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "#     pipeline = AgenticDermatologyPipeline()\n",
    "    \n",
    "#     # Get all unique encounter IDs\n",
    "#     all_pairs = agentic_data.get_all_encounter_question_pairs()\n",
    "#     unique_encounter_ids = sorted(list(set(pair[0] for pair in all_pairs)))\n",
    "#     print(f\"Found {len(unique_encounter_ids)} unique encounters to process\")\n",
    "    \n",
    "#     # Process each encounter\n",
    "#     all_encounter_results = {}\n",
    "#     for i, encounter_id in enumerate(unique_encounter_ids):\n",
    "#         print(f\"Processing encounter {i+1}/{len(unique_encounter_ids)}: {encounter_id}...\")\n",
    "#         encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "#         if encounter_results:\n",
    "#             all_encounter_results.update(encounter_results)\n",
    "        \n",
    "#         # Save intermediate results periodically\n",
    "#         if (i+1) % 5 == 0 or (i+1) == len(unique_encounter_ids):\n",
    "#             timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#             intermediate_output_file = os.path.join(\n",
    "#                 Config.OUTPUT_DIR, \n",
    "#                 f\"intermediate_agentic_results_{i+1}_of_{len(unique_encounter_ids)}_{timestamp}.json\"\n",
    "#             )\n",
    "#             with open(intermediate_output_file, 'w') as f:\n",
    "#                 json.dump(all_encounter_results, f, indent=2)\n",
    "#             print(f\"Saved intermediate results after processing {i+1} encounters\")\n",
    "    \n",
    "#     # Format and save final predictions\n",
    "#     timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#     output_file = os.path.join(\n",
    "#         Config.OUTPUT_DIR, \n",
    "#         f\"data_cvqa_sys_agentic_all_{timestamp}.json\"\n",
    "#     )\n",
    "    \n",
    "#     formatted_predictions = pipeline.format_results_for_evaluation(all_encounter_results, output_file)\n",
    "    \n",
    "#     print(f\"Processed {len(formatted_predictions)} encounters successfully\")\n",
    "#     return formatted_predictions\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     formatted_predictions = run_all_encounters_pipeline()\n",
    "#     print(f\"Total complete encounters processed: {len(formatted_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7144aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_encounter_pipeline(encounter_id):\n",
    "    \"\"\"Run the agentic pipeline for a single encounter.\"\"\"\n",
    "    # Load model predictions and validation dataset\n",
    "    model_predictions_dict = DataLoader.load_all_model_predictions(Config.MODEL_PREDICTIONS_DIR)\n",
    "    all_models_df = pd.concat(model_predictions_dict.values(), ignore_index=True)\n",
    "    validation_df = DataLoader.load_validation_dataset(Config.VAL_DATASET_PATH)\n",
    "    \n",
    "    # Create agentic data and pipeline\n",
    "    agentic_data = AgenticRAGData(all_models_df, validation_df)\n",
    "    pipeline = AgenticDermatologyPipeline()\n",
    "    \n",
    "    # Process the encounter\n",
    "    encounter_results = pipeline.process_single_encounter(agentic_data, encounter_id)\n",
    "    \n",
    "    # Format and save predictions\n",
    "    output_file = os.path.join(\n",
    "        Config.OUTPUT_DIR, \n",
    "        f\"data_cvqa_sys_agentic_{encounter_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    )\n",
    "    formatted_predictions = pipeline.format_results_for_evaluation(encounter_results, output_file)\n",
    "    \n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daddcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    encounter_id = \"ENC00854\"\n",
    "    formatted_predictions = run_single_encounter_pipeline(encounter_id)\n",
    "    print(f\"Processed encounter {encounter_id} with {len(formatted_predictions)} prediction entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c77fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
