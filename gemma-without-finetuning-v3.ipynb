{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "846bb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical Image Inference with Baseline Model\n",
    "\n",
    "## Installations and Setup\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34860b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbf44ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE: None\n",
      "TRANSFORMERS_CACHE: None\n",
      "HF_HOME: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\n",
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/bin/python\n",
      "Python version: 3.10.10 (main, Apr 15 2024, 11:52:16) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
      "Python version info: sys.version_info(major=3, minor=10, micro=10, releaselevel='final', serial=0)\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "os.environ.pop(\"TRANSFORMERS_CACHE\", None)\n",
    "os.environ[\"HF_HOME\"] = \"/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\"\n",
    "\n",
    "# Print environment info\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(sys.executable)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eefc3922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>answer_index</th>\n",
       "      <th>question_en</th>\n",
       "      <th>options_en</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>responses</th>\n",
       "      <th>query_title_en</th>\n",
       "      <th>query_content_en</th>\n",
       "      <th>image_paths</th>\n",
       "      <th>responses_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U04473</td>\n",
       "      <td>[IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]</td>\n",
       "      <td>[{'author_id': 'U00217', 'content_zh': '银屑病，似与...</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Psoriasis seems to have no relation to pleura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00002</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U06063</td>\n",
       "      <td>[IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...</td>\n",
       "      <td>[{'author_id': 'U11305', 'content_zh': '脚气', '...</td>\n",
       "      <td>What is on the bottom of the right foot?</td>\n",
       "      <td>The patient is a 50-year-old male, who has bee...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Beriberi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00003</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U00780</td>\n",
       "      <td>[IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...</td>\n",
       "      <td>[{'author_id': 'U01131', 'content_zh': '瘙痒症，有无...</td>\n",
       "      <td>Interpreting Images - Is it magical skin?</td>\n",
       "      <td>Male, 65 years old, skin lesions as shown in t...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Pruritus, is there any other special medical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00004</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>2</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>widespread</td>\n",
       "      <td>U00209</td>\n",
       "      <td>[IMG_ENC00004_00001.jpg, IMG_ENC00004_00002.jpg]</td>\n",
       "      <td>[{'author_id': 'U06715', 'content_zh': '肢端角化病？...</td>\n",
       "      <td>Skin Disease</td>\n",
       "      <td>Male, 15 years old, keratosis on both palms, s...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Acrokeratosis?, Progressive Symmetrical Eryth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00005</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U09050</td>\n",
       "      <td>[IMG_ENC00005_00001.jpg]</td>\n",
       "      <td>[{'author_id': 'U09402', 'content_zh': '是否神经性皮...</td>\n",
       "      <td>Perifollicular atrophy?</td>\n",
       "      <td>Young female, silver-gray dot-like atrophy spo...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Is it neurodermatitis?, Impotence?, Lichen Sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id          qid  answer_index                        question_en  \\\n",
       "0     ENC00001  CQID010-001             1  How much of the body is affected?   \n",
       "1     ENC00002  CQID010-001             1  How much of the body is affected?   \n",
       "2     ENC00003  CQID010-001             1  How much of the body is affected?   \n",
       "3     ENC00004  CQID010-001             2  How much of the body is affected?   \n",
       "4     ENC00005  CQID010-001             1  How much of the body is affected?   \n",
       "\n",
       "                                          options_en question_type_en  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "1  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "2  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "3  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "4  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "\n",
       "  question_category_en   answer_text author_id  \\\n",
       "0              General  limited area    U04473   \n",
       "1              General  limited area    U06063   \n",
       "2              General  limited area    U00780   \n",
       "3              General    widespread    U00209   \n",
       "4              General  limited area    U09050   \n",
       "\n",
       "                                           image_ids  \\\n",
       "0   [IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]   \n",
       "1  [IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...   \n",
       "2  [IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...   \n",
       "3   [IMG_ENC00004_00001.jpg, IMG_ENC00004_00002.jpg]   \n",
       "4                           [IMG_ENC00005_00001.jpg]   \n",
       "\n",
       "                                           responses  \\\n",
       "0  [{'author_id': 'U00217', 'content_zh': '银屑病，似与...   \n",
       "1  [{'author_id': 'U11305', 'content_zh': '脚气', '...   \n",
       "2  [{'author_id': 'U01131', 'content_zh': '瘙痒症，有无...   \n",
       "3  [{'author_id': 'U06715', 'content_zh': '肢端角化病？...   \n",
       "4  [{'author_id': 'U09402', 'content_zh': '是否神经性皮...   \n",
       "\n",
       "                              query_title_en  \\\n",
       "0       Pleural effusion accompanied by rash   \n",
       "1   What is on the bottom of the right foot?   \n",
       "2  Interpreting Images - Is it magical skin?   \n",
       "3                               Skin Disease   \n",
       "4                    Perifollicular atrophy?   \n",
       "\n",
       "                                    query_content_en  \\\n",
       "0  A patient with pleural effusion is accompanied...   \n",
       "1  The patient is a 50-year-old male, who has bee...   \n",
       "2  Male, 65 years old, skin lesions as shown in t...   \n",
       "3  Male, 15 years old, keratosis on both palms, s...   \n",
       "4  Young female, silver-gray dot-like atrophy spo...   \n",
       "\n",
       "                                         image_paths  \\\n",
       "0  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "1  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "2  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "3  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "4  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "\n",
       "                                        responses_en  \n",
       "0  [Psoriasis seems to have no relation to pleura...  \n",
       "1                                         [Beriberi]  \n",
       "2  [Pruritus, is there any other special medical ...  \n",
       "3  [Acrokeratosis?, Progressive Symmetrical Eryth...  \n",
       "4  [Is it neurodermatitis?, Impotence?, Lichen Sc...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_file = os.path.join(\"2025_dataset\", \"train\", \"final_df_2.csv\")\n",
    "train_images_dir = os.path.join(\"2025_dataset\", \"train\", \"images_train\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "\n",
    "train_df['image_ids'] = train_df['image_ids'].apply(eval)\n",
    "train_df['responses_en'] = train_df['responses_en'].apply(eval)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74e9342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 samples for training\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25f6ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe shape: (10, 8)\n",
      "Columns: ['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids', 'question_type_en', 'question_category_en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sample row:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>question_en</th>\n",
       "      <th>options_en</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00002</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00003</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id          qid                        question_en  \\\n",
       "0     ENC00001  CQID010-001  How much of the body is affected?   \n",
       "1     ENC00002  CQID010-001  How much of the body is affected?   \n",
       "2     ENC00003  CQID010-001  How much of the body is affected?   \n",
       "\n",
       "                                          options_en   answer_text  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "1  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "2  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "\n",
       "                                           image_ids question_type_en  \\\n",
       "0   [IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]             Site   \n",
       "1  [IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...             Site   \n",
       "2  [IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...             Site   \n",
       "\n",
       "  question_category_en  \n",
       "0              General  \n",
       "1              General  \n",
       "2              General  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = train_df[['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids', 'question_type_en', 'question_category_en']]\n",
    "\n",
    "print(f\"Filtered dataframe shape: {train_df.shape}\")\n",
    "print(\"Columns:\", train_df.columns.tolist())\n",
    "\n",
    "display(\"Sample row:\", train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0a3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Only take the first image from the list\n",
    "            if not row['image_ids'] or len(row['image_ids']) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get just the first image path\n",
    "            image_path = os.path.join(train_images_dir, row['image_ids'][0])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Format options text\n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(eval(row['options_en']))])\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = f\"Type: {row.get('question_type_en', '')}, Category: {row.get('question_category_en', '')}\"\n",
    "            \n",
    "            # Create the full query text with instructions\n",
    "            query_text = f\"Question: Based on the image, {row['question_en']}\\nQuestion Metadata: {metadata}\\nOptions: {options_text}\"\n",
    "#             query_text += \"\\n\\nCRITICAL INSTRUCTION: Only respond with an option if it is **clearly and unambiguously** supported by the image. If the image is unclear, incomplete, or could fit multiple answers, respond with: 'Not mentioned'. You must respond with the **exact text** of one option below. No numbers, no explanation. Given the medical context, err on the side of caution.\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"encounter_id\": row['encounter_id'],\n",
    "                \"qid\": row['qid'],\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"answer_text\": row['answer_text'],\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f3aea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Only take the first image from the list\n",
    "            if not row['image_ids'] or len(row['image_ids']) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get just the first image path\n",
    "            image_path = os.path.join(train_images_dir, row['image_ids'][0])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Format options text\n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(eval(row['options_en']))])\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = f\"Type: {row.get('question_type_en', '')}, Category: {row.get('question_category_en', '')}\"\n",
    "            \n",
    "            # Create the full query text with instructions\n",
    "            query_text = f\"Question: Based on the image, {row['question_en']}\\nQuestion Metadata: {metadata}\\nOptions: {options_text}\"\n",
    "#             query_text += \"\\n\\nCRITICAL INSTRUCTION: Only respond with an option if it is **clearly and unambiguously** supported by the image. If the image is unclear, incomplete, or could fit multiple answers, respond with: 'Not mentioned'. You must respond with the **exact text** of one option below. No numbers, no explanation. Given the medical context, err on the side of caution.\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"encounter_id\": row['encounter_id'],\n",
    "                \"qid\": row['qid'],\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"answer_text\": row['answer_text'],\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9fb3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Open just one image, convert to RGB\n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        # Use consistent system message\n",
    "        system_message = \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"processed_data\"):\n",
    "    total_processed = 0\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955fc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"processed_data_debug\"\n",
    "\n",
    "if os.path.exists(processed_data_dir):\n",
    "    shutil.rmtree(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed69f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8205a9d20cac46a089315f95622a8bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 examples so far\n",
      "Total processed examples: 10\n"
     ]
    }
   ],
   "source": [
    "total_examples = preprocess_dataset(train_df, batch_size=500, save_dir=processed_data_dir)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb34e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3ae0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b520a711697040f2853a966e148d98be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8f150ca09c41e99fc76bc6701f1e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default chat template: {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "Special tokens map: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16. Use a different GPU.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs, token=hf_token)\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "print(f\"Default chat template: {processor.tokenizer.chat_template}\")\n",
    "print(f\"Special tokens map: {processor.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "829d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_analysis(image_path, question, options, question_type=\"\", question_category=\"\", model=model, processor=processor):\n",
    "    \"\"\"\n",
    "    Generate medical image analysis based on a question and image using the base model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the medical image\n",
    "        question: The medical question to analyze\n",
    "        options: List of possible options to choose from\n",
    "        question_type: Optional metadata about question type\n",
    "        question_category: Optional metadata about question category\n",
    "        model: The model to use for inference\n",
    "        processor: The processor to use for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        The model's response (selected option)\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    else:\n",
    "        # Already a PIL Image\n",
    "        image = image_path.convert(\"RGB\") if hasattr(image_path, 'convert') else image_path\n",
    "    \n",
    "    # Format options text\n",
    "    options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(options)])\n",
    "    \n",
    "    # Create metadata string\n",
    "    metadata = f\"Type: {question_type}, Category: {question_category}\"\n",
    "    \n",
    "    # Create the query text\n",
    "    query_text = f\"Question: Based on the image, {question}\\nQuestion Metadata: {metadata}\\nOptions: {options_text}\"\n",
    "    \n",
    "    # System message\n",
    "    system_message = \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"\n",
    "    \n",
    "    # Convert to messages format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": query_text},\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "        ]},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Extract image from messages - needs to be in correct format for processor\n",
    "    image_inputs = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            for content in msg[\"content\"]:\n",
    "                if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                    image_inputs.append(content[\"image\"])\n",
    "                    break\n",
    "    \n",
    "    # Tokenize the text and process the images\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image_inputs],  # Nested list as processor expects\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Move the inputs to the device - IMPORTANT: preserve the object structure\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    stop_token_ids = [processor.tokenizer.eos_token_id, processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "    generated_ids = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=64,  # Shorter is fine for option selection\n",
    "        top_p=0.9,\n",
    "        do_sample=True, \n",
    "        temperature=0.5,  # Lower temp for more precise answers\n",
    "        eos_token_id=stop_token_ids,\n",
    "        disable_compile=True\n",
    "    )\n",
    "    \n",
    "    # Trim the generation and decode the output to text\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99505127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_cases(csv_file, images_dir, num_samples=10):\n",
    "    \"\"\"\n",
    "    Load test cases from the CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert string representations of lists to actual lists\n",
    "    df['image_ids'] = df['image_ids'].apply(eval)\n",
    "    \n",
    "    # Only take a subset for testing\n",
    "    test_df = df.head(num_samples)\n",
    "    \n",
    "    test_cases = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        if not row['image_ids'] or len(row['image_ids']) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the first image path\n",
    "        image_path = os.path.join(images_dir, row['image_ids'][0])\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image not found at {image_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Format options\n",
    "        options = eval(row['options_en'])\n",
    "        \n",
    "        test_cases.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"question\": row['question_en'],\n",
    "            \"options\": options,\n",
    "            \"question_type\": row.get('question_type_en', ''),\n",
    "            \"question_category\": row.get('question_category_en', ''),\n",
    "            \"expected_answer\": row['answer_text']\n",
    "        })\n",
    "    \n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "619a961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Test ===\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: widespread\n",
      "\n",
      "=== Dataset Tests ===\n",
      "\n",
      "Test Case 1:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: widespread\n",
      "Expected answer: limited area\n",
      "✗ Incorrect\n",
      "\n",
      "Test Case 2:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: limited area\n",
      "Expected answer: limited area\n",
      "✓ Correct\n",
      "\n",
      "Test Case 3:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: limited area\n",
      "Expected answer: limited area\n",
      "✓ Correct\n",
      "\n",
      "Test Case 4:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: limited area\n",
      "Expected answer: widespread\n",
      "✗ Incorrect\n",
      "\n",
      "Test Case 5:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: limited area\n",
      "Expected answer: limited area\n",
      "✓ Correct\n",
      "\n",
      "Accuracy: 3/5 = 60.00%\n",
      "\n",
      "Inference completed and GPU memory released\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with a sample image\n",
    "    print(\"\\n=== Sample Test ===\")\n",
    "    image_path = \"2025_dataset/train/images_train/IMG_ENC00001_00001.jpg\"\n",
    "    question = \"How much of the body is affected?\"\n",
    "    options = [\"single spot\", \"limited area\", \"widespread\", \"Not mentioned\"]\n",
    "    question_type = \"Site\"\n",
    "    question_category = \"General\"\n",
    "    \n",
    "    result = generate_medical_analysis(\n",
    "        image_path=image_path,\n",
    "        question=question,\n",
    "        options=options,\n",
    "        question_type=question_type,\n",
    "        question_category=question_category\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Options: {options}\")\n",
    "    print(f\"Model's answer: {result}\")\n",
    "    \n",
    "    # Test with multiple examples from the dataset\n",
    "    print(\"\\n=== Dataset Tests ===\")\n",
    "    csv_file = os.path.join(\"2025_dataset\", \"train\", \"final_df_2.csv\")\n",
    "    images_dir = os.path.join(\"2025_dataset\", \"train\", \"images_train\")\n",
    "    \n",
    "    try:\n",
    "        test_cases = load_test_cases(csv_file, images_dir, num_samples=5)\n",
    "        \n",
    "        correct = 0\n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"\\nTest Case {i+1}:\")\n",
    "            result = generate_medical_analysis(\n",
    "                image_path=test_case[\"image_path\"],\n",
    "                question=test_case[\"question\"],\n",
    "                options=test_case[\"options\"],\n",
    "                question_type=test_case[\"question_type\"],\n",
    "                question_category=test_case[\"question_category\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"Question: {test_case['question']}\")\n",
    "            print(f\"Options: {test_case['options']}\")\n",
    "            print(f\"Model's answer: {result}\")\n",
    "            print(f\"Expected answer: {test_case['expected_answer']}\")\n",
    "            \n",
    "            if result.lower() == test_case['expected_answer'].lower():\n",
    "                correct += 1\n",
    "                print(\"✓ Correct\")\n",
    "            else:\n",
    "                print(\"✗ Incorrect\")\n",
    "        \n",
    "        if test_cases:\n",
    "            print(f\"\\nAccuracy: {correct}/{len(test_cases)} = {correct/len(test_cases):.2%}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test cases: {e}\")\n",
    "        \n",
    "        # Fall back to hardcoded test cases if CSV loading fails\n",
    "        print(\"\\n=== Fallback Test Cases ===\")\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"image_path\": \"2025_dataset/train/images_train/IMG_ENC00002_00001.jpg\",\n",
    "                \"question\": \"How much of the body is affected?\",\n",
    "                \"options\": [\"single spot\", \"limited area\", \"widespread\", \"Not mentioned\"],\n",
    "                \"question_type\": \"Site\",\n",
    "                \"question_category\": \"General\"\n",
    "            },\n",
    "            {\n",
    "                \"image_path\": \"2025_dataset/train/images_train/IMG_ENC00003_00001.jpg\",\n",
    "                \"question\": \"How much of the body is affected?\",\n",
    "                \"options\": [\"single spot\", \"limited area\", \"widespread\", \"Not mentioned\"],\n",
    "                \"question_type\": \"Site\", \n",
    "                \"question_category\": \"General\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            if os.path.exists(test_case[\"image_path\"]):\n",
    "                print(f\"\\nTest Case {i+1}:\")\n",
    "                result = generate_medical_analysis(**test_case)\n",
    "                print(f\"Question: {test_case['question']}\")\n",
    "                print(f\"Options: {test_case['options']}\")\n",
    "                print(f\"Model's answer: {result}\")\n",
    "            else:\n",
    "                print(f\"Image not found: {test_case['image_path']}\")\n",
    "        \n",
    "    # Free up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nInference completed and GPU memory released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206469bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
