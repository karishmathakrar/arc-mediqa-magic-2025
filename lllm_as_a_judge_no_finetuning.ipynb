{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c57acb-4318-4ea1-9bfb-bd919dd42413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import types\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf08982-0842-495d-b199-9a6df27b3a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load environment variables and configure Gemini\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "genai.configure(api_key=api_key)\n",
    "base_model = [\n",
    "    m for m in genai.list_models()\n",
    "    if \"createTunedModel\" in m.supported_generation_methods and\n",
    "    \"flash\" in m.name][0]\n",
    "base_model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5aeeb-77b2-4f90-a9d2-8fef94f96c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the dataset columns to verify its structure.\n",
    "json_file = os.path.join(\"2024_dataset\", \"train_downloaded.json\")\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "len(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5382ad97-b078-4702-b46e-6276f09f8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for values in data:\n",
    "\n",
    "    temp = defaultdict(str)\n",
    "    temp['text_input'] = values['query_content_en'] if values['query_content_en'] != '[deleted]' and values['query_content_en'] else values['query_title_en']\n",
    "    temp['output'] = values['responses'][0]['content_en']\n",
    "    dataset.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b17e7b-a19a-4d7c-91e9-b681ee819375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Score: 0.0\\n\\nChain of thought: The generated response and the baseline response are diametrically opposed.  They express completely contradictory opinions.  There is no overlap or similarity between \\\"This sounds like eczema\\\" and \\\"This does not sound like eczema.\\\"  Therefore, the similarity score must be 0.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.04349698081161037\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 109,\n",
       "        \"candidates_token_count\": 66,\n",
       "        \"total_token_count\": 175\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question based on the context\n",
    "\n",
    "Context: I will give you two sentences. The first sentence will be a generated response. The second sentence is the baseline response. \n",
    "I want you to judge how close these sentences are to eachother. Return a continuous value between [0,1] inclusive where 0 is a fully incorrect response and 1 is a fully correct response. Also provide chain of thought.\n",
    "\n",
    "Generated Response: This sounds like eczema to me.\n",
    "Baseline Response: This does not sound like eczema to me.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, generation_config={'temperature': 0})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8edf75e-27ec-4c37-bd02-15750941005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Similarity score: 0.8\\n\\nChain of thought:\\n\\nBoth sentences express the same opinion (\\\"This sounds/looks like eczema to me\\\").  The generated response is grammatically correct, while the baseline response omits the word \\\"sounds,\\\" making it slightly less formal but still conveying the same meaning. The difference is minor, primarily in grammatical correctness and word choice (\\\"sounds\\\" vs. \\\"looks\\\").  The core meaning remains identical.  Therefore, a score of 0.8 reflects the high degree of semantic similarity despite the minor grammatical difference.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.13209404816498627\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 106,\n",
       "        \"candidates_token_count\": 111,\n",
       "        \"total_token_count\": 217\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question based on the context\n",
    "\n",
    "Context: I will give you two sentences. The first sentence will be a generated response. The second sentence is the baseline response. \n",
    "I want you to judge how close these sentences are to eachother. Return a continuous value between [0,1] inclusive where 0 is a fully incorrect response and 1 is a fully correct response. Also provide chain of thought.\n",
    "\n",
    "Generated Response: This sounds like eczema to me.\n",
    "Baseline Response: This like eczema to me.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, generation_config={'temperature': 0})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49d6c692-d354-4e8b-95a4-c2cff4f76665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Similarity score: 0.8\\n\\nChain of thought:\\n\\nBoth sentences express the same opinion: that the described symptoms sound like eczema.  The generated response is grammatically correct (\\\"This sounds like eczema to me\\\"), while the baseline response omits the \\\"s\\\" in \\\"sounds,\\\" making it slightly less grammatically correct (\\\"This like eczema to me\\\"). However, the core meaning and intent remain identical. The grammatical error in the baseline response is minor and doesn't significantly alter the meaning.  Therefore, a high similarity score is appropriate.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.1002615668556907\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 120,\n",
       "        \"candidates_token_count\": 110,\n",
       "        \"total_token_count\": 230\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question based on the context\n",
    "\n",
    "Context: I will give you two sentences. The first sentence will be a generated response. The second sentence is the baseline response. \n",
    "I want you to judge how contextually similar these sentences are to eachother. Return a continuous value between [0,1] inclusive where 0 is a fully incorrect response and 1 is a fully correct response. Also provide chain of thought. Use only context to judge how similar these two sentences are.\n",
    "\n",
    "Generated Response: This sounds like eczema to me.\n",
    "Baseline Response: This like eczema to me.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, generation_config={'temperature': 0})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4ce8590-f715-415f-bd01-30e5ed8f1a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Contextual Similarity Score: 0.6\\n\\nChain of thought:\\n\\nBoth sentences offer a potential diagnosis based on limited information (presumably about a skin condition).  The generated response is more specific (\\\"eczema\\\"), while the baseline response is more cautious and general (\\\"dry skin\\\").  However, both point towards a skin-related issue.  Eczema is a condition often associated with dry skin, so there's a degree of overlap.  The difference in certainty (definitive vs. tentative) reduces the similarity score from a perfect 1.  A score of 0.6 reflects the partial overlap in meaning while acknowledging the difference in specificity and confidence.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.13164835329409\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 137,\n",
       "        \"candidates_token_count\": 135,\n",
       "        \"total_token_count\": 272\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question based on the context\n",
    "\n",
    "Context: I will give you two sentences. The first sentence will be a generated response. The second sentence is the baseline response. \n",
    "I want you to judge how contextually similar these sentences are to eachother. Return a continuous value between [0,1] inclusive where 0 is a fully incorrect response and 1 is a fully correct response. Also provide chain of thought. Use only context to judge how similar these two sentences are.\n",
    "\n",
    "Generated Response: This sounds like eczema to me.\n",
    "Baseline Response: Not 100% certain on the prognosis, but this seems like it would be related to dry skin.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, generation_config={'temperature': 0})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd2d8f2-abff-41d8-9d24-4e4e15af00ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The generated response and baseline response don't directly match.  While eczema is a condition related to dry skin, they aren't synonymous.  The generated response offers a specific diagnosis, while the baseline response suggests a possible underlying cause.  Therefore, the similarity score is low.\\n\\nSimilarity score: **0.3**\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.09079547426593837\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 101,\n",
       "        \"candidates_token_count\": 67,\n",
       "        \"total_token_count\": 168\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question based on the context\n",
    "\n",
    "Context: I will give you two sentences. The first sentence will be a generated response. The second sentence is the baseline response. \n",
    "Does the diagnosis in the generated response match the baseline response? Please provide a similarity score between [0,1] inclusive.\n",
    "\n",
    "Generated Response: This sounds like eczema to me.\n",
    "Baseline Response: Not 100% certain on the prognosis, but this seems like it would be related to dry skin.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, generation_config={'temperature': 0})\n",
    "response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
