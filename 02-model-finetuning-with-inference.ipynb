{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da607113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12fab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.10 (main, Apr 15 2024, 11:52:16) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA A100 80GB PCIe\n",
      "HF_HOME: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\n"
     ]
    }
   ],
   "source": [
    "# Display environment information\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set up environment variables and cache directories\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "print(f\"HF_HOME: {os.getenv('HF_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3271e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"2025_dataset\")\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"valid\")\n",
    "VAL_IMAGES_DIR = os.path.join(VAL_DIR, \"images_valid\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0747824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = os.path.join(OUTPUT_DIR, \"multi_label_dataset.csv\")\n",
    "print(train_csv_file)\n",
    "train_images_dir = os.path.join(TRAIN_DIR, \"images_train\")\n",
    "print(train_images_dir)\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3123d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_options(options_str):\n",
    "    \"\"\"\n",
    "    Safely convert a string representation of a list to an actual list.\n",
    "    \"\"\"\n",
    "    if not isinstance(options_str, str):\n",
    "        return options_str\n",
    "        \n",
    "    try:\n",
    "        # Use ast.literal_eval which is safer than eval()\n",
    "        import ast\n",
    "        return ast.literal_eval(options_str)\n",
    "    except (SyntaxError, ValueError):\n",
    "        # Try common formats\n",
    "        if options_str.startswith('[') and options_str.endswith(']'):\n",
    "            # Strip brackets and split by commas\n",
    "            return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "        elif ',' in options_str:\n",
    "            # Just split by commas\n",
    "            return [opt.strip() for opt in options_str.split(',')]\n",
    "        else:\n",
    "            # Single option\n",
    "            return [options_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples and save them as a pickle file.\n",
    "    Includes query title and content as clinical context.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get image path - using image_id instead of image_ids\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            # Use the full image path if it's already in the dataframe\n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                # Otherwise construct from images_dir and image_id\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} â€” {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get options from options_en\n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "#                 options = row['options_en']\n",
    "#                 if isinstance(options, str):\n",
    "#                     try:\n",
    "#                         options = eval(options)\n",
    "#                     except:\n",
    "#                         options = options.split(',')\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                \n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(options)])\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            # Get question text\n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Get clinical context from query title and content\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "            \n",
    "            # Create the clinical context section\n",
    "            clinical_context = \"\"\n",
    "            if query_title:\n",
    "                clinical_context += f\"Clinical Context: {query_title}\\n\"\n",
    "            if query_content:\n",
    "                clinical_context += f\"{query_content}\\n\"\n",
    "            \n",
    "            # Create the full query text with clinical context\n",
    "            query_text = (f\"Question: Based on the image, {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Options: {options_text}\")\n",
    "            \n",
    "            # Get answer text - from valid_answers\n",
    "            if 'valid_answers' in row and row['valid_answers']:\n",
    "                # For multi-label, join all valid answers\n",
    "                answers = row['valid_answers']\n",
    "                if isinstance(answers, list):\n",
    "                    if len(answers) > 1:\n",
    "                        # Join multiple answers with commas\n",
    "                        answer_text = \", \".join(answers)\n",
    "                    elif len(answers) == 1:\n",
    "                        answer_text = answers[0]\n",
    "                    else:\n",
    "                        answer_text = \"Not mentioned\"\n",
    "                else:\n",
    "                    answer_text = str(answers)\n",
    "            elif 'multi_label' in row:\n",
    "                answer_text = row['multi_label']\n",
    "            else:\n",
    "                answer_text = \"Not mentioned\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"answer_text\": answer_text,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"outputsprocessed_data\", images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the entire dataset in batches\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Use train_images_dir global variable if images_dir is not provided\n",
    "    if images_dir is None:\n",
    "        images_dir = train_images_dir\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ea4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save the dataset\n",
    "processed_data_dir = \"outputs/processed_data\"\n",
    "\n",
    "# Clear any existing processed data (optional)\n",
    "if os.path.exists(processed_data_dir):\n",
    "    shutil.rmtree(processed_data_dir)\n",
    "    \n",
    "# Process the dataset\n",
    "total_examples = preprocess_dataset(train_df, batch_size=100, save_dir=processed_data_dir)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9268c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the processed data\n",
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[0]\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "print(\"\\nSample of processed data (second example):\")\n",
    "sample_data = batch_data[1]\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_llm_input(processed_data_dir, num_samples=3):\n",
    "    \"\"\"\n",
    "    Load and display what the LLM receives during training, including\n",
    "    the images, query text with clinical context, and expected answers.\n",
    "    \"\"\"\n",
    "    # Load the first batch file\n",
    "    batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        batch_data = pickle.load(f)\n",
    "    \n",
    "    # Print info about the number of samples\n",
    "    print(f\"Total samples in batch: {len(batch_data)}\")\n",
    "    \n",
    "    # Display the requested number of samples\n",
    "    for i, sample in enumerate(batch_data[:num_samples]):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SAMPLE {i+1} of {num_samples}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Print metadata\n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(f\"Question ID: {sample['qid']}\")\n",
    "        print(f\"Type: {sample['question_type']}\")\n",
    "        print(f\"Category: {sample['question_category']}\")\n",
    "        \n",
    "        # Load and display the image\n",
    "        img = Image.open(sample['image_path'])\n",
    "        width, height = img.size\n",
    "        print(f\"Image dimensions: {width}x{height}, Format: {img.format}\")\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Sample {i+1}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display the inputs the LLM receives\n",
    "        print(\"\\nINPUT TO LLM:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(sample['query_text'])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display expected answer\n",
    "        print(\"\\nEXPECTED OUTPUT FROM LLM:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(sample['answer_text'])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display the system message that sets the context for the LLM\n",
    "        print(\"\\nSYSTEM MESSAGE (contexts the interaction):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Run the function to show what the LLM gets\n",
    "inspect_llm_input(processed_data_dir, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Open image and convert to RGB\n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        # Define system message for medical image analysis\n",
    "        system_message = \"\"\"You are a medical image analysis assistant. Your task is to examine the provided clinical images along with clinical context, and select the option(s) that best describe what you see. \n",
    "\n",
    "        IMPORTANT: You must respond ONLY with the exact text of the option(s) that apply. \n",
    "        - Do not provide any explanations\n",
    "        - Do not include option numbers\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Format as a conversation with system, user, and assistant messages\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8df444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token if needed\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # Make sure to set this in a .env file or environment\n",
    "\n",
    "# Set model ID\n",
    "model_id = \"google/gemma-3-4b-it\"  # We'll use Gemma 3 4B with image understanding capabilities\n",
    "\n",
    "# Load processor first to use in the dataset class\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions\n",
    "def create_dummy_image():\n",
    "    \"\"\"Create a small black image as a placeholder.\"\"\"\n",
    "    return Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function for batching examples.\"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Extract image from messages\n",
    "        image_input = None\n",
    "        for msg in example[\"messages\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                for content in msg[\"content\"]:\n",
    "                    if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                        image_input = content[\"image\"]\n",
    "                        break\n",
    "        \n",
    "        if image_input is None:\n",
    "            image_input = create_dummy_image()\n",
    "            \n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        texts.append(text.strip())\n",
    "        images.append([image_input])\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts, \n",
    "        images=images,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get token IDs for special tokens to mask in loss computation\n",
    "    boi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    eoi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"eoi_token\"]\n",
    "    )\n",
    "    \n",
    "    # Mask tokens that shouldn't contribute to the loss\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == boi_token_id] = -100\n",
    "    labels[labels == eoi_token_id] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the dataset\n",
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    # Display the message structure for each sample\n",
    "    for i in range(sample_size):\n",
    "        example = dataset[i]\n",
    "        print(f\"\\nExample {i+1} message structure:\")\n",
    "        print(f\"System role: {example['messages'][0]}\")\n",
    "        print(f\"User role content:\")\n",
    "        print(f\"  Text: {example['messages'][1]['content'][0]['text'][:200]}...\")  # Show first 200 chars\n",
    "        print(f\"  Image: {type(example['messages'][1]['content'][1]['image'])}\")\n",
    "        print(f\"Assistant role: {example['messages'][2]}\")\n",
    "    \n",
    "    # Test the collate function\n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"\\nCollated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c64995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    # Process each batch. Note: in training, pass this to model.forward()\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU can support bfloat16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8:\n",
    "    print(\"WARNING: GPU may not fully support bfloat16. Consider using float16 instead.\")\n",
    "\n",
    "# Configure the model parameters\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Configure quantization for memory efficiency\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs, token=hf_token)\n",
    "\n",
    "# Print info about the chat template\n",
    "print(f\"Default chat template: {processor.tokenizer.chat_template}\")\n",
    "print(f\"Special tokens map: {processor.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32722ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",  # Apply LoRA to all linear layers\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],  # Save these modules fully\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08624c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training configuration\n",
    "output_dir = \"outputs/finetuned-model\"\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,  # Adjust based on dataset size\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients to simulate larger batch\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hub\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,  # Critical for custom datasets\n",
    "    label_names=[\"labels\"],  # Explicitly setting label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with all components\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commenting this out as my training didn't complete\n",
    "\n",
    "# # Save the trained model\n",
    "# trainer.save_model()\n",
    "# print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commenting this out as my training didn't complete\n",
    "\n",
    "# # Clean up memory first\n",
    "# del model\n",
    "# del trainer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c408469",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"outputs/finetuned-model/checkpoint-1974\"\n",
    "\n",
    "# Load base model\n",
    "# model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True, token=hf_token)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\",\n",
    "                                                    low_cpu_mem_usage=True, token=hf_token)\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "# peft_model = PeftModel.from_pretrained(model, output_dir)\n",
    "peft_model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_dir = \"outputs/merged_model\"\n",
    "merged_model.save_pretrained(merged_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "# Save the processor alongside the model\n",
    "# processor = AutoProcessor.from_pretrained(output_dir)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_path)\n",
    "processor.save_pretrained(merged_dir)\n",
    "\n",
    "print(f\"Merged model saved to {merged_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aacbccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare validation data\n",
    "def prepare_validation_data():\n",
    "    \"\"\"\n",
    "    Create a validation dataframe similar to the training dataframe.\n",
    "    \"\"\"\n",
    "    print(\"Preparing validation data...\")\n",
    "    \n",
    "    # Load question definitions\n",
    "    questions_path = os.path.join(TRAIN_DIR, \"closedquestions_definitions_imageclef2025.json\")\n",
    "    with open(questions_path, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "        \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    questions_df = pd.json_normalize(questions)[[\"qid\", \"question_en\", \"options_en\", \"question_type_en\", \"question_category_en\"]]\n",
    "    \n",
    "    # Load validation data with query information\n",
    "    val_json_path = os.path.join(VAL_DIR, \"valid.json\")\n",
    "    val_df = pd.read_json(val_json_path)\n",
    "    \n",
    "    # Extract relevant columns including query content and title\n",
    "    query_info_df = val_df[[\"encounter_id\", \"image_ids\", \"query_title_en\", \"query_content_en\", \"author_id\"]]\n",
    "    \n",
    "    # Load CVQA data (ground truth answers)\n",
    "    cvqa_path = os.path.join(VAL_DIR, \"valid_cvqa.json\")\n",
    "    with open(cvqa_path, 'r') as f:\n",
    "        cvqa_data = json.load(f)\n",
    "    cvqa_df = pd.json_normalize(cvqa_data)\n",
    "    \n",
    "    # Melt to get one row per question\n",
    "    cvqa_long = cvqa_df.melt(id_vars=[\"encounter_id\"], \n",
    "                             var_name=\"qid\", \n",
    "                             value_name=\"answer_index\")\n",
    "    \n",
    "    # Filter out encounter_id rows\n",
    "    cvqa_long = cvqa_long[cvqa_long[\"qid\"] != \"encounter_id\"]\n",
    "    \n",
    "    # Merge CVQA with questions\n",
    "    cvqa_merged = cvqa_long.merge(questions_df, on=\"qid\", how=\"left\")\n",
    "    \n",
    "    # Get answer text\n",
    "    def get_answer_text(row):\n",
    "        try:\n",
    "            return row[\"options_en\"][row[\"answer_index\"]]\n",
    "        except (IndexError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    cvqa_merged[\"answer_text\"] = cvqa_merged.apply(get_answer_text, axis=1)\n",
    "    \n",
    "    # Merge with validation data\n",
    "    final_df = cvqa_merged.merge(query_info_df, on=\"encounter_id\", how=\"left\")\n",
    "    \n",
    "    # Extract the base CQID code\n",
    "    final_df['base_qid'] = final_df['qid'].str.extract(r'(CQID\\d+)')\n",
    "    \n",
    "    # Group by encounter_id and base_qid to see all answers for each question family\n",
    "    grouped_by_family = final_df.groupby(['encounter_id', 'base_qid']).agg({\n",
    "        'qid': list,\n",
    "        'question_en': list,\n",
    "        'answer_text': list,\n",
    "        'answer_index': list,\n",
    "        'image_ids': 'first',\n",
    "        'options_en': 'first',\n",
    "        'question_type_en': 'first',\n",
    "        'question_category_en': 'first',\n",
    "        'query_title_en': 'first',\n",
    "        'query_content_en': 'first',\n",
    "        'author_id': 'first'\n",
    "    })\n",
    "    \n",
    "    # Reset index for easier manipulation\n",
    "    grouped_by_family = grouped_by_family.reset_index()\n",
    "    \n",
    "    # Modified function to extract all valid answers (treating \"Not mentioned\" appropriately)\n",
    "    def get_valid_answers(row):\n",
    "        \"\"\"\n",
    "        Extract all valid answers, with special handling for \"Not mentioned\".\n",
    "        If \"Not mentioned\" is the only answer for all slots, we keep it.\n",
    "        Otherwise, we collect all non-\"Not mentioned\" answers.\n",
    "        \"\"\"\n",
    "        answers = row['answer_text']\n",
    "        answer_indices = row['answer_index']\n",
    "        \n",
    "        if all(ans == \"Not mentioned\" for ans in answers):\n",
    "            return [\"Not mentioned\"], [answer_indices[0]]  # If all are \"Not mentioned\", return it as valid\n",
    "        \n",
    "        valid_answers = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, ans in enumerate(answers):\n",
    "            if ans != \"Not mentioned\" and ans not in valid_answers:\n",
    "                valid_answers.append(ans)\n",
    "                valid_indices.append(answer_indices[i])\n",
    "        \n",
    "        return valid_answers, valid_indices\n",
    "    \n",
    "    # Apply to all question families\n",
    "    grouped_by_family[['valid_answers', 'valid_indices']] = grouped_by_family.apply(\n",
    "        lambda row: pd.Series(get_valid_answers(row)), axis=1)\n",
    "    \n",
    "    # Create the multi-label validation dataset\n",
    "    multi_label_data = []\n",
    "    \n",
    "    # Process all validation encounters\n",
    "    for _, row in tqdm(grouped_by_family.iterrows(), desc=\"Creating validation dataset\"):\n",
    "        encounter_id = row['encounter_id']\n",
    "        base_qid = row['base_qid']\n",
    "        valid_answers = row['valid_answers']\n",
    "        valid_indices = row['valid_indices']\n",
    "        image_ids = row['image_ids']\n",
    "        question_text = row['question_en'][0]  # Taking the first question as reference\n",
    "        query_title = row['query_title_en']\n",
    "        query_content = row['query_content_en']\n",
    "        author_id = row['author_id']\n",
    "        options_en = row['options_en']\n",
    "        question_type_en = row['question_type_en']\n",
    "        question_category_en = row['question_category_en']\n",
    "        \n",
    "        # For each image in the encounter\n",
    "        for img_id in image_ids:\n",
    "            img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n",
    "            \n",
    "            # Skip if image doesn't exist\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image {img_id} not found at {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            multi_label_data.append({\n",
    "                'encounter_id': encounter_id,\n",
    "                'base_qid': base_qid,\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path,\n",
    "                'valid_answers': valid_answers,\n",
    "                'valid_indices': valid_indices,\n",
    "                'question_text': question_text,\n",
    "                'query_title_en': query_title,\n",
    "                'query_content_en': query_content,\n",
    "                'author_id': author_id,\n",
    "                'options_en': options_en,\n",
    "                'question_type_en': question_type_en, \n",
    "                'question_category_en': question_category_en,\n",
    "                'is_multi_label': len(valid_answers) > 1\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    val_dataset = pd.DataFrame(multi_label_data)\n",
    "    \n",
    "    # Save the dataset\n",
    "    val_dataset.to_csv(os.path.join(OUTPUT_DIR, \"val_dataset.csv\"), index=False)\n",
    "    \n",
    "    print(f\"Validation dataset created with {len(val_dataset)} entries\")\n",
    "    \n",
    "    return val_dataset\n",
    "\n",
    "# Function to process a batch for inference\n",
    "def process_inference_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples for inference and save them as a pickle file.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get image path\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            # Use the full image path if it's already in the dataframe\n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                # Otherwise construct from images_dir and image_id\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} â€” {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get options from options_en\n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "#                 options = row['options_en']\n",
    "#                 if isinstance(options, str):\n",
    "#                     try:\n",
    "#                         options = eval(options)\n",
    "#                     except:\n",
    "#                         options = options.split(',')\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                \n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(options)])\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            # Get question text\n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Get clinical context from query title and content\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "            \n",
    "            # Create the clinical context section\n",
    "            clinical_context = \"\"\n",
    "            if query_title:\n",
    "                clinical_context += f\"Clinical Context: {query_title}\\n\"\n",
    "            if query_content:\n",
    "                clinical_context += f\"{query_content}\\n\"\n",
    "            \n",
    "            # Create the full query text with clinical context\n",
    "            query_text = (f\"Question: Based on the image, {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Options: {options_text}\")\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"val_batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_validation_dataset(df, batch_size=50, save_dir=\"outputs/processed_val_data\", images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the entire validation dataset in batches\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Use VAL_IMAGES_DIR global variable if images_dir is not provided\n",
    "    if images_dir is None:\n",
    "        images_dir = VAL_IMAGES_DIR\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_inference_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305c0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for inference\n",
    "class MedicalImageInference:\n",
    "    def __init__(self, model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
    "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "    def predict(self, query_text, image_path, max_new_tokens=100):\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Create the system message\n",
    "#             system_message = \"You are a medical image analysis assistant. Your task is to examine the provided clinical images along with any provided clinical context, and select the option(s) that best describe what you see. Multiple answers may be correct for some questions. Respond with the exact text of the option(s) that apply. Given the medical context, err on the side of caution when uncertain.\"\n",
    "\n",
    "            system_message = \"\"\"You are a medical image analysis assistant. Your task is to examine the provided clinical images along with clinical context, and select the option(s) that best describe what you see. \n",
    "\n",
    "            IMPORTANT: You must respond ONLY with the exact text of the option(s) that apply. \n",
    "            - Do not provide any explanations\n",
    "            - Do not include option numbers\n",
    "            - Do not write \"Options:\" or similar prefixes\n",
    "            - Do not write \"Answer:\" or similar prefixes\n",
    "            - Multiple answers should be separated by commas\n",
    "            - If unsure, respond with \"Not mentioned\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            # Format as a conversation with system and user messages\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query_text},\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Create model inputs\n",
    "            inputs = self.processor(\n",
    "                text=self.processor.apply_chat_template(messages, tokenize=False),\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate prediction\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "            # Get only the new tokens (the model's answer)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = generated_ids[0][input_length:]\n",
    "\n",
    "            # Decode only the new tokens\n",
    "            prediction = self.processor.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # Clean the prediction - remove any remaining template artifacts\n",
    "            prediction = prediction.strip()\n",
    "            if prediction.startswith(\"model\\n\"):\n",
    "                prediction = prediction[len(\"model\\n\"):]\n",
    "            \n",
    "            # Extract just the answer text\n",
    "            if \"Answer:\" in prediction:\n",
    "                parts = prediction.split(\"Answer:\")\n",
    "                if len(parts) > 1:\n",
    "                    prediction = parts[1].strip()\n",
    "                \n",
    "            if prediction.startswith(\"<start_of_turn>model\") or prediction.startswith(\"<start_of_turn>assistant\"):\n",
    "                prediction = prediction.split(\"\\n\", 1)[1] if \"\\n\" in prediction else \"\"\n",
    "            if prediction.endswith(\"<end_of_turn>\"):\n",
    "                prediction = prediction[:-len(\"<end_of_turn>\")]\n",
    "\n",
    "            return prediction.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {image_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return \"Not mentioned\"  # Default to not mentioned in case of errors\n",
    "    \n",
    "    def batch_predict(self, processed_data_dir, output_file, max_samples=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of preprocessed data\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        sample_count = 0\n",
    "        \n",
    "        # Load all batch files\n",
    "        batch_files = sorted([f for f in os.listdir(processed_data_dir) if f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "        \n",
    "        # Process each batch file\n",
    "        for batch_file in tqdm(batch_files, desc=\"Processing batches\"):\n",
    "            with open(os.path.join(processed_data_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for sample in tqdm(batch_data, desc=f\"Predicting {batch_file}\", leave=False):\n",
    "                # Get prediction\n",
    "                prediction = self.predict(sample[\"query_text\"], sample[\"image_path\"])\n",
    "                \n",
    "                # Save results\n",
    "                results.append({\n",
    "                    \"encounter_id\": sample[\"id\"],\n",
    "                    \"base_qid\": sample[\"qid\"],\n",
    "                    \"image_id\": os.path.basename(sample[\"image_path\"]),\n",
    "                    \"prediction\": prediction\n",
    "                })\n",
    "                \n",
    "                sample_count += 1\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "                \n",
    "            if max_samples and sample_count >= max_samples:\n",
    "                break\n",
    "        \n",
    "        # Convert to DataFrame and save\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "#     def aggregate_predictions(self, predictions_df):\n",
    "#         \"\"\"\n",
    "#         Aggregate predictions for each encounter and question ID\n",
    "#         For each encounter-question pair, collect unique predictions across all images\n",
    "#         \"\"\"\n",
    "#         # Group by encounter_id and base_qid\n",
    "#         grouped = predictions_df.groupby(['encounter_id', 'base_qid'])\n",
    "        \n",
    "#         aggregated_results = []\n",
    "        \n",
    "#         for (encounter_id, base_qid), group in tqdm(grouped, desc=\"Aggregating predictions\"):\n",
    "#             # Extract all predictions for this group\n",
    "#             predictions = group['prediction'].tolist()\n",
    "#             image_ids = group['image_id'].tolist()\n",
    "            \n",
    "#             # Process predictions to standardize format\n",
    "#             cleaned_predictions = []\n",
    "#             for pred in predictions:\n",
    "#                 # Handle predictions that might be in a list format\n",
    "#                 if pred.startswith('[') and pred.endswith(']'):\n",
    "#                     try:\n",
    "#                         # Try to evaluate as a Python list\n",
    "#                         pred_list = eval(pred)\n",
    "#                         if isinstance(pred_list, list):\n",
    "#                             cleaned_predictions.extend(pred_list)\n",
    "#                             continue\n",
    "#                     except:\n",
    "#                         pass\n",
    "                \n",
    "#                 # Handle comma-separated values\n",
    "#                 if ',' in pred:\n",
    "#                     cleaned_predictions.extend([p.strip() for p in pred.split(',')])\n",
    "#                 else:\n",
    "#                     cleaned_predictions.append(pred.strip())\n",
    "            \n",
    "#             # Get unique predictions\n",
    "#             unique_predictions = list(set(cleaned_predictions))\n",
    "            \n",
    "#             # If \"Not mentioned\" is in predictions but there are other predictions,\n",
    "#             # remove \"Not mentioned\"\n",
    "#             if len(unique_predictions) > 1 and \"Not mentioned\" in unique_predictions:\n",
    "#                 unique_predictions.remove(\"Not mentioned\")\n",
    "            \n",
    "#             # Create a single, combined prediction\n",
    "#             combined_prediction = \", \".join(unique_predictions)\n",
    "            \n",
    "#             aggregated_results.append({\n",
    "#                 \"encounter_id\": encounter_id,\n",
    "#                 \"base_qid\": base_qid,\n",
    "#                 \"image_ids\": image_ids,\n",
    "#                 \"unique_predictions\": unique_predictions,\n",
    "#                 \"combined_prediction\": combined_prediction\n",
    "#             })\n",
    "        \n",
    "#         # Convert to DataFrame\n",
    "#         aggregated_df = pd.DataFrame(aggregated_results)\n",
    "        \n",
    "#         return aggregated_df\n",
    "\n",
    "    # Update the aggregate_predictions method in the MedicalImageInference class\n",
    "    def aggregate_predictions(self, predictions_df, validation_df=None):\n",
    "        \"\"\"\n",
    "        Aggregate predictions for each encounter and question ID\n",
    "        For each encounter-question pair, collect unique predictions across all images,\n",
    "        respecting the maximum allowed answers for each question type.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions_df: DataFrame with prediction results\n",
    "        - validation_df: Optional DataFrame containing validation data with options_en\n",
    "        \"\"\"\n",
    "        # Define maximum allowed answers for each question type\n",
    "        max_answers = {\n",
    "            'CQID010': 1,  # Single answer\n",
    "            'CQID011': 6,  # Up to 6 answers\n",
    "            'CQID012': 6,  # Up to 6 answers\n",
    "            'CQID015': 1,  # Single answer\n",
    "            'CQID020': 9,  # Up to 9 answers\n",
    "            'CQID025': 1,  # Single answer\n",
    "            'CQID034': 1,  # Single answer\n",
    "            'CQID035': 1,  # Single answer\n",
    "            'CQID036': 1   # Single answer\n",
    "        }\n",
    "\n",
    "        # Set default max_answers for any question type not explicitly listed\n",
    "        default_max_answers = 1\n",
    "\n",
    "        # Group by encounter_id and base_qid\n",
    "        grouped = predictions_df.groupby(['encounter_id', 'base_qid'])\n",
    "\n",
    "        aggregated_results = []\n",
    "\n",
    "        for (encounter_id, base_qid), group in tqdm(grouped, desc=\"Aggregating predictions\"):\n",
    "            # Extract all predictions for this group\n",
    "            predictions = group['prediction'].tolist()\n",
    "            image_ids = group['image_id'].tolist()\n",
    "\n",
    "            # Process predictions to standardize format\n",
    "            cleaned_predictions = []\n",
    "            for pred in predictions:\n",
    "                # Handle predictions that might be in a list format\n",
    "                if isinstance(pred, str):\n",
    "                    if pred.startswith('[') and pred.endswith(']'):\n",
    "                        try:\n",
    "                            # Try to evaluate as a Python list\n",
    "                            pred_list = safe_convert_options(pred)\n",
    "    #                         pred_list = eval(pred)\n",
    "                            if isinstance(pred_list, list):\n",
    "                                cleaned_predictions.extend(pred_list)\n",
    "                                continue\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                # Handle comma-separated values\n",
    "                if isinstance(pred, str) and ',' in pred:\n",
    "#                 if ',' in pred:\n",
    "                    cleaned_predictions.extend([p.strip() for p in pred.split(',')])\n",
    "                else:\n",
    "                    cleaned_predictions.append(str(pred).strip())\n",
    "#                     cleaned_predictions.append(pred.strip())\n",
    "\n",
    "            # Count frequencies of each prediction\n",
    "            from collections import Counter\n",
    "            prediction_counts = Counter(cleaned_predictions)\n",
    "\n",
    "            # Get question type for determining max allowed answers\n",
    "            question_type = base_qid.split('-')[0] if '-' in base_qid else base_qid\n",
    "\n",
    "            # Determine max allowed answers for this question type\n",
    "            allowed_max = max_answers.get(question_type, default_max_answers)\n",
    "\n",
    "            # Sort predictions by frequency (most common first)\n",
    "            sorted_predictions = sorted(prediction_counts.items(), \n",
    "                                       key=lambda x: x[1], \n",
    "                                       reverse=True)\n",
    "\n",
    "            # Get top N predictions where N is the max allowed\n",
    "            top_predictions = [p[0] for p in sorted_predictions[:allowed_max]]\n",
    "\n",
    "            # If there are ties at the cutoff point, randomly select to meet the max limit\n",
    "            if len(sorted_predictions) > allowed_max:\n",
    "                # Check if there's a tie at the cutoff\n",
    "                cutoff_count = sorted_predictions[allowed_max-1][1]\n",
    "                tied_predictions = [p[0] for p in sorted_predictions if p[1] == cutoff_count]\n",
    "\n",
    "                # If we have more tied predictions than slots available\n",
    "                if len(tied_predictions) > 1 and len(top_predictions) > allowed_max - len(tied_predictions):\n",
    "                    # Remove all tied predictions from top_predictions\n",
    "                    top_predictions = [p for p in top_predictions if p not in tied_predictions]\n",
    "\n",
    "                    # Randomly select from tied predictions to fill remaining slots\n",
    "                    import random\n",
    "                    random.seed(42)  # For reproducibility\n",
    "                    slots_remaining = allowed_max - len(top_predictions)\n",
    "                    selected_tied = random.sample(tied_predictions, slots_remaining)\n",
    "\n",
    "                    # Add the randomly selected tied predictions\n",
    "                    top_predictions.extend(selected_tied)\n",
    "\n",
    "            # If \"Not mentioned\" is in predictions but there are other predictions,\n",
    "            # remove \"Not mentioned\" (unless it's the only prediction)\n",
    "            if len(top_predictions) > 1 and \"Not mentioned\" in top_predictions:\n",
    "                top_predictions.remove(\"Not mentioned\")\n",
    "\n",
    "            # Create a single, combined prediction\n",
    "            combined_prediction = \", \".join(top_predictions)\n",
    "\n",
    "            # Initialize options_en as None\n",
    "            options_en = None\n",
    "\n",
    "            # If validation_df is provided, try to get options_en from it\n",
    "            if validation_df is not None:\n",
    "                # Find matching rows in validation_df\n",
    "                matching_rows = validation_df[(validation_df['encounter_id'] == encounter_id) & \n",
    "                                             (validation_df['base_qid'] == base_qid)]\n",
    "                if not matching_rows.empty:\n",
    "                    # Get options_en from the first matching row\n",
    "                    options_en = matching_rows.iloc[0].get('options_en')\n",
    "\n",
    "            result_dict = {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"base_qid\": base_qid,\n",
    "                \"image_ids\": image_ids,\n",
    "                \"unique_predictions\": top_predictions,  # Now limited to max allowed\n",
    "                \"combined_prediction\": combined_prediction\n",
    "            }\n",
    "\n",
    "            # Add options_en only if it's available\n",
    "            if options_en is not None:\n",
    "                result_dict[\"options_en\"] = options_en\n",
    "\n",
    "            aggregated_results.append(result_dict)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42442c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing validation dataset...\n",
      "Preparing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de9040574f2401e82ab3efa84b3bcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating validation dataset: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset created with 1413 entries\n",
      "Running in test mode with a small subset of data...\n",
      "Processing batch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4701441909de4d418b3c8395d951d404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples so far\n",
      "Processing batch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11699b60ea84e84a0373a2c9a0782ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 examples so far\n",
      "Processing batch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5290975a4bd4ffe9b96eae1dfb0e253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300 examples so far\n",
      "Processing batch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383fffaaf5414427bae365c5bbcc3e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 examples so far\n",
      "Processing batch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705d1d49c3b34a4092ddf7272f2a4300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 examples so far\n",
      "Total processed validation examples: 500\n"
     ]
    }
   ],
   "source": [
    "# Create a simple class to simulate command-line arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.test = True  # Set to True to run in test mode\n",
    "        self.skip_data_prep = False  # Set to True to skip data preparation\n",
    "        self.batch_size = 100\n",
    "        self.max_samples = 50  # Limit number of samples for inference\n",
    "        self.model_path = \"outputs/merged_model\"\n",
    "\n",
    "args = Args()\n",
    "processed_val_dir = \"outputs/processed_val_data\"\n",
    "os.makedirs(processed_val_dir, exist_ok=True)\n",
    "\n",
    "if not args.skip_data_prep:\n",
    "    # Prepare data for validation\n",
    "    print(\"Preparing validation dataset...\")\n",
    "    val_df = prepare_validation_data()\n",
    "    \n",
    "    # Subset for testing if requested\n",
    "    if args.test:\n",
    "        print(\"Running in test mode with a small subset of data...\")\n",
    "        test_size = min(500, len(val_df))\n",
    "        val_df = val_df.head(test_size)\n",
    "    \n",
    "    # Process validation data\n",
    "    # Clear any existing processed data\n",
    "    import shutil\n",
    "    if os.path.exists(processed_val_dir):\n",
    "        shutil.rmtree(processed_val_dir)\n",
    "        os.makedirs(processed_val_dir)\n",
    "    \n",
    "    total_examples = preprocess_validation_dataset(val_df, batch_size=args.batch_size, save_dir=processed_val_dir)\n",
    "    print(f\"Total processed validation examples: {total_examples}\")\n",
    "else:\n",
    "    print(\"Skipping data preparation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7feb1fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>valid_answers</th>\n",
       "      <th>valid_indices</th>\n",
       "      <th>question_text</th>\n",
       "      <th>query_title_en</th>\n",
       "      <th>query_content_en</th>\n",
       "      <th>author_id</th>\n",
       "      <th>options_en</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "      <th>is_multi_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>[limited area]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>Is this Vitiligo?  Please see picture.</td>\n",
       "      <td>The patient is a middle age female, about 50 y...</td>\n",
       "      <td>U00295</td>\n",
       "      <td>[single spot, limited area, widespread, Not me...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00852_00002.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>[limited area]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>Is this Vitiligo?  Please see picture.</td>\n",
       "      <td>The patient is a middle age female, about 50 y...</td>\n",
       "      <td>U00295</td>\n",
       "      <td>[single spot, limited area, widespread, Not me...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>[upper extremities, head]</td>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>1 Where is the affected area?</td>\n",
       "      <td>Is this Vitiligo?  Please see picture.</td>\n",
       "      <td>The patient is a middle age female, about 50 y...</td>\n",
       "      <td>U00295</td>\n",
       "      <td>[head, neck, upper extremities, lower extremit...</td>\n",
       "      <td>Site Location</td>\n",
       "      <td>General</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00852_00002.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>[upper extremities, head]</td>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>1 Where is the affected area?</td>\n",
       "      <td>Is this Vitiligo?  Please see picture.</td>\n",
       "      <td>The patient is a middle age female, about 50 y...</td>\n",
       "      <td>U00295</td>\n",
       "      <td>[head, neck, upper extremities, lower extremit...</td>\n",
       "      <td>Site Location</td>\n",
       "      <td>General</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID012</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>/storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...</td>\n",
       "      <td>[size of palm]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1 How large are the affected areas? Please spe...</td>\n",
       "      <td>Is this Vitiligo?  Please see picture.</td>\n",
       "      <td>The patient is a middle age female, about 50 y...</td>\n",
       "      <td>U00295</td>\n",
       "      <td>[size of thumb nail, size of palm, larger area...</td>\n",
       "      <td>Size</td>\n",
       "      <td>General</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid                image_id  \\\n",
       "0     ENC00852  CQID010  IMG_ENC00852_00001.jpg   \n",
       "1     ENC00852  CQID010  IMG_ENC00852_00002.jpg   \n",
       "2     ENC00852  CQID011  IMG_ENC00852_00001.jpg   \n",
       "3     ENC00852  CQID011  IMG_ENC00852_00002.jpg   \n",
       "4     ENC00852  CQID012  IMG_ENC00852_00001.jpg   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...   \n",
       "1  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...   \n",
       "2  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...   \n",
       "3  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...   \n",
       "4  /storage/coda1/p-dsgt_clef2025/0/kthakrar3/med...   \n",
       "\n",
       "               valid_answers valid_indices  \\\n",
       "0             [limited area]           [1]   \n",
       "1             [limited area]           [1]   \n",
       "2  [upper extremities, head]        [2, 0]   \n",
       "3  [upper extremities, head]        [2, 0]   \n",
       "4             [size of palm]           [1]   \n",
       "\n",
       "                                       question_text  \\\n",
       "0                  How much of the body is affected?   \n",
       "1                  How much of the body is affected?   \n",
       "2                      1 Where is the affected area?   \n",
       "3                      1 Where is the affected area?   \n",
       "4  1 How large are the affected areas? Please spe...   \n",
       "\n",
       "                           query_title_en  \\\n",
       "0  Is this Vitiligo?  Please see picture.   \n",
       "1  Is this Vitiligo?  Please see picture.   \n",
       "2  Is this Vitiligo?  Please see picture.   \n",
       "3  Is this Vitiligo?  Please see picture.   \n",
       "4  Is this Vitiligo?  Please see picture.   \n",
       "\n",
       "                                    query_content_en author_id  \\\n",
       "0  The patient is a middle age female, about 50 y...    U00295   \n",
       "1  The patient is a middle age female, about 50 y...    U00295   \n",
       "2  The patient is a middle age female, about 50 y...    U00295   \n",
       "3  The patient is a middle age female, about 50 y...    U00295   \n",
       "4  The patient is a middle age female, about 50 y...    U00295   \n",
       "\n",
       "                                          options_en question_type_en  \\\n",
       "0  [single spot, limited area, widespread, Not me...             Site   \n",
       "1  [single spot, limited area, widespread, Not me...             Site   \n",
       "2  [head, neck, upper extremities, lower extremit...    Site Location   \n",
       "3  [head, neck, upper extremities, lower extremit...    Site Location   \n",
       "4  [size of thumb nail, size of palm, larger area...             Size   \n",
       "\n",
       "  question_category_en  is_multi_label  \n",
       "0              General           False  \n",
       "1              General           False  \n",
       "2              General            True  \n",
       "3              General            True  \n",
       "4              General           False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6685d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_mentioned_rows = val_df[val_df['valid_answers'].apply(lambda x: 'Not mentioned' in x)]\n",
    "# not_mentioned_rows.head(5) # all looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd849093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from outputs/merged_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b2707ea02b49309a13f8f318c19add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference (max_samples=50)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eaedec78674d43bacc9fc0035fff5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5ce576f7be4b67b7e4841ba4fa3ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting val_batch_0.pkl:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "print(f\"Loading model from {args.model_path}...\")\n",
    "inference = MedicalImageInference(args.model_path)\n",
    "\n",
    "# Run inference on processed data\n",
    "predictions_file = os.path.join(OUTPUT_DIR, \n",
    "                               f\"val_predictions{'_test' if args.test else ''}.csv\")\n",
    "print(f\"Running inference (max_samples={args.max_samples if args.max_samples else 'all'})...\")\n",
    "predictions_df = inference.batch_predict(processed_val_dir, predictions_file, max_samples=args.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7530eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ec662aa8f240acac520646b8730982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating predictions:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/val_predictions_test.csv and /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/aggregated_predictions_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Aggregate predictions\n",
    "print(\"Aggregating predictions...\")\n",
    "\n",
    "# Load the validation dataset\n",
    "val_dataset = pd.read_csv(os.path.join(OUTPUT_DIR, \"val_dataset.csv\"))\n",
    "\n",
    "# Run the inference\n",
    "\n",
    "# Aggregate predictions with options_en\n",
    "aggregated_df = inference.aggregate_predictions(predictions_df, validation_df=val_dataset)\n",
    "\n",
    "# Save aggregated results\n",
    "aggregated_file = os.path.join(OUTPUT_DIR, \n",
    "                              f\"aggregated_predictions{'_test' if args.test else ''}.csv\")\n",
    "aggregated_df.to_csv(aggregated_file, index=False)\n",
    "\n",
    "print(f\"Inference complete. Results saved to {predictions_file} and {aggregated_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30332859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of raw predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>image_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>widespread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00852_00002.jpg</td>\n",
       "      <td>widespread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>upper extremities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid                image_id         prediction\n",
       "0     ENC00852  CQID010  IMG_ENC00852_00001.jpg         widespread\n",
       "1     ENC00852  CQID010  IMG_ENC00852_00002.jpg         widespread\n",
       "2     ENC00852  CQID011  IMG_ENC00852_00001.jpg  upper extremities"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sample of predictions for inspection\n",
    "print(\"\\nSample of raw predictions:\")\n",
    "predictions_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39598c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of aggregated predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>unique_predictions</th>\n",
       "      <th>combined_prediction</th>\n",
       "      <th>options_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>[IMG_ENC00852_00001.jpg, IMG_ENC00852_00002.jpg]</td>\n",
       "      <td>[widespread]</td>\n",
       "      <td>widespread</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>[IMG_ENC00852_00001.jpg, IMG_ENC00852_00002.jpg]</td>\n",
       "      <td>[upper extremities]</td>\n",
       "      <td>upper extremities</td>\n",
       "      <td>['head', 'neck', 'upper extremities', 'lower e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID012</td>\n",
       "      <td>[IMG_ENC00852_00001.jpg, IMG_ENC00852_00002.jpg]</td>\n",
       "      <td>[size of palm]</td>\n",
       "      <td>size of palm</td>\n",
       "      <td>['size of thumb nail', 'size of palm', 'larger...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid                                         image_ids  \\\n",
       "0     ENC00852  CQID010  [IMG_ENC00852_00001.jpg, IMG_ENC00852_00002.jpg]   \n",
       "1     ENC00852  CQID011  [IMG_ENC00852_00001.jpg, IMG_ENC00852_00002.jpg]   \n",
       "2     ENC00852  CQID012  [IMG_ENC00852_00001.jpg, IMG_ENC00852_00002.jpg]   \n",
       "\n",
       "    unique_predictions combined_prediction  \\\n",
       "0         [widespread]          widespread   \n",
       "1  [upper extremities]   upper extremities   \n",
       "2       [size of palm]        size of palm   \n",
       "\n",
       "                                          options_en  \n",
       "0  ['single spot', 'limited area', 'widespread', ...  \n",
       "1  ['head', 'neck', 'upper extremities', 'lower e...  \n",
       "2  ['size of thumb nail', 'size of palm', 'larger...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d20f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw predictions\n",
    "predictions = pd.read_csv(\"/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/val_predictions_test.csv\")\n",
    "\n",
    "# Load aggregated predictions\n",
    "aggregated = pd.read_csv(\"/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/aggregated_predictions_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c211c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of individual predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>image_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>widespread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>IMG_ENC00852_00002.jpg</td>\n",
       "      <td>widespread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>upper extremities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>IMG_ENC00852_00002.jpg</td>\n",
       "      <td>upper extremities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID012</td>\n",
       "      <td>IMG_ENC00852_00001.jpg</td>\n",
       "      <td>size of palm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid                image_id         prediction\n",
       "0     ENC00852  CQID010  IMG_ENC00852_00001.jpg         widespread\n",
       "1     ENC00852  CQID010  IMG_ENC00852_00002.jpg         widespread\n",
       "2     ENC00852  CQID011  IMG_ENC00852_00001.jpg  upper extremities\n",
       "3     ENC00852  CQID011  IMG_ENC00852_00002.jpg  upper extremities\n",
       "4     ENC00852  CQID012  IMG_ENC00852_00001.jpg       size of palm"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample predictions\n",
    "print(\"Sample of individual predictions:\")\n",
    "predictions.head(5)[[\"encounter_id\", \"base_qid\", \"image_id\", \"prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bd2065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of aggregated predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>base_qid</th>\n",
       "      <th>combined_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID010</td>\n",
       "      <td>widespread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID011</td>\n",
       "      <td>upper extremities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID012</td>\n",
       "      <td>size of palm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID015</td>\n",
       "      <td>Not mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00852</td>\n",
       "      <td>CQID020</td>\n",
       "      <td>Leukoplakia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id base_qid combined_prediction\n",
       "0     ENC00852  CQID010          widespread\n",
       "1     ENC00852  CQID011   upper extremities\n",
       "2     ENC00852  CQID012        size of palm\n",
       "3     ENC00852  CQID015       Not mentioned\n",
       "4     ENC00852  CQID020         Leukoplakia"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated.head(5)[[\"encounter_id\", \"base_qid\", \"combined_prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35b482b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common predictions:\n",
      "prediction\n",
      "upper extremities                                                                  5\n",
      "widespread                                                                         4\n",
      "Not mentioned                                                                      4\n",
      "stubborn skin lesions on the left hand\\nsize of palm\\nlarger area on both hands    3\n",
      "size of palm                                                                       2\n",
      "red, white                                                                         2\n",
      "Leukoplakia                                                                        2\n",
      "Leukoplakia, erythma                                                               2\n",
      "Leukoplakia, rough                                                                 2\n",
      "yes                                                                                2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count frequency of different answers\n",
    "answer_counts = predictions[\"prediction\"].value_counts().head(10)\n",
    "print(\"\\nMost common predictions:\")\n",
    "print(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34e0cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To do inference on full dataset:\n",
    "# args = Args()\n",
    "# args.test = False  # Run on the full validation set\n",
    "# args.skip_data_prep = False  # Process all data\n",
    "# args.max_samples = None  # No limit on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2510e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_for_official_eval_with_display(aggregated_df, output_file):\n",
    "    \"\"\"\n",
    "    Format predictions as expected by the official evaluation script,\n",
    "    mapping text answers to indices and distributing multiple answers\n",
    "    across question variants when appropriate.\n",
    "    Also displays the text values alongside their indices for verification.\n",
    "    \"\"\"\n",
    "    # Define the question IDs and their allowed variants\n",
    "    QIDS = [\n",
    "        \"CQID010-001\",  # how much of body is affected (single answer)\n",
    "        \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",  # multiple answers allowed\n",
    "        \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",  # multiple answers allowed\n",
    "        \"CQID015-001\",  # single answer\n",
    "        \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "        \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",  # multiple answers allowed\n",
    "        \"CQID025-001\",  # single answer\n",
    "        \"CQID034-001\",  # single answer\n",
    "        \"CQID035-001\",  # single answer\n",
    "        \"CQID036-001\",  # single answer\n",
    "    ]\n",
    "    \n",
    "    # Create a mapping of question base IDs to their allowed variants\n",
    "    qid_variants = {}\n",
    "    for qid in QIDS:\n",
    "        base_qid, variant = qid.split('-')\n",
    "        if base_qid not in qid_variants:\n",
    "            qid_variants[base_qid] = []\n",
    "        qid_variants[base_qid].append(qid)\n",
    "    \n",
    "    # Get all required base QIDs for a complete encounter\n",
    "    required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "    \n",
    "    formatted_predictions = []\n",
    "    display_info = []\n",
    "    \n",
    "    # Group by encounter_id\n",
    "    for encounter_id, group in aggregated_df.groupby('encounter_id'):\n",
    "        # Get all base_qids for this encounter\n",
    "        encounter_base_qids = set(group['base_qid'].unique())\n",
    "        \n",
    "        # Skip encounters that don't have all required questions\n",
    "        if not required_base_qids.issubset(encounter_base_qids):\n",
    "            print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "            continue\n",
    "        \n",
    "        # Create a prediction entry for this encounter\n",
    "        pred_entry = {'encounter_id': encounter_id}\n",
    "        encounter_display = {'encounter_id': encounter_id, 'questions': []}\n",
    "        \n",
    "        # Process each question for this encounter\n",
    "        for _, row in group.iterrows():\n",
    "            base_qid = row['base_qid']\n",
    "            \n",
    "            # Skip if we don't have variants defined for this question\n",
    "            if base_qid not in qid_variants:\n",
    "                continue\n",
    "            \n",
    "            # Get the options list for this question\n",
    "            options = safe_convert_options(row['options_en'])\n",
    "\n",
    "#             options = row['options_en']\n",
    "#             if isinstance(options, str):\n",
    "#                 try:\n",
    "#                     options = eval(options)\n",
    "#                 except:\n",
    "#                     options = options.split(',')\n",
    "            \n",
    "            # Find the index of \"Not mentioned\" in the options\n",
    "            not_mentioned_index = None\n",
    "            for i, opt in enumerate(options):\n",
    "                if opt == \"Not mentioned\":\n",
    "                    not_mentioned_index = i\n",
    "                    break\n",
    "            \n",
    "            # If \"Not mentioned\" is not in the options, default to the last option\n",
    "            if not_mentioned_index is None:\n",
    "                not_mentioned_index = len(options) - 1\n",
    "            \n",
    "            # Get predictions\n",
    "            if isinstance(row['unique_predictions'], list):\n",
    "                predictions = row['unique_predictions']\n",
    "            else:\n",
    "                try:\n",
    "                    predictions = eval(row['unique_predictions'])\n",
    "                except:\n",
    "                    predictions = [row['unique_predictions']]\n",
    "            \n",
    "            # Map text predictions to indices\n",
    "            prediction_indices = []\n",
    "            prediction_texts = []\n",
    "            \n",
    "            for pred in predictions:\n",
    "                pred_text = str(pred).strip()\n",
    "                prediction_texts.append(pred_text)\n",
    "                \n",
    "                # Find index of the prediction in options\n",
    "                found = False\n",
    "                for i, option in enumerate(options):\n",
    "                    if pred_text.lower() == option.lower():\n",
    "                        prediction_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                # If prediction not found in options, use index 100\n",
    "                if not found:\n",
    "                    prediction_indices.append(100)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            unique_indices = []\n",
    "            unique_texts = []\n",
    "            for idx, text in zip(prediction_indices, prediction_texts):\n",
    "                if idx not in unique_indices:\n",
    "                    unique_indices.append(idx)\n",
    "                    unique_texts.append(text)\n",
    "            \n",
    "            # If 100 is in the list along with valid indices, remove 100\n",
    "            if len(unique_indices) > 1 and 100 in unique_indices:\n",
    "                idx_to_remove = unique_indices.index(100)\n",
    "                unique_indices.remove(100)\n",
    "                unique_texts.pop(idx_to_remove)\n",
    "            \n",
    "            # Get the available variants for this question\n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            # Store info for display\n",
    "            question_display = {\n",
    "                'base_qid': base_qid,\n",
    "                'predicted_texts': unique_texts,\n",
    "                'predicted_indices': unique_indices,\n",
    "                'options': options,\n",
    "                'not_mentioned_index': not_mentioned_index,\n",
    "                'variant_assignments': {}\n",
    "            }\n",
    "            \n",
    "            # For single-answer questions (with only one variant)\n",
    "            if len(available_variants) == 1:\n",
    "                if unique_indices:\n",
    "                    # Store as a single integer, not a list\n",
    "                    pred_entry[available_variants[0]] = unique_indices[0]\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': unique_indices[0],\n",
    "                        'text': unique_texts[0] if unique_texts else \"None\"\n",
    "                    }\n",
    "                else:\n",
    "                    # Default to \"Not mentioned\" if no prediction\n",
    "                    pred_entry[available_variants[0]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            # For multi-answer questions\n",
    "            else:\n",
    "                # Distribute answers across available variants\n",
    "                for i, idx in enumerate(unique_indices):\n",
    "                    if i < len(available_variants):\n",
    "                        # Store each answer as a single integer, not a list\n",
    "                        pred_entry[available_variants[i]] = idx\n",
    "                        question_display['variant_assignments'][available_variants[i]] = {\n",
    "                            'index': idx,\n",
    "                            'text': unique_texts[i] if i < len(unique_texts) else \"None\"\n",
    "                        }\n",
    "                \n",
    "                # Fill remaining variants with a default value (usually \"Not mentioned\")\n",
    "                for i in range(len(unique_indices), len(available_variants)):\n",
    "                    # Use correct \"Not mentioned\" index for this question\n",
    "                    pred_entry[available_variants[i]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[i]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            encounter_display['questions'].append(question_display)\n",
    "        \n",
    "        formatted_predictions.append(pred_entry)\n",
    "        display_info.append(encounter_display)\n",
    "    \n",
    "    if not formatted_predictions:\n",
    "        print(\"Warning: No complete encounters found in the data!\")\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_predictions, f, indent=2)\n",
    "    \n",
    "    # Display information about the predictions\n",
    "    for encounter in display_info:\n",
    "        print(f\"\\nEncounter: {encounter['encounter_id']}\")\n",
    "        for question in encounter['questions']:\n",
    "            print(f\"  Question: {question['base_qid']}\")\n",
    "            print(f\"  Predicted texts: {question['predicted_texts']}\")\n",
    "            print(f\"  Predicted indices: {question['predicted_indices']}\")\n",
    "            print(f\"  'Not mentioned' index: {question['not_mentioned_index']}\")\n",
    "            print(\"  Variant assignments:\")\n",
    "            for variant, assignment in question['variant_assignments'].items():\n",
    "                print(f\"    {variant}: index={assignment['index']} ({assignment['text']})\")\n",
    "            print(f\"  Available options: {question['options']}\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b905637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping encounter ENC00854 - missing required questions\n",
      "\n",
      "Encounter: ENC00852\n",
      "  Question: CQID010\n",
      "  Predicted texts: ['widespread']\n",
      "  Predicted indices: [2]\n",
      "  'Not mentioned' index: 3\n",
      "  Variant assignments:\n",
      "    CQID010-001: index=2 (widespread)\n",
      "  Available options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "\n",
      "  Question: CQID011\n",
      "  Predicted texts: ['upper extremities']\n",
      "  Predicted indices: [2]\n",
      "  'Not mentioned' index: 7\n",
      "  Variant assignments:\n",
      "    CQID011-001: index=2 (upper extremities)\n",
      "    CQID011-002: index=7 (Not mentioned)\n",
      "    CQID011-003: index=7 (Not mentioned)\n",
      "    CQID011-004: index=7 (Not mentioned)\n",
      "    CQID011-005: index=7 (Not mentioned)\n",
      "    CQID011-006: index=7 (Not mentioned)\n",
      "  Available options: ['head', 'neck', 'upper extremities', 'lower extremities', 'chest/abdomen', 'back', 'other (please specify)', 'Not mentioned']\n",
      "\n",
      "  Question: CQID012\n",
      "  Predicted texts: ['size of palm']\n",
      "  Predicted indices: [1]\n",
      "  'Not mentioned' index: 3\n",
      "  Variant assignments:\n",
      "    CQID012-001: index=1 (size of palm)\n",
      "    CQID012-002: index=3 (Not mentioned)\n",
      "    CQID012-003: index=3 (Not mentioned)\n",
      "    CQID012-004: index=3 (Not mentioned)\n",
      "    CQID012-005: index=3 (Not mentioned)\n",
      "    CQID012-006: index=3 (Not mentioned)\n",
      "  Available options: ['size of thumb nail', 'size of palm', 'larger area', 'Not mentioned']\n",
      "\n",
      "  Question: CQID015\n",
      "  Predicted texts: ['Not mentioned']\n",
      "  Predicted indices: [6]\n",
      "  'Not mentioned' index: 6\n",
      "  Variant assignments:\n",
      "    CQID015-001: index=6 (Not mentioned)\n",
      "  Available options: ['within hours', 'within days', 'within weeks', 'within months', 'over a year', 'multiple years', 'Not mentioned']\n",
      "\n",
      "  Question: CQID020\n",
      "  Predicted texts: ['Leukoplakia']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 9\n",
      "  Variant assignments:\n",
      "    CQID020-001: index=100 (Leukoplakia)\n",
      "    CQID020-002: index=9 (Not mentioned)\n",
      "    CQID020-003: index=9 (Not mentioned)\n",
      "    CQID020-004: index=9 (Not mentioned)\n",
      "    CQID020-005: index=9 (Not mentioned)\n",
      "    CQID020-006: index=9 (Not mentioned)\n",
      "    CQID020-007: index=9 (Not mentioned)\n",
      "    CQID020-008: index=9 (Not mentioned)\n",
      "    CQID020-009: index=9 (Not mentioned)\n",
      "  Available options: ['raised or bumpy', 'flat', 'skin loss or sunken', 'thick or raised', 'thin or close to the surface', 'warty', 'crust', 'scab', 'weeping', 'Not mentioned']\n",
      "\n",
      "  Question: CQID025\n",
      "  Predicted texts: ['Not mentioned']\n",
      "  Predicted indices: [2]\n",
      "  'Not mentioned' index: 2\n",
      "  Variant assignments:\n",
      "    CQID025-001: index=2 (Not mentioned)\n",
      "  Available options: ['yes', 'no', 'Not mentioned']\n",
      "\n",
      "  Question: CQID034\n",
      "  Predicted texts: ['red']\n",
      "  Predicted indices: [2]\n",
      "  'Not mentioned' index: 11\n",
      "  Variant assignments:\n",
      "    CQID034-001: index=2 (red)\n",
      "  Available options: ['normal skin color', 'pink', 'red', 'brown', 'blue', 'purple', 'black', 'white', 'combination (please specify)', 'hyperpigmentation', 'hypopigmentation', 'Not mentioned']\n",
      "\n",
      "  Question: CQID035\n",
      "  Predicted texts: ['Leukoplakia']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 2\n",
      "  Variant assignments:\n",
      "    CQID035-001: index=100 (Leukoplakia)\n",
      "  Available options: ['single', 'multiple (please specify)', 'Not mentioned']\n",
      "\n",
      "  Question: CQID036\n",
      "  Predicted texts: ['Leukoplakia']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 2\n",
      "  Variant assignments:\n",
      "    CQID036-001: index=100 (Leukoplakia)\n",
      "  Available options: ['smooth', 'rough', 'Not mentioned']\n",
      "\n",
      "\n",
      "Encounter: ENC00853\n",
      "  Question: CQID010\n",
      "  Predicted texts: ['widespread']\n",
      "  Predicted indices: [2]\n",
      "  'Not mentioned' index: 3\n",
      "  Variant assignments:\n",
      "    CQID010-001: index=2 (widespread)\n",
      "  Available options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "\n",
      "  Question: CQID011\n",
      "  Predicted texts: ['Ã§eÅŸitli yerlerde olduÄŸu gibi']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 7\n",
      "  Variant assignments:\n",
      "    CQID011-001: index=100 (Ã§eÅŸitli yerlerde olduÄŸu gibi)\n",
      "    CQID011-002: index=7 (Not mentioned)\n",
      "    CQID011-003: index=7 (Not mentioned)\n",
      "    CQID011-004: index=7 (Not mentioned)\n",
      "    CQID011-005: index=7 (Not mentioned)\n",
      "    CQID011-006: index=7 (Not mentioned)\n",
      "  Available options: ['head', 'neck', 'upper extremities', 'lower extremities', 'chest/abdomen', 'back', 'other (please specify)', 'Not mentioned']\n",
      "\n",
      "  Question: CQID012\n",
      "  Predicted texts: ['Ã§eÅŸitli yerlerde olduÄŸu gibi']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 3\n",
      "  Variant assignments:\n",
      "    CQID012-001: index=100 (Ã§eÅŸitli yerlerde olduÄŸu gibi)\n",
      "    CQID012-002: index=3 (Not mentioned)\n",
      "    CQID012-003: index=3 (Not mentioned)\n",
      "    CQID012-004: index=3 (Not mentioned)\n",
      "    CQID012-005: index=3 (Not mentioned)\n",
      "    CQID012-006: index=3 (Not mentioned)\n",
      "  Available options: ['size of thumb nail', 'size of palm', 'larger area', 'Not mentioned']\n",
      "\n",
      "  Question: CQID015\n",
      "  Predicted texts: ['multiple years']\n",
      "  Predicted indices: [5]\n",
      "  'Not mentioned' index: 6\n",
      "  Variant assignments:\n",
      "    CQID015-001: index=5 (multiple years)\n",
      "  Available options: ['within hours', 'within days', 'within weeks', 'within months', 'over a year', 'multiple years', 'Not mentioned']\n",
      "\n",
      "  Question: CQID020\n",
      "  Predicted texts: ['thin or close to the surface']\n",
      "  Predicted indices: [4]\n",
      "  'Not mentioned' index: 9\n",
      "  Variant assignments:\n",
      "    CQID020-001: index=4 (thin or close to the surface)\n",
      "    CQID020-002: index=9 (Not mentioned)\n",
      "    CQID020-003: index=9 (Not mentioned)\n",
      "    CQID020-004: index=9 (Not mentioned)\n",
      "    CQID020-005: index=9 (Not mentioned)\n",
      "    CQID020-006: index=9 (Not mentioned)\n",
      "    CQID020-007: index=9 (Not mentioned)\n",
      "    CQID020-008: index=9 (Not mentioned)\n",
      "    CQID020-009: index=9 (Not mentioned)\n",
      "  Available options: ['raised or bumpy', 'flat', 'skin loss or sunken', 'thick or raised', 'thin or close to the surface', 'warty', 'crust', 'scab', 'weeping', 'Not mentioned']\n",
      "\n",
      "  Question: CQID025\n",
      "  Predicted texts: ['yes']\n",
      "  Predicted indices: [0]\n",
      "  'Not mentioned' index: 2\n",
      "  Variant assignments:\n",
      "    CQID025-001: index=0 (yes)\n",
      "  Available options: ['yes', 'no', 'Not mentioned']\n",
      "\n",
      "  Question: CQID034\n",
      "  Predicted texts: ['*   **Ten years of suffering**\\n*   **Recurrent disease**\\n*   **Severe itching**\\n*   **Happens wherever I scratch**\\n*   **Multiple locations**']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 11\n",
      "  Variant assignments:\n",
      "    CQID034-001: index=100 (*   **Ten years of suffering**\n",
      "*   **Recurrent disease**\n",
      "*   **Severe itching**\n",
      "*   **Happens wherever I scratch**\n",
      "*   **Multiple locations**)\n",
      "  Available options: ['normal skin color', 'pink', 'red', 'brown', 'blue', 'purple', 'black', 'white', 'combination (please specify)', 'hyperpigmentation', 'hypopigmentation', 'Not mentioned']\n",
      "\n",
      "  Question: CQID035\n",
      "  Predicted texts: ['recurrent']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 2\n",
      "  Variant assignments:\n",
      "    CQID035-001: index=100 (recurrent)\n",
      "  Available options: ['single', 'multiple (please specify)', 'Not mentioned']\n",
      "\n",
      "  Question: CQID036\n",
      "  Predicted texts: ['recurrent']\n",
      "  Predicted indices: [100]\n",
      "  'Not mentioned' index: 2\n",
      "  Variant assignments:\n",
      "    CQID036-001: index=100 (recurrent)\n",
      "  Available options: ['smooth', 'rough', 'Not mentioned']\n",
      "\n",
      "Formatted predictions saved to /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/val_cvqa_sys_test.json (2 complete encounters)\n",
      "Formatted predictions saved to /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/val_cvqa_sys_test.json\n"
     ]
    }
   ],
   "source": [
    "# Format and save predictions for official evaluation\n",
    "predictions_json = os.path.join(OUTPUT_DIR, f\"val_cvqa_sys{'_test' if args.test else ''}.json\")\n",
    "format_predictions_for_official_eval_with_display(aggregated_df, predictions_json)\n",
    "\n",
    "print(f\"Formatted predictions saved to {predictions_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd30e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Information:\n",
      "Question text: 1 How large are the affected areas? Please specify which affected area for each selection.\n",
      "Question type: Size\n",
      "Question category: General\n",
      "Options: ['size of thumb nail', 'size of palm', 'larger area', 'Not mentioned']\n",
      "Multi-label: False\n",
      "\n",
      "Clinical Context:\n",
      "Query title: Please help take a look, what kind of skin disease is this?\n",
      "Query content: Suffering from the disease for more than 10 years.  It is recurrent and is vey itchy!  It happens wherever I scratch in some places.\n",
      "\n",
      "Image information:\n",
      "Image ID: IMG_ENC00853_00001.jpg\n",
      "Image path: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/2025_dataset/valid/images_valid/IMG_ENC00853_00001.jpg\n"
     ]
    }
   ],
   "source": [
    "# Filter validation DataFrame to get the specific question and encounter\n",
    "specific_question = val_dataset[(val_dataset['encounter_id'] == 'ENC00853') & \n",
    "                               (val_dataset['base_qid'] == 'CQID012')]\n",
    "\n",
    "# Display all relevant columns\n",
    "print(\"Question Information:\")\n",
    "print(f\"Question text: {specific_question['question_text'].values[0]}\")\n",
    "print(f\"Question type: {specific_question['question_type_en'].values[0]}\")\n",
    "print(f\"Question category: {specific_question['question_category_en'].values[0]}\")\n",
    "print(f\"Options: {specific_question['options_en'].values[0]}\")\n",
    "print(f\"Multi-label: {specific_question['is_multi_label'].values[0]}\")\n",
    "print(\"\\nClinical Context:\")\n",
    "print(f\"Query title: {specific_question['query_title_en'].values[0]}\")\n",
    "print(f\"Query content: {specific_question['query_content_en'].values[0]}\")\n",
    "print(\"\\nImage information:\")\n",
    "print(f\"Image ID: {specific_question['image_id'].values[0]}\")\n",
    "print(f\"Image path: {specific_question['image_path'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "482db9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 prediction entries:\n",
      "\n",
      "Prediction 1:\n",
      "{'CQID010-001': 2,\n",
      " 'CQID011-001': 2,\n",
      " 'CQID011-002': 7,\n",
      " 'CQID011-003': 7,\n",
      " 'CQID011-004': 7,\n",
      " 'CQID011-005': 7,\n",
      " 'CQID011-006': 7,\n",
      " 'CQID012-001': 1,\n",
      " 'CQID012-002': 3,\n",
      " 'CQID012-003': 3,\n",
      " 'CQID012-004': 3,\n",
      " 'CQID012-005': 3,\n",
      " 'CQID012-006': 3,\n",
      " 'CQID015-001': 6,\n",
      " 'CQID020-001': 100,\n",
      " 'CQID020-002': 9,\n",
      " 'CQID020-003': 9,\n",
      " 'CQID020-004': 9,\n",
      " 'CQID020-005': 9,\n",
      " 'CQID020-006': 9,\n",
      " 'CQID020-007': 9,\n",
      " 'CQID020-008': 9,\n",
      " 'CQID020-009': 9,\n",
      " 'CQID025-001': 2,\n",
      " 'CQID034-001': 2,\n",
      " 'CQID035-001': 100,\n",
      " 'CQID036-001': 100,\n",
      " 'encounter_id': 'ENC00852'}\n",
      "\n",
      "Prediction 2:\n",
      "{'CQID010-001': 2,\n",
      " 'CQID011-001': 100,\n",
      " 'CQID011-002': 7,\n",
      " 'CQID011-003': 7,\n",
      " 'CQID011-004': 7,\n",
      " 'CQID011-005': 7,\n",
      " 'CQID011-006': 7,\n",
      " 'CQID012-001': 100,\n",
      " 'CQID012-002': 3,\n",
      " 'CQID012-003': 3,\n",
      " 'CQID012-004': 3,\n",
      " 'CQID012-005': 3,\n",
      " 'CQID012-006': 3,\n",
      " 'CQID015-001': 5,\n",
      " 'CQID020-001': 4,\n",
      " 'CQID020-002': 9,\n",
      " 'CQID020-003': 9,\n",
      " 'CQID020-004': 9,\n",
      " 'CQID020-005': 9,\n",
      " 'CQID020-006': 9,\n",
      " 'CQID020-007': 9,\n",
      " 'CQID020-008': 9,\n",
      " 'CQID020-009': 9,\n",
      " 'CQID025-001': 0,\n",
      " 'CQID034-001': 100,\n",
      " 'CQID035-001': 100,\n",
      " 'CQID036-001': 100,\n",
      " 'encounter_id': 'ENC00853'}\n",
      "\n",
      "Looking for predictions with index 100 (not in options):\n",
      "\n",
      "Found prediction not in options:\n",
      "Encounter: ENC00852\n",
      "Question: CQID020-001\n",
      "Prediction indices: 100\n",
      "Original prediction text: Leukoplakia\n",
      "Available options: ['raised or bumpy', 'flat', 'skin loss or sunken', 'thick or raised', 'thin or close to the surface', 'warty', 'crust', 'scab', 'weeping', 'Not mentioned']\n",
      "\n",
      "Number of questions per encounter:\n",
      "27 questions: 2 encounters\n"
     ]
    }
   ],
   "source": [
    "# allows us to double check the saved file\n",
    "\n",
    "# Load the formatted predictions JSON\n",
    "with open('/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/val_cvqa_sys_test.json', 'r') as f:\n",
    "    formatted_preds = json.load(f)\n",
    "\n",
    "# Display the first 3 entries\n",
    "print(\"First 3 prediction entries:\")\n",
    "for i in range(min(3, len(formatted_preds))):\n",
    "    print(f\"\\nPrediction {i+1}:\")\n",
    "    pprint(formatted_preds[i])\n",
    "\n",
    "# Show an example of answers not in options (if any)\n",
    "print(\"\\nLooking for predictions with index 100 (not in options):\")\n",
    "found = False\n",
    "for entry in formatted_preds:\n",
    "    for key, value in entry.items():\n",
    "        if key != 'encounter_id':  # Skip the encounter_id\n",
    "            if (isinstance(value, list) and 100 in value) or value == 100:\n",
    "                print(f\"\\nFound prediction not in options:\")\n",
    "                print(f\"Encounter: {entry['encounter_id']}\")\n",
    "                print(f\"Question: {key}\")\n",
    "                print(f\"Prediction indices: {value}\")\n",
    "                \n",
    "                # Load original predictions for this encounter\n",
    "                agg_df = pd.read_csv('/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/aggregated_predictions_test.csv')\n",
    "                base_qid = key.split('-')[0]\n",
    "                encounter = entry['encounter_id']\n",
    "                match = agg_df[(agg_df['encounter_id'] == encounter) & (agg_df['base_qid'] == base_qid)]\n",
    "                if not match.empty:\n",
    "                    print(f\"Original prediction text: {match['combined_prediction'].values[0]}\")\n",
    "                    print(f\"Available options: {match['options_en'].values[0]}\")\n",
    "                found = True\n",
    "                break\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print(\"No predictions with index 100 found in the first few entries.\")\n",
    "\n",
    "# Show statistics\n",
    "question_counts = {}\n",
    "for entry in formatted_preds:\n",
    "    qid_count = len(entry) - 1  # Subtract 1 for encounter_id\n",
    "    if qid_count in question_counts:\n",
    "        question_counts[qid_count] += 1\n",
    "    else:\n",
    "        question_counts[qid_count] = 1\n",
    "\n",
    "print(\"\\nNumber of questions per encounter:\")\n",
    "for count, num_entries in sorted(question_counts.items()):\n",
    "    print(f\"{count} questions: {num_entries} encounters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad8f01",
   "metadata": {},
   "source": [
    "Run evaluation in terminal with the following command: \n",
    "\n",
    "```\n",
    "python evaluate_new.py outputs/val_cvqa_sys_test.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd679b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
