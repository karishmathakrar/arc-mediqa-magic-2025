{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import traceback\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine learning and deep learning\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# TensorBoard related\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# Others\n",
    "from dotenv import load_dotenv\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display environment information\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set up environment variables and cache directories\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "print(f\"HF_HOME: {os.getenv('HF_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3271e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"2025_dataset\")\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"valid\")\n",
    "VAL_IMAGES_DIR = os.path.join(VAL_DIR, \"images_valid\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0747824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = os.path.join(OUTPUT_DIR, \"multi_label_dataset.csv\")\n",
    "print(train_csv_file)\n",
    "train_images_dir = os.path.join(TRAIN_DIR, \"images_train\")\n",
    "print(train_images_dir)\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3123d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_options(options_str):\n",
    "    \"\"\"\n",
    "    Safely convert a string representation of a list to an actual list.\n",
    "    \"\"\"\n",
    "    if not isinstance(options_str, str):\n",
    "        return options_str\n",
    "        \n",
    "    try:\n",
    "        # Use ast.literal_eval which is safer than eval()\n",
    "        return ast.literal_eval(options_str)\n",
    "    except (SyntaxError, ValueError):\n",
    "        # Try common formats\n",
    "        if options_str.startswith('[') and options_str.endswith(']'):\n",
    "            # Strip brackets and split by commas\n",
    "            return [opt.strip().strip(\"'\\\"\") for opt in options_str[1:-1].split(',')]\n",
    "        elif ',' in options_str:\n",
    "            # Just split by commas\n",
    "            return [opt.strip() for opt in options_str.split(',')]\n",
    "        else:\n",
    "            # Single option\n",
    "            return [options_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples and save them as a pickle file.\n",
    "    Includes query title and content as clinical context.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get image path - using image_id instead of image_ids\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            # Use the full image path if it's already in the dataframe\n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                # Otherwise construct from images_dir and image_id\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} â€” {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get options from options_en\n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "                \n",
    "                # Clean up options by removing \"(please specify)\" phrases\n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        # Remove \"(please specify)\" from option text\n",
    "                        cleaned_opt = opt.replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "                        cleaned_options.append(opt)\n",
    "                options = cleaned_options\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                \n",
    "            options_text = \", \".join(options)\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            # Get question text and clean it\n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Remove \"Please specify which affected area for each selection.\" from CQID012\n",
    "            if \"Please specify which affected area for each selection\" in question:\n",
    "                question = question.replace(\" Please specify which affected area for each selection.\", \"\")\n",
    "            \n",
    "            # Remove leading numbers like \"1 \" from the beginning of questions\n",
    "            question = re.sub(r'^\\d+\\s+', '', question)\n",
    "            \n",
    "            # Get clinical context from query title and content\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "            \n",
    "            # Create the clinical context section\n",
    "            clinical_context = \"\"\n",
    "            if query_title or query_content:\n",
    "                clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "                if query_title:\n",
    "                    clinical_context += f\"{query_title}\\n\"\n",
    "                if query_content:\n",
    "                    clinical_context += f\"{query_content}\\n\"\n",
    "\n",
    "            # Create the full query text with clinical context\n",
    "            query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Available Options (choose from these): {options_text}\")\n",
    "            \n",
    "            # Get answer text - from valid_answers and clean it\n",
    "            if 'valid_answers' in row and row['valid_answers']:\n",
    "                # For multi-label, join all valid answers\n",
    "                answers = row['valid_answers']\n",
    "                if isinstance(answers, list):\n",
    "                    # Clean the answers by removing \"(please specify)\"\n",
    "                    cleaned_answers = []\n",
    "                    for ans in answers:\n",
    "                        if isinstance(ans, str):\n",
    "                            # Remove any quotes and trailing/leading spaces\n",
    "                            cleaned_ans = ans.strip(\"'\\\" \")\n",
    "                            # Remove \"(please specify)\"\n",
    "                            cleaned_ans = cleaned_ans.replace(\" (please specify)\", \"\")\n",
    "                            cleaned_answers.append(cleaned_ans)\n",
    "                        else:\n",
    "                            cleaned_answers.append(str(ans).strip(\"'\\\" \"))\n",
    "                    \n",
    "                    if len(cleaned_answers) > 1:\n",
    "                        # Join multiple answers with commas\n",
    "                        answer_text = \", \".join(cleaned_answers)\n",
    "                    elif len(cleaned_answers) == 1:\n",
    "                        answer_text = cleaned_answers[0]\n",
    "                    else:\n",
    "                        answer_text = \"Not mentioned\"\n",
    "                else:\n",
    "                    # Clean single answer\n",
    "                    if isinstance(answers, str):\n",
    "                        # Remove any quotes and leading/trailing spaces\n",
    "                        answer_text = answers.strip(\"'\\\" \")\n",
    "                        answer_text = answer_text.replace(\" (please specify)\", \"\")\n",
    "                    else:\n",
    "                        answer_text = str(answers).strip(\"'\\\" \")\n",
    "                        \n",
    "                # Make sure the answer is stored as plain text, not as a string representation of a list\n",
    "#                 if isinstance(answer_text, str) and answer_text.startswith(\"['\") and answer_text.endswith(\"']\"):\n",
    "                if isinstance(answer_text, str) and answer_text.startswith(\"[\") and answer_text.endswith(\"]\"):\n",
    "                    # Extract content from list representation\n",
    "                    clean_text = answer_text.strip(\"[]'\")\n",
    "                    # Split by quoted separators and rejoin with clean commas\n",
    "                    parts = [part.strip() for part in clean_text.split(\"', '\")]\n",
    "                    answer_text = \", \".join(parts)\n",
    "            \n",
    "            elif 'multi_label' in row:\n",
    "                answer_text = row['multi_label']\n",
    "            else:\n",
    "                answer_text = \"Not mentioned\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"answer_text\": answer_text,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"outputsprocessed_data\", images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the entire dataset in batches\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Use train_images_dir global variable if images_dir is not provided\n",
    "    if images_dir is None:\n",
    "        images_dir = train_images_dir\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ea4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save the dataset\n",
    "processed_data_dir = \"outputs/processed_data\"\n",
    "\n",
    "# Clear any existing processed data (optional)\n",
    "if os.path.exists(processed_data_dir):\n",
    "    shutil.rmtree(processed_data_dir)\n",
    "    \n",
    "# Process the dataset\n",
    "total_examples = preprocess_dataset(train_df, batch_size=100, save_dir=processed_data_dir)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9268c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the processed data\n",
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[0]\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "print(\"\\nSample of processed data (second example):\")\n",
    "sample_data = batch_data[1]\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_llm_training_inputs(processed_data_dir, qids=[\"CQID034\", \"CQID012\", \"CQID011\"], \n",
    "                               multi_label_examples=True, single_label_examples=True, \n",
    "                               num_samples_per_qid=1, show_images=True):\n",
    "    \"\"\"\n",
    "    Shows the exact inputs that would be sent to the LLM during training.\n",
    "    \n",
    "    Args:\n",
    "        processed_data_dir: Directory containing processed batch files\n",
    "        qids: List of QIDs to look for\n",
    "        multi_label_examples: Whether to include multi-label examples\n",
    "        single_label_examples: Whether to include single-label examples\n",
    "        num_samples_per_qid: How many samples to show for each QID\n",
    "        show_images: Whether to display the images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Look in multiple batch files\n",
    "    batch_files = sorted([f for f in os.listdir(processed_data_dir) \n",
    "                          if f.startswith(\"batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    # Dictionary to store examples by QID and label type\n",
    "    examples_by_qid = defaultdict(lambda: {\"multi\": [], \"single\": []})\n",
    "    \n",
    "    # Scan batch files to find examples\n",
    "    for batch_file in batch_files:\n",
    "        file_path = os.path.join(processed_data_dir, batch_file)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            # Filter by requested QIDs\n",
    "            for sample in batch_data:\n",
    "                sample_qid = sample['qid']\n",
    "                if any(sample_qid.startswith(qid) for qid in qids):\n",
    "                    # Check if it's multi-label\n",
    "                    is_multi_label = ',' in sample['answer_text']\n",
    "                    \n",
    "                    # Store in appropriate category\n",
    "                    if is_multi_label and multi_label_examples:\n",
    "                        examples_by_qid[sample_qid][\"multi\"].append(sample)\n",
    "                    elif not is_multi_label and single_label_examples:\n",
    "                        examples_by_qid[sample_qid][\"single\"].append(sample)\n",
    "            \n",
    "            # Check if we've found enough examples for each QID\n",
    "            all_found = True\n",
    "            for qid in qids:\n",
    "                qid_examples = [key for key in examples_by_qid.keys() if key.startswith(qid)]\n",
    "                if not qid_examples:\n",
    "                    all_found = False\n",
    "                    break\n",
    "                    \n",
    "                for q in qid_examples:\n",
    "                    if multi_label_examples and len(examples_by_qid[q][\"multi\"]) < num_samples_per_qid:\n",
    "                        if not examples_by_qid[q][\"multi\"]:  # Only if we have no examples yet\n",
    "                            all_found = False\n",
    "                    if single_label_examples and len(examples_by_qid[q][\"single\"]) < num_samples_per_qid:\n",
    "                        if not examples_by_qid[q][\"single\"]:  # Only if we have no examples yet\n",
    "                            all_found = False\n",
    "            \n",
    "            if all_found:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {batch_file}: {e}\")\n",
    "    \n",
    "    # Display the examples\n",
    "    sample_count = 0\n",
    "    \n",
    "    for qid in qids:\n",
    "        matching_qids = [key for key in examples_by_qid.keys() if key.startswith(qid)]\n",
    "        \n",
    "        if not matching_qids:\n",
    "            print(f\"\\nNo examples found for QID {qid}\")\n",
    "            continue\n",
    "            \n",
    "        for matching_qid in matching_qids:\n",
    "            # Display multi-label examples\n",
    "            if multi_label_examples and examples_by_qid[matching_qid][\"multi\"]:\n",
    "                examples = examples_by_qid[matching_qid][\"multi\"][:num_samples_per_qid]\n",
    "                for i, sample in enumerate(examples):\n",
    "                    sample_count += 1\n",
    "                    display_llm_training_input(sample, sample_count, show_images)\n",
    "            \n",
    "            # Display single-label examples\n",
    "            if single_label_examples and examples_by_qid[matching_qid][\"single\"]:\n",
    "                examples = examples_by_qid[matching_qid][\"single\"][:num_samples_per_qid]\n",
    "                for i, sample in enumerate(examples):\n",
    "                    sample_count += 1\n",
    "                    display_llm_training_input(sample, sample_count, show_images)\n",
    "    \n",
    "    if sample_count == 0:\n",
    "        print(\"No matching examples found. Try different QIDs or check your data directory.\")\n",
    "\n",
    "def display_llm_training_input(sample, sample_num, show_images=True):\n",
    "    \"\"\"\n",
    "    Display a single example formatted exactly as it would be sent to the LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"EXAMPLE #{sample_num}: {sample['qid']}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"Question ID: {sample['qid']}\")\n",
    "    print(f\"Type: {sample['question_type']}\")\n",
    "    print(f\"Category: {sample['question_category']}\")\n",
    "    print(f\"Image path: {sample['image_path']}\")\n",
    "    \n",
    "    # Determine if multi-label\n",
    "    is_multi_label = ',' in sample['answer_text']\n",
    "    print(f\"Multi-label: {is_multi_label}\")\n",
    "    \n",
    "    # Show the image if requested\n",
    "    if show_images and os.path.exists(sample['image_path']):\n",
    "        try:\n",
    "            img = Image.open(sample['image_path'])\n",
    "            width, height = img.size\n",
    "            print(f\"Image dimensions: {width}x{height}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image for {os.path.basename(sample['image_path'])}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image: {e}\")\n",
    "    \n",
    "    # Define the system message as in your MedicalImageDataset class   \n",
    "    system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "    IMPORTANT: \n",
    "    - Respond ONLY with the exact text of the option(s) that apply\n",
    "    - Do not provide any explanations\n",
    "    - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "    - Do not write \"Options:\" or similar prefixes\n",
    "    - Do not write \"Answer:\" or similar prefixes\n",
    "    - Multiple answers should be separated by commas\n",
    "    - If unsure, respond with \"Not mentioned\"\n",
    "    - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "    - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format as would be sent to the LLM\n",
    "    print(\"\\nACTUAL LLM INPUT (as formatted in your MedicalImageDataset):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\"System message:\")\n",
    "    print(system_message)\n",
    "    print(\"\\nUser message:\")\n",
    "    print(sample['query_text'])\n",
    "    print(\"[IMAGE WOULD BE INCLUDED HERE]\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Expected output\n",
    "    print(\"\\nEXPECTED LLM OUTPUT:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(sample['answer_text'])\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # If multi-label, show split answers\n",
    "    if is_multi_label:\n",
    "        answers = [ans.strip() for ans in sample['answer_text'].split(\",\")]\n",
    "#         print(f\"\\nSplit answers:\")\n",
    "#         for ans in answers:\n",
    "#             print(f\"- {ans}\")\n",
    "    \n",
    "    # Show what this would look like after chat template is applied\n",
    "    print(\"\\nCHAT TEMPLATE FORMAT (approximate):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(\"<bos><begin_of_system>\\nYou are a medical image analysis assistant. Your task is to examine the provided clinical images along with clinical context, and select the option(s) that best describe what you see. \\nIMPORTANT: You must respond ONLY with the exact text of the option(s) that apply. \\n- Do not provide any explanations\\n- Do not include option numbers\\n- Do not write \\\"Options:\\\" or similar prefixes\\n- Do not write \\\"Answer:\\\" or similar prefixes\\n- Multiple answers should be separated by commas\\n- If unsure, respond with \\\"Not mentioned\\\"\\n<end_of_system>\\n\\n<begin_of_user>\\n\" + sample['query_text'] + \"\\n<boi>IMAGE_EMBEDDING_TOKENS<eoi>\\n<end_of_user>\\n\\n<begin_of_assistant>\\n\" + sample['answer_text'] + \"<end_of_assistant>\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Example usage\n",
    "inspect_llm_training_inputs(\n",
    "    processed_data_dir=\"outputs/processed_data\",  # Update with your directory\n",
    "    qids=[\"CQID034\", \"CQID012\", \"CQID011\"],      # QIDs to look for\n",
    "    multi_label_examples=True,                    # Include multi-label examples\n",
    "    single_label_examples=True,                   # Include single-label examples\n",
    "    num_samples_per_qid=1,                        # Number of examples per QID\n",
    "    show_images=True                              # Show the images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Open image and convert to RGB\n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        # Define system message for medical image analysis\n",
    "        \n",
    "        system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "        IMPORTANT: \n",
    "        - Respond ONLY with the exact text of the option(s) that apply\n",
    "        - Do not provide any explanations\n",
    "        - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\"\n",
    "        - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "        - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "        \"\"\"\n",
    "        \n",
    "        # Format as a conversation with system, user, and assistant messages\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face token if needed\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # Make sure to set this in a .env file or environment\n",
    "\n",
    "# Set model ID\n",
    "model_id = \"google/gemma-3-4b-it\"  # We'll use Gemma 3 4B with image understanding capabilities\n",
    "\n",
    "# Load processor first to use in the dataset class\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions\n",
    "def create_dummy_image():\n",
    "    \"\"\"Create a small black image as a placeholder.\"\"\"\n",
    "    return Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function for batching examples.\"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Extract image from messages\n",
    "        image_input = None\n",
    "        for msg in example[\"messages\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                for content in msg[\"content\"]:\n",
    "                    if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                        image_input = content[\"image\"]\n",
    "                        break\n",
    "        \n",
    "        if image_input is None:\n",
    "            image_input = create_dummy_image()\n",
    "            \n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        texts.append(text.strip())\n",
    "        images.append([image_input])\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts, \n",
    "        images=images,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get token IDs for special tokens to mask in loss computation\n",
    "    boi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    eoi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"eoi_token\"]\n",
    "    )\n",
    "    \n",
    "    # Mask tokens that shouldn't contribute to the loss\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == boi_token_id] = -100\n",
    "    labels[labels == eoi_token_id] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the dataset\n",
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Confirm we are correctly loading examples from the processed data\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    # Display the message structure for each sample\n",
    "    for i in range(sample_size):\n",
    "        example = dataset[i]\n",
    "        print(f\"\\nExample {i+1} message structure:\")\n",
    "        print(example)\n",
    "#         print(f\"System role: {example['messages'][0]}\")\n",
    "#         print(f\"User role content:\")\n",
    "#         print(f\"  {example['messages'][1]['content'][0]['text'][:200]}...\")  # Show first 200 chars\n",
    "#         print(f\"  Image: {type(example['messages'][1]['content'][1]['image'])}\")\n",
    "#         print(f\"Assistant role: {example['messages'][2]}\")\n",
    "    \n",
    "    # Test the collate function\n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"\\nCollated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c64995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    # Process each batch. Note: in training, pass this to model.forward()\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(torch_dtype=None):\n",
    "    \"\"\"Create standardized model configuration dictionary\"\"\"\n",
    "    if torch_dtype is None:\n",
    "        torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        \n",
    "    # Configure model parameters\n",
    "    model_kwargs = dict(\n",
    "        attn_implementation=\"eager\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    # Configure quantization for memory efficiency\n",
    "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "        bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"]\n",
    "    )\n",
    "    \n",
    "    return model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU can support bfloat16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8:\n",
    "    print(\"WARNING: GPU may not fully support bfloat16. Consider using float16 instead.\")\n",
    "\n",
    "# Get standardized model configuration\n",
    "model_kwargs = get_model_config(torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs, token=hf_token)\n",
    "\n",
    "# Print info about the chat template\n",
    "print(f\"Default chat template: {processor.tokenizer.chat_template}\")\n",
    "print(f\"Special tokens map: {processor.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32722ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",  # Apply LoRA to all linear layers\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],  # Save these modules fully\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08624c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model name from model_id\n",
    "model_name = model_id.split('/')[-1]  # Gets \"gemma-3-4b-it\" from \"google/gemma-3-4b-it\"\n",
    "\n",
    "# Add timestamp for uniqueness\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Set up training configuration\n",
    "output_dir = f\"outputs/finetuned-model/{model_name}_{timestamp}\"\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,  # Adjust based on dataset size\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients to simulate larger batch\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hub\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,  # Critical for custom datasets\n",
    "    label_names=[\"labels\"],  # Explicitly setting label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with all components\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ec859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the latest training output directory\n",
    "def find_latest_training_dir(base_dir=\"outputs\"):\n",
    "    # Find all finetuned model directories with the new structure\n",
    "    model_dirs = glob.glob(os.path.join(base_dir, \"finetuned-model\", \"*\"))\n",
    "    if not model_dirs:\n",
    "        # Fall back to old format if none found with new structure\n",
    "        model_dirs = glob.glob(os.path.join(base_dir, \"finetuned-model*\"))\n",
    "        \n",
    "    if not model_dirs:\n",
    "        raise FileNotFoundError(\"No finetuned model directories found\")\n",
    "    \n",
    "    # Get the most recent directory\n",
    "    latest_dir = max(model_dirs, key=os.path.getmtime)\n",
    "    return latest_dir\n",
    "\n",
    "def plot_training_loss(training_dir=None, window_size=10):\n",
    "    # Find the latest training directory if not provided\n",
    "    if training_dir is None:\n",
    "        training_dir = find_latest_training_dir()\n",
    "    \n",
    "    # Extract model name and timestamp from directory name\n",
    "    dir_name = os.path.basename(training_dir)\n",
    "    \n",
    "    # Find the tensorboard logs directory\n",
    "    log_dir = os.path.join(training_dir, \"runs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise FileNotFoundError(f\"TensorBoard logs not found in {log_dir}\")\n",
    "    \n",
    "    # Find all event files\n",
    "    event_files = []\n",
    "    for root, dirs, files in os.walk(log_dir):\n",
    "        for file in files:\n",
    "            if file.startswith(\"events.out.tfevents\"):\n",
    "                event_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(\"No TensorBoard event files found\")\n",
    "    \n",
    "    # Get the most recent event file\n",
    "    latest_event = max(event_files, key=os.path.getmtime)\n",
    "    \n",
    "    # Load the events using TensorBoard's event accumulator\n",
    "    ea = event_accumulator.EventAccumulator(os.path.dirname(latest_event))\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Check available tags\n",
    "    tags = ea.Tags()\n",
    "    loss_tag = None\n",
    "    for tag in tags['scalars']:\n",
    "        if 'loss' in tag.lower():\n",
    "            loss_tag = tag\n",
    "            break\n",
    "    \n",
    "    if not loss_tag:\n",
    "        raise ValueError(\"No loss data found in TensorBoard logs\")\n",
    "    \n",
    "    # Extract the training loss data\n",
    "    train_loss_steps = []\n",
    "    train_loss_values = []\n",
    "    for event in ea.Scalars(loss_tag):\n",
    "        train_loss_steps.append(event.step)\n",
    "        train_loss_values.append(event.value)\n",
    "    \n",
    "    # Create a DataFrame for easier manipulation\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Step': train_loss_steps,\n",
    "        'Training Loss': train_loss_values\n",
    "    })\n",
    "    \n",
    "    # Plot the training loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_df['Step'], loss_df['Training Loss'], label='Training Loss')\n",
    "    plt.title(f'Training Loss Curve for {dir_name}')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add moving average to smooth the curve if there are enough data points\n",
    "    if len(loss_df) > window_size:\n",
    "        loss_df['Moving Avg'] = loss_df['Training Loss'].rolling(window=window_size).mean()\n",
    "        plt.plot(loss_df['Step'], loss_df['Moving Avg'], 'r-', \n",
    "                 label=f'Moving Average (window={window_size})')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(\"outputs\", f\"training_loss_{dir_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training loss plot saved to {plot_path}\")\n",
    "    \n",
    "    # Print a summary table of the loss values\n",
    "    step_interval = max(1, len(loss_df) // 20)  # Show ~20 rows\n",
    "    print(\"\\nTraining Loss Summary:\")\n",
    "    print(\"Step\\tTraining Loss\\tMoving Avg\")\n",
    "    for i in range(0, len(loss_df), step_interval):\n",
    "        if 'Moving Avg' in loss_df.columns:\n",
    "            print(f\"{loss_df.iloc[i]['Step']:.0f}\\t{loss_df.iloc[i]['Training Loss']:.4f}\\t{loss_df.iloc[i]['Moving Avg']:.5f}\")\n",
    "        else:\n",
    "            print(f\"{loss_df.iloc[i]['Step']:.0f}\\t{loss_df.iloc[i]['Training Loss']:.4f}\\tNaN\")\n",
    "    \n",
    "    return loss_df\n",
    "\n",
    "# Example usage:\n",
    "plot_training_loss()  # Automatically find and plot the latest training run\n",
    "# Or specify a specific training directory:\n",
    "# plot_training_loss(\"outputs/finetuned-model_gemma-3-4b-it_20250419_0945\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_model(base_model_id, checkpoint_path=None, token=None, output_dir=None):\n",
    "    \"\"\"Merge LoRA weights into base model and save to output directory\"\"\"\n",
    "    # Extract model name from base_model_id\n",
    "    model_name = base_model_id.split('/')[-1]\n",
    "    \n",
    "    # If checkpoint_path is not provided, find the latest checkpoint\n",
    "    if checkpoint_path is None:\n",
    "        \n",
    "        # Look for checkpoints in the new directory structure\n",
    "        checkpoint_pattern = f\"outputs/finetuned-model/{model_name}_*/checkpoint-*\"\n",
    "        checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "        \n",
    "        # Fall back to old structure if no checkpoints found\n",
    "        if not checkpoint_dirs:\n",
    "            checkpoint_pattern = f\"outputs/finetuned-model_{model_name}_*/checkpoint-*\"\n",
    "            checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "        \n",
    "        if not checkpoint_dirs:\n",
    "            raise FileNotFoundError(f\"No checkpoints found matching pattern {checkpoint_pattern}\")\n",
    "        \n",
    "        # Sort by checkpoint number (assuming format checkpoint-XXXX)\n",
    "        checkpoint_dirs = sorted(checkpoint_dirs, \n",
    "                                key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)), \n",
    "                                reverse=True)\n",
    "        \n",
    "        checkpoint_path = checkpoint_dirs[0]\n",
    "        print(f\"Using latest checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Create a descriptive output directory if none provided\n",
    "    if output_dir is None:\n",
    "        checkpoint_name = os.path.basename(checkpoint_path)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        output_dir = f\"outputs/merged/{model_name}_{checkpoint_name}_{timestamp}\"\n",
    "    \n",
    "    # Get standardized model configuration for base model\n",
    "    model_kwargs = get_model_config(torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    # Load base model with token\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        base_model_id, \n",
    "        low_cpu_mem_usage=True,\n",
    "        **model_kwargs,\n",
    "        token=token\n",
    "    )\n",
    "    \n",
    "    # Merge LoRA weights into base model\n",
    "    peft_model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    # Save the merged model\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    merged_model.save_pretrained(output_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "    \n",
    "    # Save the processor alongside the model\n",
    "    processor = AutoProcessor.from_pretrained(checkpoint_path)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del model\n",
    "    del peft_model\n",
    "    del merged_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Merged model saved to {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.test = False\n",
    "        self.skip_data_prep = False\n",
    "        self.batch_size = 100\n",
    "        self.max_samples = None\n",
    "        \n",
    "        # Model setup options\n",
    "        self.use_merged_model = True\n",
    "        self.base_model_id = \"google/gemma-3-4b-it\"\n",
    "        \n",
    "        # Set model path based on configuration\n",
    "        if self.use_merged_model:\n",
    "            # Extract model name from base_model_id\n",
    "            model_name = self.base_model_id.split('/')[-1]\n",
    "            \n",
    "            # Find the latest merged model\n",
    "            merged_pattern = f\"outputs/merged/{model_name}_*\"\n",
    "            merged_dirs = glob.glob(merged_pattern)\n",
    "            \n",
    "            if not merged_dirs:\n",
    "                # Fall back to old directory structure if needed\n",
    "                merged_pattern = f\"outputs/merged_{model_name}_*\"\n",
    "                merged_dirs = glob.glob(merged_pattern)\n",
    "            \n",
    "            if merged_dirs:\n",
    "                # Use the most recently created merged model\n",
    "                self.model_path = sorted(merged_dirs, key=os.path.getctime, reverse=True)[0]\n",
    "                print(f\"Using existing merged model at {self.model_path}\")\n",
    "            else:\n",
    "                # Find the latest checkpoint in new directory structure\n",
    "                checkpoint_pattern = f\"outputs/finetuned-model/{model_name}_*/checkpoint-*\"\n",
    "                checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "                \n",
    "                if not checkpoint_dirs:\n",
    "                    # Fall back to old directory structure\n",
    "                    checkpoint_pattern = f\"outputs/finetuned-model_{model_name}_*/checkpoint-*\"\n",
    "                    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "                \n",
    "                if not checkpoint_dirs:\n",
    "                    raise FileNotFoundError(f\"No checkpoints found for model {model_name}\")\n",
    "                \n",
    "                # Sort by checkpoint number\n",
    "                checkpoint_dirs = sorted(checkpoint_dirs, \n",
    "                                        key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)), \n",
    "                                        reverse=True)\n",
    "                \n",
    "                checkpoint_path = checkpoint_dirs[0]\n",
    "                print(f\"Creating merged model from latest checkpoint: {checkpoint_path}\")\n",
    "                \n",
    "                # Create a unique merged model directory\n",
    "                checkpoint_name = os.path.basename(checkpoint_path)\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "                merged_dir = f\"outputs/merged/{model_name}_{checkpoint_name}_{timestamp}\"\n",
    "                \n",
    "                self.model_path = merge_lora_model(\n",
    "                    self.base_model_id,\n",
    "                    checkpoint_path=checkpoint_path,\n",
    "                    token=hf_token,\n",
    "                    output_dir=merged_dir\n",
    "                )\n",
    "                print(f\"Using new merged model at {self.model_path}\")\n",
    "        else:\n",
    "            # Use base model directly\n",
    "            self.model_path = self.base_model_id\n",
    "            print(f\"Using BASE MODEL ONLY at {self.model_path} (no fine-tuning applied)\")\n",
    "        \n",
    "        print(\"\\nModel for inference:\", \"FINE-TUNED\" if self.use_merged_model else \"BASE MODEL (NO FINE-TUNING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Args and see which model is being used\n",
    "args = Args()\n",
    "print(f\"Selected model path: {args.model_path}\")\n",
    "print(f\"Using fine-tuned model: {args.use_merged_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_lora_model(base_model_id, checkpoint_path, token=None, output_dir=\"outputs/merged_model\"):\n",
    "#     \"\"\"Merge LoRA weights into base model and save to output directory\"\"\"\n",
    "#     # Get standardized model configuration for base model\n",
    "#     model_kwargs = get_model_config(torch_dtype=torch.bfloat16)\n",
    "    \n",
    "#     # Load base model with token\n",
    "#     model = AutoModelForImageTextToText.from_pretrained(\n",
    "#         base_model_id, \n",
    "#         low_cpu_mem_usage=True,\n",
    "#         **model_kwargs,\n",
    "#         token=token  # Add token here\n",
    "#     )\n",
    "    \n",
    "#     # Merge LoRA weights into base model\n",
    "#     peft_model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "#     merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "#     # Save the merged model\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     merged_model.save_pretrained(output_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "    \n",
    "#     # Save the processor alongside the model\n",
    "#     processor = AutoProcessor.from_pretrained(checkpoint_path)\n",
    "#     processor.save_pretrained(output_dir)\n",
    "    \n",
    "#     # Clean up memory\n",
    "#     del model\n",
    "#     del peft_model\n",
    "#     del merged_model\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     print(f\"Merged model saved to {output_dir}\")\n",
    "#     return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.test = False  # Set to False to run on full dataset and True to run in test mode\n",
    "#         self.skip_data_prep = False  # Set to False to process all data and True to skip data prep (not recommended)\n",
    "#         self.batch_size = 100\n",
    "#         self.max_samples = None  # No limit on samples # Set to i.e.,50 for just 50 samples for inference\n",
    "        \n",
    "#         # Model setup options\n",
    "#         self.use_merged_model = False\n",
    "#         self.base_model_id = \"google/gemma-3-4b-it\"\n",
    "#         self.checkpoint_path = \"outputs/finetuned-model/checkpoint-2958\"\n",
    "        \n",
    "#         # Set model path based on configuration\n",
    "#         if self.use_merged_model:\n",
    "#             # Check if merged model exists, otherwise create it\n",
    "#             merged_dir = \"outputs/merged_model\"\n",
    "#             if not os.path.exists(merged_dir):\n",
    "#                 print(f\"Creating merged model from base model {self.base_model_id} and checkpoint {self.checkpoint_path}\")\n",
    "#                 self.model_path = merge_lora_model(\n",
    "#                     self.base_model_id, \n",
    "#                     self.checkpoint_path,\n",
    "#                     token=hf_token,\n",
    "#                     output_dir=merged_dir\n",
    "#                 )\n",
    "#                 print(f\"Using FINE-TUNED model (newly created) at {self.model_path}\")\n",
    "#             else:\n",
    "#                 self.model_path = merged_dir\n",
    "#                 print(f\"Using FINE-TUNED model (existing) at {self.model_path}\")\n",
    "#         else:\n",
    "#             # Use base model directly\n",
    "#             self.model_path = self.base_model_id\n",
    "#             print(f\"Using BASE MODEL ONLY at {self.model_path} (no fine-tuning applied)\")\n",
    "        \n",
    "#         # Print a clear summary\n",
    "#         print(\"\\nModel for inference:\", \"FINE-TUNED\" if self.use_merged_model else \"BASE MODEL (NO FINE-TUNING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare validation data\n",
    "def prepare_validation_data():\n",
    "    \"\"\"\n",
    "    Create a validation dataframe similar to the training dataframe.\n",
    "    \"\"\"\n",
    "    print(\"Preparing validation data...\")\n",
    "    \n",
    "    # Load question definitions\n",
    "    questions_path = os.path.join(TRAIN_DIR, \"closedquestions_definitions_imageclef2025.json\")\n",
    "    with open(questions_path, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "        \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    questions_df = pd.json_normalize(questions)[[\"qid\", \"question_en\", \"options_en\", \"question_type_en\", \"question_category_en\"]]\n",
    "    \n",
    "    # Load validation data with query information\n",
    "    val_json_path = os.path.join(VAL_DIR, \"valid.json\")\n",
    "    val_df = pd.read_json(val_json_path)\n",
    "    \n",
    "    # Extract relevant columns including query content and title\n",
    "    query_info_df = val_df[[\"encounter_id\", \"image_ids\", \"query_title_en\", \"query_content_en\", \"author_id\"]]\n",
    "    \n",
    "    # Load CVQA data (ground truth answers)\n",
    "    cvqa_path = os.path.join(VAL_DIR, \"valid_cvqa.json\")\n",
    "    with open(cvqa_path, 'r') as f:\n",
    "        cvqa_data = json.load(f)\n",
    "    cvqa_df = pd.json_normalize(cvqa_data)\n",
    "    \n",
    "    # Melt to get one row per question\n",
    "    cvqa_long = cvqa_df.melt(id_vars=[\"encounter_id\"], \n",
    "                             var_name=\"qid\", \n",
    "                             value_name=\"answer_index\")\n",
    "    \n",
    "    # Filter out encounter_id rows\n",
    "    cvqa_long = cvqa_long[cvqa_long[\"qid\"] != \"encounter_id\"]\n",
    "    \n",
    "    # Merge CVQA with questions\n",
    "    cvqa_merged = cvqa_long.merge(questions_df, on=\"qid\", how=\"left\")\n",
    "    \n",
    "    # Get answer text\n",
    "    def get_answer_text(row):\n",
    "        try:\n",
    "            return row[\"options_en\"][row[\"answer_index\"]]\n",
    "        except (IndexError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    cvqa_merged[\"answer_text\"] = cvqa_merged.apply(get_answer_text, axis=1)\n",
    "    \n",
    "    # Merge with validation data\n",
    "    final_df = cvqa_merged.merge(query_info_df, on=\"encounter_id\", how=\"left\")\n",
    "    \n",
    "    # Extract the base CQID code\n",
    "    final_df['base_qid'] = final_df['qid'].str.extract(r'(CQID\\d+)')\n",
    "    \n",
    "    # Group by encounter_id and base_qid to see all answers for each question family\n",
    "    grouped_by_family = final_df.groupby(['encounter_id', 'base_qid']).agg({\n",
    "        'qid': list,\n",
    "        'question_en': list,\n",
    "        'answer_text': list,\n",
    "        'answer_index': list,\n",
    "        'image_ids': 'first',\n",
    "        'options_en': 'first',\n",
    "        'question_type_en': 'first',\n",
    "        'question_category_en': 'first',\n",
    "        'query_title_en': 'first',\n",
    "        'query_content_en': 'first',\n",
    "        'author_id': 'first'\n",
    "    })\n",
    "    \n",
    "    # Reset index for easier manipulation\n",
    "    grouped_by_family = grouped_by_family.reset_index()\n",
    "       \n",
    "    def get_valid_answers(row):\n",
    "        \"\"\"\n",
    "        Extract all valid answers, with special handling for \"Not mentioned\".\n",
    "        If \"Not mentioned\" is the only answer for all slots, we keep it.\n",
    "        Otherwise, we collect all non-\"Not mentioned\" answers.\n",
    "        \"\"\"\n",
    "        answers = row['answer_text']\n",
    "        answer_indices = row['answer_index']\n",
    "\n",
    "        if all(ans == \"Not mentioned\" for ans in answers):\n",
    "            return [\"Not mentioned\"], [answer_indices[0]]  # If all are \"Not mentioned\", return it as valid\n",
    "\n",
    "        valid_answers = []\n",
    "        valid_indices = []\n",
    "\n",
    "        for i, ans in enumerate(answers):\n",
    "            if ans != \"Not mentioned\":\n",
    "                # Clean the answer string by removing quotes and extra whitespace\n",
    "                if isinstance(ans, str):\n",
    "                    cleaned_ans = ans.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                    # Only add if not already in valid_answers (after cleaning)\n",
    "                    if cleaned_ans not in valid_answers:\n",
    "                        valid_answers.append(cleaned_ans)\n",
    "                        valid_indices.append(answer_indices[i])\n",
    "                else:\n",
    "                    # Handle non-string answers\n",
    "                    str_ans = str(ans).strip(\"'\\\" \")\n",
    "                    if str_ans not in valid_answers:\n",
    "                        valid_answers.append(str_ans)\n",
    "                        valid_indices.append(answer_indices[i])\n",
    "\n",
    "        return valid_answers, valid_indices\n",
    "    \n",
    "    # Apply to all question families\n",
    "    grouped_by_family[['valid_answers', 'valid_indices']] = grouped_by_family.apply(\n",
    "        lambda row: pd.Series(get_valid_answers(row)), axis=1)\n",
    "    \n",
    "    # Create the multi-label validation dataset\n",
    "    multi_label_data = []\n",
    "    \n",
    "    # Process all validation encounters\n",
    "    for _, row in tqdm(grouped_by_family.iterrows(), desc=\"Creating validation dataset\"):\n",
    "        encounter_id = row['encounter_id']\n",
    "        base_qid = row['base_qid']\n",
    "        valid_answers = row['valid_answers']\n",
    "        valid_indices = row['valid_indices']\n",
    "        image_ids = row['image_ids']\n",
    "        question_text = row['question_en'][0]  # Taking the first question as reference\n",
    "        query_title = row['query_title_en']\n",
    "        query_content = row['query_content_en']\n",
    "        author_id = row['author_id']\n",
    "        options_en = row['options_en']\n",
    "        question_type_en = row['question_type_en']\n",
    "        question_category_en = row['question_category_en']\n",
    "        \n",
    "        # For each image in the encounter\n",
    "        for img_id in image_ids:\n",
    "            img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n",
    "            \n",
    "            # Skip if image doesn't exist\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image {img_id} not found at {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            multi_label_data.append({\n",
    "                'encounter_id': encounter_id,\n",
    "                'base_qid': base_qid,\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path,\n",
    "                'valid_answers': valid_answers,\n",
    "                'valid_indices': valid_indices,\n",
    "                'question_text': question_text,\n",
    "                'query_title_en': query_title,\n",
    "                'query_content_en': query_content,\n",
    "                'author_id': author_id,\n",
    "                'options_en': options_en,\n",
    "                'question_type_en': question_type_en, \n",
    "                'question_category_en': question_category_en,\n",
    "                'is_multi_label': len(valid_answers) > 1\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    val_dataset = pd.DataFrame(multi_label_data)\n",
    "    \n",
    "    # Save the dataset\n",
    "    val_dataset.to_csv(os.path.join(OUTPUT_DIR, \"val_dataset.csv\"), index=False)\n",
    "    \n",
    "    print(f\"Validation dataset created with {len(val_dataset)} entries\")\n",
    "    \n",
    "    return val_dataset\n",
    "\n",
    "# Function to process a batch for inference\n",
    "def process_inference_batch(batch_df, batch_idx, save_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Process a batch of data samples for inference and save them as a pickle file.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Get image path\n",
    "            image_id = row.get('image_id')\n",
    "            if not image_id:\n",
    "                continue\n",
    "                \n",
    "            # Use the full image path if it's already in the dataframe\n",
    "            if 'image_path' in row and os.path.exists(row['image_path']):\n",
    "                image_path = row['image_path']\n",
    "            else:\n",
    "                # Otherwise construct from images_dir and image_id\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} â€” {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Get options from options_en\n",
    "            if 'options_en' in row:\n",
    "                options = safe_convert_options(row['options_en'])\n",
    "                \n",
    "                # Clean up options by removing \"(please specify)\" phrases\n",
    "                cleaned_options = []\n",
    "                for opt in options:\n",
    "                    if isinstance(opt, str):\n",
    "                        # Remove \"(please specify)\" from option text\n",
    "#                         cleaned_opt = opt.replace(\" (please specify)\", \"\")\n",
    "                        cleaned_opt = opt.strip(\"'\\\" \").replace(\" (please specify)\", \"\")\n",
    "                        cleaned_options.append(cleaned_opt)\n",
    "                    else:\n",
    "#                         cleaned_options.append(opt)\n",
    "                        cleaned_options.append(str(opt).strip(\"'\\\" \"))\n",
    "                options = cleaned_options\n",
    "            else:\n",
    "                options = [\"Yes\", \"No\", \"Not mentioned\"]\n",
    "                print(\"Error with parsing options\")\n",
    "                \n",
    "            options_text = \", \".join(options)\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = \"\"\n",
    "            if 'question_type_en' in row:\n",
    "                metadata += f\"Type: {row['question_type_en']}\"\n",
    "                \n",
    "            if 'question_category_en' in row:\n",
    "                metadata += f\", Category: {row['question_category_en']}\"\n",
    "            \n",
    "            # Get question text and clean it\n",
    "            question = row.get('question_text', 'What do you see in this image?')\n",
    "            \n",
    "            # Remove \"Please specify which affected area for each selection.\" from CQID012\n",
    "            if \"Please specify which affected area for each selection\" in question:\n",
    "                question = question.replace(\" Please specify which affected area for each selection.\", \"\")\n",
    "            \n",
    "            # Remove leading numbers like \"1 \" from the beginning of questions\n",
    "            question = re.sub(r'^\\d+\\s+', '', question)\n",
    "            \n",
    "            # Get clinical context from query title and content\n",
    "            query_title = row.get('query_title_en', '')\n",
    "            query_content = row.get('query_content_en', '')\n",
    "                       \n",
    "            # Create the clinical context section\n",
    "            clinical_context = \"\"\n",
    "            if query_title or query_content:\n",
    "                clinical_context += \"Background Clinical Information (to help with your analysis):\\n\"\n",
    "                if query_title:\n",
    "                    clinical_context += f\"{query_title}\\n\"\n",
    "                if query_content:\n",
    "                    clinical_context += f\"{query_content}\\n\"\n",
    "\n",
    "            # Create the full query text with clinical context\n",
    "            query_text = (f\"MAIN QUESTION TO ANSWER: {question}\\n\"\n",
    "                         f\"Question Metadata: {metadata}\\n\"\n",
    "                         f\"{clinical_context}\"\n",
    "                         f\"Available Options (choose from these): {options_text}\")\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"id\": row.get('encounter_id', str(idx)),\n",
    "                \"qid\": row.get('base_qid', ''),\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"val_batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)\n",
    "\n",
    "def preprocess_validation_dataset(df, batch_size=50, save_dir=\"outputs/processed_val_data\", images_dir=None):\n",
    "    \"\"\"\n",
    "    Process the entire validation dataset in batches\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Use VAL_IMAGES_DIR global variable if images_dir is not provided\n",
    "    if images_dir is None:\n",
    "        images_dir = VAL_IMAGES_DIR\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_inference_batch(batch_df, batch_idx, save_dir, images_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageInference:\n",
    "    def __init__(self, model_path, token=None, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path, token=token)\n",
    "        \n",
    "        # Get standardized model configuration\n",
    "        model_kwargs = get_model_config()\n",
    "        \n",
    "        # Load the model with the same configuration\n",
    "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "            model_path, \n",
    "            **model_kwargs,\n",
    "            token=token  # Pass token here as well\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "    def predict(self, query_text, image_path, max_new_tokens=100):\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Create the system message\n",
    "\n",
    "            system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "            IMPORTANT: \n",
    "            - Respond ONLY with the exact text of the option(s) that apply\n",
    "            - Do not provide any explanations\n",
    "            - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "            - Do not write \"Options:\" or similar prefixes\n",
    "            - Do not write \"Answer:\" or similar prefixes\n",
    "            - Multiple answers should be separated by commas\n",
    "            - If unsure, respond with \"Not mentioned\"\n",
    "            - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "            - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "            \"\"\"\n",
    "            \n",
    "            # Format as a conversation with system and user messages\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query_text},\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Create model inputs\n",
    "            inputs = self.processor(\n",
    "                text=self.processor.apply_chat_template(messages, tokenize=False),\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate prediction\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "            # Get only the new tokens (the model's answer)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = generated_ids[0][input_length:]\n",
    "\n",
    "            # Decode only the new tokens\n",
    "            prediction = self.processor.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # Clean the prediction - remove any remaining template artifacts\n",
    "            prediction = prediction.strip()\n",
    "            if prediction.startswith(\"model\\n\"):\n",
    "                prediction = prediction[len(\"model\\n\"):]\n",
    "            \n",
    "            # Extract just the answer text\n",
    "            if \"Answer:\" in prediction:\n",
    "                parts = prediction.split(\"Answer:\")\n",
    "                if len(parts) > 1:\n",
    "                    prediction = parts[1].strip()\n",
    "                \n",
    "            if prediction.startswith(\"<start_of_turn>model\") or prediction.startswith(\"<start_of_turn>assistant\"):\n",
    "                prediction = prediction.split(\"\\n\", 1)[1] if \"\\n\" in prediction else \"\"\n",
    "            if prediction.endswith(\"<end_of_turn>\"):\n",
    "                prediction = prediction[:-len(\"<end_of_turn>\")]\n",
    "\n",
    "            return prediction.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {image_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return \"Not mentioned\"  # Default to not mentioned in case of errors\n",
    "    \n",
    "    def batch_predict(self, processed_data_dir, output_file, max_samples=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of preprocessed data\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        sample_count = 0\n",
    "        \n",
    "        # Load all batch files\n",
    "        batch_files = sorted([f for f in os.listdir(processed_data_dir) if f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "        \n",
    "        # Process each batch file\n",
    "        for batch_file in tqdm(batch_files, desc=\"Processing batches\"):\n",
    "            with open(os.path.join(processed_data_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for sample in tqdm(batch_data, desc=f\"Predicting {batch_file}\", leave=False):\n",
    "                # Get prediction\n",
    "                prediction = self.predict(sample[\"query_text\"], sample[\"image_path\"])\n",
    "                \n",
    "                # Save results\n",
    "                results.append({\n",
    "                    \"encounter_id\": sample[\"id\"],\n",
    "                    \"base_qid\": sample[\"qid\"],\n",
    "                    \"image_id\": os.path.basename(sample[\"image_path\"]),\n",
    "                    \"prediction\": prediction\n",
    "                })\n",
    "                \n",
    "                sample_count += 1\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "                \n",
    "            if max_samples and sample_count >= max_samples:\n",
    "                break\n",
    "        \n",
    "        # Convert to DataFrame and save\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    # Update the aggregate_predictions method in the MedicalImageInference class\n",
    "    def aggregate_predictions(self, predictions_df, validation_df=None):\n",
    "        \"\"\"\n",
    "        Aggregate predictions for each encounter and question ID\n",
    "        For each encounter-question pair, collect unique predictions across all images,\n",
    "        respecting the maximum allowed answers for each question type.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions_df: DataFrame with prediction results\n",
    "        - validation_df: Optional DataFrame containing validation data with options_en\n",
    "        \"\"\"\n",
    "        # Define maximum allowed answers for each question type\n",
    "        max_answers = {\n",
    "            'CQID010': 1,  # Single answer\n",
    "            'CQID011': 6,  # Up to 6 answers\n",
    "            'CQID012': 6,  # Up to 6 answers\n",
    "            'CQID015': 1,  # Single answer\n",
    "            'CQID020': 9,  # Up to 9 answers\n",
    "            'CQID025': 1,  # Single answer\n",
    "            'CQID034': 1,  # Single answer\n",
    "            'CQID035': 1,  # Single answer\n",
    "            'CQID036': 1   # Single answer\n",
    "        }\n",
    "\n",
    "        # Set default max_answers for any question type not explicitly listed\n",
    "        default_max_answers = 1\n",
    "\n",
    "        # Group by encounter_id and base_qid\n",
    "        grouped = predictions_df.groupby(['encounter_id', 'base_qid'])\n",
    "\n",
    "        aggregated_results = []\n",
    "\n",
    "        for (encounter_id, base_qid), group in tqdm(grouped, desc=\"Aggregating predictions\"):\n",
    "            # Extract all predictions for this group\n",
    "            predictions = group['prediction'].tolist()\n",
    "            image_ids = group['image_id'].tolist()\n",
    "\n",
    "            # Process predictions to standardize format\n",
    "            cleaned_predictions = []\n",
    "            for pred in predictions:\n",
    "                # Handle predictions that might be in a list format\n",
    "                if isinstance(pred, str):\n",
    "                    # Remove \"(please specify)\" from prediction\n",
    "                    pred = pred.replace(\" (please specify)\", \"\")\n",
    "\n",
    "                    if pred.startswith('[') and pred.endswith(']'):\n",
    "                        try:\n",
    "                            # Try to evaluate as a Python list\n",
    "                            pred_list = safe_convert_options(pred)\n",
    "                            if isinstance(pred_list, list):\n",
    "                                # Clean each item in the list\n",
    "                                pred_list = [p.replace(\" (please specify)\", \"\") if isinstance(p, str) else p for p in pred_list]\n",
    "                                cleaned_predictions.extend(pred_list)\n",
    "                                continue\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    # Handle comma-separated values\n",
    "                    if ',' in pred:\n",
    "                        # Clean each comma-separated item\n",
    "                        items = [p.strip().replace(\" (please specify)\", \"\") for p in pred.split(',')]\n",
    "                        cleaned_predictions.extend(items)\n",
    "                    else:\n",
    "                        cleaned_predictions.append(pred.strip())\n",
    "                else:\n",
    "                    cleaned_predictions.append(str(pred).strip())\n",
    "\n",
    "            all_cleaned_predictions = cleaned_predictions.copy()\n",
    "\n",
    "            # Count frequencies of each prediction\n",
    "            cleaned_predictions = [p.lower() if isinstance(p, str) else str(p).lower() for p in cleaned_predictions]\n",
    "            prediction_counts = Counter(cleaned_predictions)\n",
    "\n",
    "            # Get question type for determining max allowed answers\n",
    "            question_type = base_qid.split('-')[0] if '-' in base_qid else base_qid\n",
    "\n",
    "            # Determine max allowed answers for this question type\n",
    "            allowed_max = max_answers.get(question_type, default_max_answers)\n",
    "\n",
    "            # Sort predictions by frequency (most common first)\n",
    "            sorted_predictions = sorted(prediction_counts.items(), \n",
    "                                       key=lambda x: x[1], \n",
    "                                       reverse=True)\n",
    "\n",
    "            all_sorted_predictions = sorted_predictions.copy()\n",
    "            \n",
    "            # Get top N predictions where N is the max allowed\n",
    "            top_predictions = [p[0] for p in sorted_predictions[:allowed_max]]\n",
    "\n",
    "            # If there are ties at the cutoff point, randomly select to meet the max limit\n",
    "            if len(sorted_predictions) > allowed_max:\n",
    "                # Check if there's a tie at the cutoff\n",
    "                cutoff_count = sorted_predictions[allowed_max-1][1]\n",
    "                tied_predictions = [p[0] for p in sorted_predictions if p[1] == cutoff_count]\n",
    "\n",
    "                # If we have more tied predictions than slots available\n",
    "                if len(tied_predictions) > 1 and len(top_predictions) > allowed_max - len(tied_predictions):\n",
    "                    # Remove all tied predictions from top_predictions\n",
    "                    top_predictions = [p for p in top_predictions if p not in tied_predictions]\n",
    "\n",
    "                    # Randomly select from tied predictions to fill remaining slots\n",
    "\n",
    "                    random.seed(42)  # For reproducibility\n",
    "                    slots_remaining = allowed_max - len(top_predictions)\n",
    "                    selected_tied = random.sample(tied_predictions, slots_remaining)\n",
    "\n",
    "                    # Add the randomly selected tied predictions\n",
    "                    top_predictions.extend(selected_tied)\n",
    "\n",
    "            # If \"Not mentioned\" is in predictions but there are other predictions,\n",
    "            # remove \"Not mentioned\" (unless it's the only prediction)\n",
    "            if len(top_predictions) > 1 and \"Not mentioned\" in top_predictions:\n",
    "                top_predictions.remove(\"Not mentioned\")\n",
    "\n",
    "            # Create a single, combined prediction\n",
    "            combined_prediction = \", \".join(top_predictions)\n",
    "\n",
    "            # Initialize options_en as None\n",
    "            options_en = None\n",
    "\n",
    "            # If validation_df is provided, try to get options_en from it\n",
    "            if validation_df is not None:\n",
    "                # Find matching rows in validation_df\n",
    "                matching_rows = validation_df[(validation_df['encounter_id'] == encounter_id) & \n",
    "                                             (validation_df['base_qid'] == base_qid)]\n",
    "                if not matching_rows.empty:\n",
    "                    # Get options_en from the first matching row\n",
    "                    options_en = matching_rows.iloc[0].get('options_en')\n",
    "\n",
    "            result_dict = {\n",
    "                \"encounter_id\": encounter_id,\n",
    "                \"base_qid\": base_qid,\n",
    "                \"image_ids\": image_ids,\n",
    "                \"unique_predictions\": top_predictions,  # Now limited to max allowed\n",
    "                \"combined_prediction\": combined_prediction,\n",
    "                \"all_raw_predictions\": all_cleaned_predictions,\n",
    "                \"all_sorted_predictions\": all_sorted_predictions\n",
    "            }\n",
    "\n",
    "            # Add options_en only if it's available\n",
    "            if options_en is not None:\n",
    "                result_dict[\"options_en\"] = options_en\n",
    "\n",
    "            aggregated_results.append(result_dict)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42442c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "processed_val_dir = \"outputs/processed_val_data\"\n",
    "os.makedirs(processed_val_dir, exist_ok=True)\n",
    "\n",
    "if not args.skip_data_prep:\n",
    "    # Prepare data for validation\n",
    "    print(\"Preparing validation dataset...\")\n",
    "    val_df = prepare_validation_data()\n",
    "    \n",
    "    # Subset for testing if requested\n",
    "    if args.test:\n",
    "        print(\"Running in test mode with a small subset of data...\")\n",
    "        test_size = min(500, len(val_df))\n",
    "        val_df = val_df.head(test_size)\n",
    "    \n",
    "    # Process validation data\n",
    "    # Clear any existing processed data\n",
    "\n",
    "    if os.path.exists(processed_val_dir):\n",
    "        shutil.rmtree(processed_val_dir)\n",
    "        os.makedirs(processed_val_dir)\n",
    "    \n",
    "    total_examples = preprocess_validation_dataset(val_df, batch_size=args.batch_size, save_dir=processed_val_dir)\n",
    "    print(f\"Total processed validation examples: {total_examples}\")\n",
    "else:\n",
    "    print(\"Skipping data preparation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7711cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code cell after loading your train_df\n",
    "\n",
    "# Check which questions have multiple labels\n",
    "print(\"Checking for multi-label questions...\")\n",
    "\n",
    "# First, let's extract valid_answers and see how many have multiple values\n",
    "multi_label_counts = []\n",
    "qid_multi_label_map = {}\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    # Get the valid answers\n",
    "    answers = row.get('valid_answers', None)\n",
    "    qid = row.get('base_qid', 'Unknown')\n",
    "    \n",
    "    # Skip if no answers\n",
    "    if answers is None:\n",
    "        continue\n",
    "    \n",
    "    # Try to parse the answers if they're in string format\n",
    "    if isinstance(answers, str):\n",
    "        try:\n",
    "            parsed_answers = ast.literal_eval(answers)\n",
    "            if isinstance(parsed_answers, list):\n",
    "                answers = parsed_answers\n",
    "            else:\n",
    "                answers = [answers]\n",
    "        except:\n",
    "            # If parsing fails, treat as a single answer\n",
    "            answers = [answers]\n",
    "    \n",
    "    # Count how many answers there are\n",
    "    if isinstance(answers, list):\n",
    "        answer_count = len(answers)\n",
    "    else:\n",
    "        answer_count = 1\n",
    "    \n",
    "    # Track multi-label questions\n",
    "    if answer_count > 1:\n",
    "        multi_label_counts.append(answer_count)\n",
    "        \n",
    "        # Add to our QID mapping\n",
    "        if qid in qid_multi_label_map:\n",
    "            qid_multi_label_map[qid] += 1\n",
    "        else:\n",
    "            qid_multi_label_map[qid] = 1\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Total number of multi-label examples: {len(multi_label_counts)}\")\n",
    "print(f\"Percentage of dataset with multiple labels: {len(multi_label_counts) / len(val_df) * 100:.2f}%\")\n",
    "\n",
    "if multi_label_counts:\n",
    "    print(f\"Average number of labels in multi-label examples: {sum(multi_label_counts) / len(multi_label_counts):.2f}\")\n",
    "    print(f\"Max number of labels in a single example: {max(multi_label_counts)}\")\n",
    "\n",
    "# Print breakdown by QID\n",
    "print(\"\\nBreakdown of multi-label examples by QID:\")\n",
    "for qid, count in sorted(qid_multi_label_map.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"QID: {qid}, Count: {count}\")\n",
    "\n",
    "# Optional: Check if 'is_multi_label' column exists and if it aligns with our findings\n",
    "if 'is_multi_label' in val_df.columns:\n",
    "    multi_label_flags = val_df['is_multi_label'].sum()\n",
    "    print(f\"\\nNumber of examples marked as multi-label in the dataframe: {multi_label_flags}\")\n",
    "    print(f\"Does this match our findings? {'Yes' if multi_label_flags == len(multi_label_counts) else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6685d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_mentioned_rows = val_df[val_df['valid_answers'].apply(lambda x: 'Not mentioned' in x)]\n",
    "# not_mentioned_rows.head(5) # works appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_inference_inputs(processed_val_dir, num_samples=3):\n",
    "    \"\"\"\n",
    "    Inspect the actual inputs being used during inference by examining the\n",
    "    processed validation data files.\n",
    "    \n",
    "    Args:\n",
    "        processed_val_dir: Directory containing processed validation data\n",
    "        num_samples: Number of samples to display\n",
    "    \"\"\"\n",
    "    # Find all batch files\n",
    "    batch_files = sorted([f for f in os.listdir(processed_val_dir) \n",
    "                         if f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No batch files found in {processed_val_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(batch_files)} batch files in {processed_val_dir}\")\n",
    "    \n",
    "    # Load the first batch file\n",
    "    with open(os.path.join(processed_val_dir, batch_files[0]), 'rb') as f:\n",
    "        batch_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"Batch contains {len(batch_data)} samples\")\n",
    "    \n",
    "    # Display the requested number of samples\n",
    "    for i, sample in enumerate(batch_data[:num_samples]):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SAMPLE {i+1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Display metadata\n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(f\"Question ID: {sample['qid']}\")\n",
    "        \n",
    "        # Display the data that would be fed to the model\n",
    "        print(\"\\nSYSTEM MESSAGE (Used during inference):\")\n",
    "        print(\"-\" * 80)\n",
    "        # Create the system message\n",
    "        system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "        \n",
    "        IMPORTANT: \n",
    "        - Respond ONLY with the exact text of the option(s) that apply\n",
    "        - Do not provide any explanations\n",
    "        - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "        - Do not write \"Options:\" or similar prefixes\n",
    "        - Do not write \"Answer:\" or similar prefixes\n",
    "        - Multiple answers should be separated by commas\n",
    "        - If unsure, respond with \"Not mentioned\"\n",
    "        - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "        - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "        \"\"\"\n",
    "        \n",
    "        print(system_message)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display the query text\n",
    "        print(\"\\nUSER MESSAGE TEXT (Used during inference):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(sample['query_text'])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display image information\n",
    "        print(\"\\nIMAGE (Used during inference):\")\n",
    "        print(f\"Image path: {sample['image_path']}\")\n",
    "        \n",
    "        # Try to display image dimensions if PIL is available\n",
    "        try:\n",
    "            img = Image.open(sample['image_path'])\n",
    "            print(f\"Image dimensions: {img.size[0]}x{img.size[1]}, Format: {img.format}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open image: {e}\")\n",
    "        \n",
    "#         # Display fields NOT used during inference\n",
    "#         print(\"\\nFIELDS NOT DIRECTLY USED DURING INFERENCE:\")\n",
    "#         print(\"-\" * 80)\n",
    "#         for key, value in sample.items():\n",
    "#             if key not in ['query_text', 'image_path', 'id']:\n",
    "#                 print(f\"{key}: {value}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Option to stop after showing samples\n",
    "        if i >= num_samples - 1:\n",
    "            break\n",
    "\n",
    "# Run the inspection function\n",
    "inspect_inference_inputs(processed_val_dir, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd849093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a timestamp to create unique filenames\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Extract model ID from the model path\n",
    "if args.use_merged_model:\n",
    "    # If using merged model, extract from checkpoint path\n",
    "    model_id = os.path.basename(args.checkpoint_path)\n",
    "else:\n",
    "    # If using base model directly, extract from base model ID\n",
    "    model_id = args.base_model_id.split('/')[-1]\n",
    "\n",
    "# Load model and run inference\n",
    "print(f\"Loading model from {args.model_path}...\")\n",
    "inference = MedicalImageInference(args.model_path, token=hf_token)\n",
    "\n",
    "# Run inference on processed data\n",
    "predictions_file = os.path.join(OUTPUT_DIR, \n",
    "                              f\"val_predictions_{model_id}_{timestamp}{'_test' if args.test else ''}.csv\")\n",
    "print(f\"Running inference (max_samples={args.max_samples if args.max_samples else 'all'})...\")\n",
    "predictions_df = inference.batch_predict(processed_val_dir, predictions_file, max_samples=args.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate predictions\n",
    "print(\"Aggregating predictions...\")\n",
    "# Load the validation dataset\n",
    "val_dataset = pd.read_csv(os.path.join(OUTPUT_DIR, \"val_dataset.csv\"))\n",
    "# Run the inference\n",
    "# Aggregate predictions with options_en\n",
    "aggregated_df = inference.aggregate_predictions(predictions_df, validation_df=val_dataset)\n",
    "# Save aggregated results\n",
    "aggregated_file = os.path.join(OUTPUT_DIR, \n",
    "                             f\"aggregated_predictions_{model_id}_{timestamp}{'_test' if args.test else ''}.csv\")\n",
    "aggregated_df.to_csv(aggregated_file, index=False)\n",
    "print(f\"Inference complete. Results saved to {predictions_file} and {aggregated_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271687d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample of predictions for inspection\n",
    "print(\"\\nSample of raw predictions:\")\n",
    "predictions_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39598c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test suffix\n",
    "test_suffix = '_test' if args.test else ''\n",
    "\n",
    "# Find the latest prediction file\n",
    "prediction_pattern = os.path.join(OUTPUT_DIR, f\"val_predictions_*{test_suffix}.csv\")\n",
    "prediction_files = sorted(glob.glob(prediction_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not prediction_files:\n",
    "    raise FileNotFoundError(f\"No prediction files found matching pattern {prediction_pattern}\")\n",
    "latest_prediction_file = prediction_files[0]\n",
    "print(f\"Loading latest prediction file: {latest_prediction_file}\")\n",
    "predictions = pd.read_csv(latest_prediction_file)\n",
    "\n",
    "# Find the latest aggregated file\n",
    "aggregated_pattern = os.path.join(OUTPUT_DIR, f\"aggregated_predictions_*{test_suffix}.csv\")\n",
    "aggregated_files = sorted(glob.glob(aggregated_pattern), key=os.path.getmtime, reverse=True)\n",
    "if not aggregated_files:\n",
    "    raise FileNotFoundError(f\"No aggregated files found matching pattern {aggregated_pattern}\")\n",
    "latest_aggregated_file = aggregated_files[0]\n",
    "print(f\"Loading latest aggregated file: {latest_aggregated_file}\")\n",
    "aggregated = pd.read_csv(latest_aggregated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07978e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter predictions that have commas (indicating multiple answers)\n",
    "multi_answer_preds = predictions[predictions['prediction'].str.contains(',', na=False)]\n",
    "\n",
    "# Display sample of multi-answer predictions\n",
    "print(\"Sample of predictions with multiple answers:\")\n",
    "multi_answer_preds[[\"encounter_id\", \"base_qid\", \"image_id\", \"prediction\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many multi-answer predictions we have\n",
    "print(f\"\\nTotal multi-answer predictions: {len(multi_answer_preds)}\")\n",
    "\n",
    "# See which questions tend to have multiple answers\n",
    "print(\"\\nMulti-answer predictions by question type:\")\n",
    "multi_answer_preds['base_qid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of aggregated predictions:\")\n",
    "aggregated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932db705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up the aggregated result for this question\n",
    "agg_result = aggregated[\n",
    "    (aggregated['encounter_id'] == 'ENC00852') & \n",
    "    (aggregated['base_qid'] == 'CQID034')\n",
    "]\n",
    "agg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of different answers\n",
    "answer_counts = predictions[\"prediction\"].value_counts().head(10)\n",
    "print(\"\\nMost common predictions:\")\n",
    "print(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_for_official_eval_with_display(aggregated_df, output_file):\n",
    "    \"\"\"\n",
    "    Format predictions as expected by the official evaluation script,\n",
    "    mapping text answers to indices and distributing multiple answers\n",
    "    across question variants when appropriate.\n",
    "    Also displays the text values alongside their indices for verification.\n",
    "    \"\"\"\n",
    "    # Define the question IDs and their allowed variants\n",
    "    QIDS = [\n",
    "        \"CQID010-001\",  # how much of body is affected (single answer)\n",
    "        \"CQID011-001\", \"CQID011-002\", \"CQID011-003\", \"CQID011-004\", \"CQID011-005\", \"CQID011-006\",  # multiple answers allowed\n",
    "        \"CQID012-001\", \"CQID012-002\", \"CQID012-003\", \"CQID012-004\", \"CQID012-005\", \"CQID012-006\",  # multiple answers allowed\n",
    "        \"CQID015-001\",  # single answer\n",
    "        \"CQID020-001\", \"CQID020-002\", \"CQID020-003\", \"CQID020-004\", \"CQID020-005\", \n",
    "        \"CQID020-006\", \"CQID020-007\", \"CQID020-008\", \"CQID020-009\",  # multiple answers allowed\n",
    "        \"CQID025-001\",  # single answer\n",
    "        \"CQID034-001\",  # single answer\n",
    "        \"CQID035-001\",  # single answer\n",
    "        \"CQID036-001\",  # single answer\n",
    "    ]\n",
    "    \n",
    "    # Create a mapping of question base IDs to their allowed variants\n",
    "    qid_variants = {}\n",
    "    for qid in QIDS:\n",
    "        base_qid, variant = qid.split('-')\n",
    "        if base_qid not in qid_variants:\n",
    "            qid_variants[base_qid] = []\n",
    "        qid_variants[base_qid].append(qid)\n",
    "    \n",
    "    # Get all required base QIDs for a complete encounter\n",
    "    required_base_qids = set(qid.split('-')[0] for qid in QIDS)\n",
    "    \n",
    "    formatted_predictions = []\n",
    "    display_info = []\n",
    "    \n",
    "    # Group by encounter_id\n",
    "    for encounter_id, group in aggregated_df.groupby('encounter_id'):\n",
    "        # Get all base_qids for this encounter\n",
    "        encounter_base_qids = set(group['base_qid'].unique())\n",
    "        \n",
    "        # Skip encounters that don't have all required questions\n",
    "        if not required_base_qids.issubset(encounter_base_qids):\n",
    "            print(f\"Skipping encounter {encounter_id} - missing required questions\")\n",
    "            continue\n",
    "        \n",
    "        # Create a prediction entry for this encounter\n",
    "        pred_entry = {'encounter_id': encounter_id}\n",
    "        encounter_display = {'encounter_id': encounter_id, 'questions': []}\n",
    "        \n",
    "        # Process each question for this encounter\n",
    "        for _, row in group.iterrows():\n",
    "            base_qid = row['base_qid']\n",
    "            \n",
    "            # Skip if we don't have variants defined for this question\n",
    "            if base_qid not in qid_variants:\n",
    "                continue\n",
    "            \n",
    "            # Get the options list for this question\n",
    "            options = safe_convert_options(row['options_en'])\n",
    "\n",
    "#             options = row['options_en']\n",
    "#             if isinstance(options, str):\n",
    "#                 try:\n",
    "#                     options = eval(options)\n",
    "#                 except:\n",
    "#                     options = options.split(',')\n",
    "            \n",
    "            # Find the index of \"Not mentioned\" in the options\n",
    "            not_mentioned_index = None\n",
    "            for i, opt in enumerate(options):\n",
    "                if opt == \"Not mentioned\":\n",
    "                    not_mentioned_index = i\n",
    "                    break\n",
    "            \n",
    "            # If \"Not mentioned\" is not in the options, default to the last option\n",
    "            if not_mentioned_index is None:\n",
    "                not_mentioned_index = len(options) - 1\n",
    "            \n",
    "            # Get predictions\n",
    "            if isinstance(row['unique_predictions'], list):\n",
    "                predictions = row['unique_predictions']\n",
    "            else:\n",
    "                try:\n",
    "                    predictions = eval(row['unique_predictions'])\n",
    "                except:\n",
    "                    predictions = [row['unique_predictions']]\n",
    "            \n",
    "            # Map text predictions to indices\n",
    "            prediction_indices = []\n",
    "            prediction_texts = []\n",
    "            \n",
    "            for pred in predictions:\n",
    "                pred_text = str(pred).strip()\n",
    "                prediction_texts.append(pred_text)\n",
    "                \n",
    "                # Find index of the prediction in options\n",
    "                found = False\n",
    "                for i, option in enumerate(options):\n",
    "                    if pred_text.lower() == option.lower():\n",
    "                        prediction_indices.append(i)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                # If prediction not found in options, use index 100\n",
    "                if not found:\n",
    "                    prediction_indices.append(100)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            unique_indices = []\n",
    "            unique_texts = []\n",
    "            for idx, text in zip(prediction_indices, prediction_texts):\n",
    "                if idx not in unique_indices:\n",
    "                    unique_indices.append(idx)\n",
    "                    unique_texts.append(text)\n",
    "            \n",
    "            # If 100 is in the list along with valid indices, remove 100\n",
    "            if len(unique_indices) > 1 and 100 in unique_indices:\n",
    "                idx_to_remove = unique_indices.index(100)\n",
    "                unique_indices.remove(100)\n",
    "                unique_texts.pop(idx_to_remove)\n",
    "            \n",
    "            # Get the available variants for this question\n",
    "            available_variants = qid_variants[base_qid]\n",
    "            \n",
    "            # Store info for display\n",
    "            question_display = {\n",
    "                'base_qid': base_qid,\n",
    "                'predicted_texts': unique_texts,\n",
    "                'predicted_indices': unique_indices,\n",
    "                'options': options,\n",
    "                'not_mentioned_index': not_mentioned_index,\n",
    "                'variant_assignments': {}\n",
    "            }\n",
    "            \n",
    "            # For single-answer questions (with only one variant)\n",
    "            if len(available_variants) == 1:\n",
    "                if unique_indices:\n",
    "                    # Store as a single integer, not a list\n",
    "                    pred_entry[available_variants[0]] = unique_indices[0]\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': unique_indices[0],\n",
    "                        'text': unique_texts[0] if unique_texts else \"None\"\n",
    "                    }\n",
    "                else:\n",
    "                    # Default to \"Not mentioned\" if no prediction\n",
    "                    pred_entry[available_variants[0]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[0]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            # For multi-answer questions\n",
    "            else:\n",
    "                # Distribute answers across available variants\n",
    "                for i, idx in enumerate(unique_indices):\n",
    "                    if i < len(available_variants):\n",
    "                        # Store each answer as a single integer, not a list\n",
    "                        pred_entry[available_variants[i]] = idx\n",
    "                        question_display['variant_assignments'][available_variants[i]] = {\n",
    "                            'index': idx,\n",
    "                            'text': unique_texts[i] if i < len(unique_texts) else \"None\"\n",
    "                        }\n",
    "                \n",
    "                # Fill remaining variants with a default value (usually \"Not mentioned\")\n",
    "                for i in range(len(unique_indices), len(available_variants)):\n",
    "                    # Use correct \"Not mentioned\" index for this question\n",
    "                    pred_entry[available_variants[i]] = not_mentioned_index\n",
    "                    question_display['variant_assignments'][available_variants[i]] = {\n",
    "                        'index': not_mentioned_index,\n",
    "                        'text': \"Not mentioned\"\n",
    "                    }\n",
    "            \n",
    "            encounter_display['questions'].append(question_display)\n",
    "        \n",
    "        formatted_predictions.append(pred_entry)\n",
    "        display_info.append(encounter_display)\n",
    "    \n",
    "    if not formatted_predictions:\n",
    "        print(\"Warning: No complete encounters found in the data!\")\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(formatted_predictions, f, indent=2)\n",
    "    \n",
    "    # Display information about the predictions\n",
    "    for encounter in display_info:\n",
    "        print(f\"\\nEncounter: {encounter['encounter_id']}\")\n",
    "        for question in encounter['questions']:\n",
    "            print(f\"  Question: {question['base_qid']}\")\n",
    "            print(f\"  Predicted texts: {question['predicted_texts']}\")\n",
    "            print(f\"  Predicted indices: {question['predicted_indices']}\")\n",
    "            print(f\"  'Not mentioned' index: {question['not_mentioned_index']}\")\n",
    "            print(\"  Variant assignments:\")\n",
    "            for variant, assignment in question['variant_assignments'].items():\n",
    "                print(f\"    {variant}: index={assignment['index']} ({assignment['text']})\")\n",
    "            print(f\"  Available options: {question['options']}\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"Formatted predictions saved to {output_file} ({len(formatted_predictions)} complete encounters)\")\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and save predictions for official evaluation\n",
    "predictions_json = os.path.join(OUTPUT_DIR, \n",
    "                               f\"data_cvqa_sys_{model_id}_{timestamp}{'_test' if args.test else ''}.json\")\n",
    "format_predictions_for_official_eval_with_display(aggregated, predictions_json)\n",
    "print(f\"Formatted predictions saved to {predictions_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter validation DataFrame to get the specific question and encounter\n",
    "specific_question = val_dataset[(val_dataset['encounter_id'] == 'ENC00853') & \n",
    "                               (val_dataset['base_qid'] == 'CQID012')]\n",
    "\n",
    "# Display all relevant columns\n",
    "print(\"Question Information:\")\n",
    "print(f\"Question text: {specific_question['question_text'].values[0]}\")\n",
    "print(f\"Question type: {specific_question['question_type_en'].values[0]}\")\n",
    "print(f\"Question category: {specific_question['question_category_en'].values[0]}\")\n",
    "print(f\"Options: {specific_question['options_en'].values[0]}\")\n",
    "print(f\"Multi-label: {specific_question['is_multi_label'].values[0]}\")\n",
    "print(\"\\nClinical Context:\")\n",
    "print(f\"Query title: {specific_question['query_title_en'].values[0]}\")\n",
    "print(f\"Query content: {specific_question['query_content_en'].values[0]}\")\n",
    "print(\"\\nImage information:\")\n",
    "print(f\"Image ID: {specific_question['image_id'].values[0]}\")\n",
    "print(f\"Image path: {specific_question['image_path'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482db9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to double check the saved file\n",
    "\n",
    "# Load the formatted predictions JSON\n",
    "with open(f'/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/data_cvqa_sys{\"_test\" if args.test else \"\"}.json', 'r') as f:\n",
    "    formatted_preds = json.load(f)\n",
    "    \n",
    "# Display the first 3 entries\n",
    "print(\"First 3 prediction entries:\")\n",
    "for i in range(min(3, len(formatted_preds))):\n",
    "    print(f\"\\nPrediction {i+1}:\")\n",
    "    pprint(formatted_preds[i])\n",
    "\n",
    "# Show an example of answers not in options (if any)\n",
    "print(\"\\nLooking for predictions with index 100 (not in options):\")\n",
    "found = False\n",
    "for entry in formatted_preds:\n",
    "    for key, value in entry.items():\n",
    "        if key != 'encounter_id':  # Skip the encounter_id\n",
    "            if (isinstance(value, list) and 100 in value) or value == 100:\n",
    "                print(f\"\\nFound prediction not in options:\")\n",
    "                print(f\"Encounter: {entry['encounter_id']}\")\n",
    "                print(f\"Question: {key}\")\n",
    "                print(f\"Prediction indices: {value}\")\n",
    "                \n",
    "                # Load original predictions for this encounter\n",
    "                agg_df = pd.read_csv('/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/outputs/aggregated_predictions_test.csv')\n",
    "                base_qid = key.split('-')[0]\n",
    "                encounter = entry['encounter_id']\n",
    "                match = agg_df[(agg_df['encounter_id'] == encounter) & (agg_df['base_qid'] == base_qid)]\n",
    "                if not match.empty:\n",
    "                    print(f\"Original prediction text: {match['combined_prediction'].values[0]}\")\n",
    "                    print(f\"Available options: {match['options_en'].values[0]}\")\n",
    "                found = True\n",
    "                break\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print(\"No predictions with index 100 found in the first few entries.\")\n",
    "\n",
    "# Show statistics\n",
    "question_counts = {}\n",
    "for entry in formatted_preds:\n",
    "    qid_count = len(entry) - 1  # Subtract 1 for encounter_id\n",
    "    if qid_count in question_counts:\n",
    "        question_counts[qid_count] += 1\n",
    "    else:\n",
    "        question_counts[qid_count] = 1\n",
    "\n",
    "print(\"\\nNumber of questions per encounter:\")\n",
    "for count, num_entries in sorted(question_counts.items()):\n",
    "    print(f\"{count} questions: {num_entries} encounters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35843cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model ID\n",
    "if args.use_merged_model:\n",
    "    model_id = os.path.basename(args.checkpoint_path)\n",
    "else:\n",
    "    model_id = args.base_model_id.split('/')[-1]\n",
    "\n",
    "# Create timestamp for the submission, not for finding files\n",
    "submission_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Find the most recent data_cvqa_sys file for this model\n",
    "test_suffix = '_test' if args.test else ''\n",
    "json_pattern = os.path.join(OUTPUT_DIR, f\"data_cvqa_sys_{model_id}*{test_suffix}.json\")\n",
    "json_files = sorted(glob.glob(json_pattern), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No JSON prediction files found matching pattern {json_pattern}\")\n",
    "\n",
    "most_recent_json = json_files[0]\n",
    "print(f\"Using most recent prediction JSON: {most_recent_json}\")\n",
    "\n",
    "# Create directory for empty masks_preds if it doesn't exist\n",
    "masks_preds_dir = os.path.join(OUTPUT_DIR, \"masks_preds\")\n",
    "os.makedirs(masks_preds_dir, exist_ok=True)\n",
    "\n",
    "# Create submission folder with timestamp\n",
    "submission_dir = os.path.join(OUTPUT_DIR, f\"submission_{model_id}_{submission_timestamp}\")\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# Copy the most recent JSON with the exact required filename\n",
    "dest_json = os.path.join(submission_dir, \"data_cvqa_sys.json\")\n",
    "shutil.copy2(most_recent_json, dest_json)\n",
    "\n",
    "# Create empty masks_preds directory in the submission folder\n",
    "submission_masks_dir = os.path.join(submission_dir, \"masks_preds\")\n",
    "os.makedirs(submission_masks_dir, exist_ok=True)\n",
    "\n",
    "# Create the zip file with both components\n",
    "zip_path = os.path.join(OUTPUT_DIR, f\"mysubmission_{model_id}_{submission_timestamp}.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    # Add the json file\n",
    "    zipf.write(dest_json, arcname=\"data_cvqa_sys.json\")\n",
    "    \n",
    "    # Add the masks_preds directory (it will be empty)\n",
    "    zipf.write(submission_masks_dir, arcname=\"masks_preds\")\n",
    "\n",
    "print(f\"Submission package created at: {zip_path}\")\n",
    "print(f\"Files included:\")\n",
    "print(f\" - data_cvqa_sys.json (copied from {most_recent_json})\")\n",
    "print(f\" - masks_preds/ (empty directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad8f01",
   "metadata": {},
   "source": [
    "NEEDS UPDATING: Run evaluation in terminal with the following command: \n",
    "\n",
    "```\n",
    "python evaluate_new.py outputs/data_cvqa_sys_test.json\n",
    "OR python evaluate_new.py outputs/data_cvqa_sys.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f4bfd",
   "metadata": {},
   "source": [
    "Measuring input context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_tokens(dataset_path, processor, num_samples=None):\n",
    "    \"\"\"\n",
    "    Analyze token counts in the dataset without running training or inference\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the processed dataset directory\n",
    "        processor: The processor from the model\n",
    "        num_samples: Optional limit on number of samples to process\n",
    "    \"\"\"\n",
    "    token_stats = {\n",
    "        \"samples\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    batch_files = sorted([f for f in os.listdir(dataset_path) if f.startswith(\"batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    total_tokens = 0\n",
    "    max_tokens = 0\n",
    "    min_tokens = float('inf')\n",
    "    sample_count = 0\n",
    "    \n",
    "    for batch_file in tqdm(batch_files, desc=\"Analyzing batches\"):\n",
    "        with open(os.path.join(dataset_path, batch_file), 'rb') as f:\n",
    "            batch_data = pickle.load(f)\n",
    "        \n",
    "        for sample in tqdm(batch_data, desc=f\"Analyzing {batch_file}\", leave=False):\n",
    "            # Create the input as it would be during training/inference\n",
    "            \n",
    "            system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "            \n",
    "            IMPORTANT: \n",
    "            - Respond ONLY with the exact text of the option(s) that apply\n",
    "            - Do not provide any explanations\n",
    "            - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "            - Do not write \"Options:\" or similar prefixes\n",
    "            - Do not write \"Answer:\" or similar prefixes\n",
    "            - Multiple answers should be separated by commas\n",
    "            - If unsure, respond with \"Not mentioned\"\n",
    "            - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "            - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "            \"\"\"\n",
    "            \n",
    "            # Format messages similar to during inference\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": sample[\"query_text\"]},\n",
    "                        # Don't need to actually load the image for token counting\n",
    "                        {\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\"},\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            # Get formatted text for tokenization\n",
    "            text = processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "            \n",
    "            # Count tokens\n",
    "            tokens = processor.tokenizer.encode(text)\n",
    "            token_count = len(tokens)\n",
    "            \n",
    "            # Update statistics\n",
    "            total_tokens += token_count\n",
    "            max_tokens = max(max_tokens, token_count)\n",
    "            min_tokens = min(min_tokens, token_count)\n",
    "            \n",
    "            # Store sample info\n",
    "            token_stats[\"samples\"].append({\n",
    "                \"id\": sample[\"id\"],\n",
    "                \"qid\": sample[\"qid\"],\n",
    "                \"image\": os.path.basename(sample[\"image_path\"]),\n",
    "                \"token_count\": token_count\n",
    "            })\n",
    "            \n",
    "            sample_count += 1\n",
    "            if num_samples and sample_count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        if num_samples and sample_count >= num_samples:\n",
    "            break\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    token_stats[\"summary\"] = {\n",
    "        \"total_samples\": sample_count,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_tokens_per_sample\": total_tokens / sample_count if sample_count > 0 else 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"min_tokens\": min_tokens\n",
    "    }\n",
    "    \n",
    "    # Save the analysis\n",
    "    with open(os.path.join(os.path.dirname(dataset_path), \"token_analysis.json\"), \"w\") as f:\n",
    "        json.dump(token_stats, f, indent=2)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nToken Usage Analysis:\")\n",
    "    print(f\"Total samples analyzed: {sample_count}\")\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Average tokens per sample: {total_tokens/sample_count:.2f}\")\n",
    "    print(f\"Max tokens in a sample: {max_tokens}\")\n",
    "    print(f\"Min tokens in a sample: {min_tokens}\")\n",
    "    \n",
    "    # Create histogram for visualization\n",
    "    token_counts = [s[\"token_count\"] for s in token_stats[\"samples\"]]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(token_counts, bins=30)\n",
    "    plt.title(\"Distribution of Token Counts\")\n",
    "    plt.xlabel(\"Token Count\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.savefig(os.path.join(os.path.dirname(dataset_path), \"token_distribution.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    return token_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f882d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_tokens(dataset_dir, processor, num_samples=None):\n",
    "    \"\"\"\n",
    "    Analyze token counts in the dataset without running training or inference\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to the processed dataset directory\n",
    "        processor: The processor from the model\n",
    "        num_samples: Optional limit on number of samples to process\n",
    "    \"\"\"\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Directory not found: {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    token_stats = {\n",
    "        \"samples\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    # Find batch files\n",
    "    batch_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"batch_\") and f.endswith(\".pkl\") or\n",
    "                          f.startswith(\"val_batch_\") and f.endswith(\".pkl\")])\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No batch files found in {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    total_tokens = 0\n",
    "    max_tokens = 0\n",
    "    min_tokens = float('inf')\n",
    "    sample_count = 0\n",
    "    all_token_counts = []\n",
    "    \n",
    "    for batch_file in tqdm(batch_files, desc=\"Analyzing batches\"):\n",
    "        try:\n",
    "            with open(os.path.join(dataset_dir, batch_file), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            for sample in tqdm(batch_data, desc=f\"Analyzing {batch_file}\", leave=False):\n",
    "                if not isinstance(sample, dict) or \"query_text\" not in sample:\n",
    "                    print(f\"Warning: Unexpected sample format in {batch_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Create the input as it would be during inference                \n",
    "                \n",
    "                system_message = \"\"\"You are a medical assistant. Your task is to examine the provided information, and select the option(s) that best answer my question.\n",
    "\n",
    "                IMPORTANT: \n",
    "                - Respond ONLY with the exact text of the option(s) that apply\n",
    "                - Do not provide any explanations\n",
    "                - Do not include the numbers that appear before options (like '1.' or '2')\n",
    "                - Do not write \"Options:\" or similar prefixes\n",
    "                - Do not write \"Answer:\" or similar prefixes\n",
    "                - Multiple answers should be separated by commas\n",
    "                - If unsure, respond with \"Not mentioned\"\n",
    "                - The 'Background Clinical Information' section contains context to help you answer the main question\n",
    "                - ONLY answer the question listed under \"MAIN QUESTION TO ANSWER:\" at the beginning\n",
    "                \"\"\"\n",
    "                \n",
    "                # Format messages similar to during inference\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": sample[\"query_text\"]},\n",
    "                            # Don't need to actually load the image for token counting\n",
    "                            {\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                # Get formatted text for tokenization\n",
    "                text = processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "                \n",
    "                # Count tokens\n",
    "                tokens = processor.tokenizer.encode(text)\n",
    "                token_count = len(tokens)\n",
    "                all_token_counts.append(token_count)\n",
    "                \n",
    "                # Update statistics\n",
    "                total_tokens += token_count\n",
    "                max_tokens = max(max_tokens, token_count)\n",
    "                min_tokens = min(min_tokens, token_count)\n",
    "                \n",
    "                # Store sample info\n",
    "                token_stats[\"samples\"].append({\n",
    "                    \"id\": sample.get(\"id\", \"unknown\"),\n",
    "                    \"qid\": sample.get(\"qid\", \"unknown\"),\n",
    "                    \"image\": os.path.basename(sample.get(\"image_path\", \"unknown\")),\n",
    "                    \"token_count\": token_count,\n",
    "                    \"text_length\": len(sample[\"query_text\"])\n",
    "                })\n",
    "                \n",
    "                sample_count += 1\n",
    "                if num_samples and sample_count >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if num_samples and sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if sample_count == 0:\n",
    "        print(\"No samples were successfully processed\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    token_stats[\"summary\"] = {\n",
    "        \"total_samples\": sample_count,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_tokens_per_sample\": total_tokens / sample_count,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"min_tokens\": min_tokens,\n",
    "        \"median_tokens\": np.median(all_token_counts),\n",
    "        \"percentile_90\": np.percentile(all_token_counts, 90),\n",
    "        \"percentile_99\": np.percentile(all_token_counts, 99)\n",
    "    }\n",
    "    \n",
    "    # Save the analysis\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(dataset_dir)}_token_analysis.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(token_stats, f, indent=2)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nToken Usage Analysis:\")\n",
    "    print(f\"Total samples analyzed: {sample_count}\")\n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Average tokens per sample: {total_tokens/sample_count:.2f}\")\n",
    "    print(f\"Median tokens per sample: {np.median(all_token_counts):.2f}\")\n",
    "    print(f\"90th percentile: {np.percentile(all_token_counts, 90):.2f}\")\n",
    "    print(f\"99th percentile: {np.percentile(all_token_counts, 99):.2f}\")\n",
    "    print(f\"Max tokens in a sample: {max_tokens}\")\n",
    "    print(f\"Min tokens in a sample: {min_tokens}\")\n",
    "    print(f\"Percentage of 128K context window used (max): {(max_tokens/128000)*100:.2f}%\")\n",
    "    \n",
    "    # Create better histogram\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Use bins based on data range\n",
    "    bin_count = min(50, len(set(all_token_counts)))\n",
    "    \n",
    "    # Plot histogram with actual counts\n",
    "    n, bins, patches = plt.hist(all_token_counts, bins=bin_count, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add mean line\n",
    "    plt.axvline(x=np.mean(all_token_counts), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_token_counts):.1f}')\n",
    "    \n",
    "    # Add median line\n",
    "    plt.axvline(x=np.median(all_token_counts), color='green', linestyle='-', linewidth=2, label=f'Median: {np.median(all_token_counts):.1f}')\n",
    "    \n",
    "    # Add 90th percentile line\n",
    "    plt.axvline(x=np.percentile(all_token_counts, 90), color='orange', linestyle='-.', linewidth=2, label=f'90th percentile: {np.percentile(all_token_counts, 90):.1f}')\n",
    "    \n",
    "    plt.title(f\"Distribution of Token Counts (n={sample_count})\", fontsize=16)\n",
    "    plt.xlabel(\"Token Count\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Samples\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text with stats\n",
    "    stats_text = (\n",
    "        f\"Min: {min_tokens}\\n\"\n",
    "        f\"Max: {max_tokens}\\n\"\n",
    "        f\"Mean: {np.mean(all_token_counts):.1f}\\n\"\n",
    "        f\"Median: {np.median(all_token_counts):.1f}\\n\"\n",
    "        f\"Std Dev: {np.std(all_token_counts):.1f}\\n\"\n",
    "        f\"90th %ile: {np.percentile(all_token_counts, 90):.1f}\\n\"\n",
    "        f\"% of 128K used: {(max_tokens/128000)*100:.2f}%\"\n",
    "    )\n",
    "    \n",
    "    # Position text in the upper right\n",
    "    plt.text(0.95, 0.95, stats_text, \n",
    "             transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', \n",
    "             horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(dataset_dir)}_token_distribution.png\")\n",
    "    plt.savefig(plt_path, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return token_stats\n",
    "\n",
    "# Now run for both datasets - make sure the paths are correct\n",
    "try:\n",
    "    print(\"Analyzing training data tokens...\")\n",
    "    train_token_stats = analyze_dataset_tokens(\"outputs/processed_data\", processor)\n",
    "    \n",
    "    print(\"\\nAnalyzing validation data tokens...\")\n",
    "    val_token_stats = analyze_dataset_tokens(\"outputs/processed_val_data\", processor)\n",
    "    \n",
    "    print(\"\\nToken analysis complete! Files saved to the outputs directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running token analysis: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_other_please_specify(df, column_name='valid_answers'):\n",
    "    \"\"\"\n",
    "    Check if \"other (please specify)\" appears as a ground truth answer\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check (train or validation)\n",
    "        column_name: Column containing the answers (default: 'valid_answers')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics about occurrences\n",
    "    \"\"\"\n",
    "    # Convert string representations to lists if needed\n",
    "    if df[column_name].dtype == 'object':\n",
    "        \n",
    "        def safe_eval(x):\n",
    "            try:\n",
    "                if isinstance(x, list):\n",
    "                    return x\n",
    "                if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                    return ast.literal_eval(x)\n",
    "                return [x]  # Return single item as list\n",
    "            except:\n",
    "                return [str(x)]  # Return as single-item list if eval fails\n",
    "        \n",
    "        # Apply the conversion\n",
    "        valid_answers_lists = df[column_name].apply(safe_eval)\n",
    "    else:\n",
    "        valid_answers_lists = df[column_name]\n",
    "    \n",
    "    # Check for matches\n",
    "    matches = []\n",
    "    for idx, answers in enumerate(valid_answers_lists):\n",
    "        for answer in answers:\n",
    "            if isinstance(answer, str) and \"other (please specify)\" in answer.lower():\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"encounter_id\": df.iloc[idx].get('encounter_id', 'unknown'),\n",
    "                    \"base_qid\": df.iloc[idx].get('base_qid', 'unknown'),\n",
    "                    \"answer\": answer,\n",
    "                    \"all_answers\": answers\n",
    "                })\n",
    "    \n",
    "    # Summarize findings\n",
    "    results = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"match_count\": len(matches),\n",
    "        \"percentage\": (len(matches) / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        \"matches\": matches\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Results for {column_name} in dataset with {len(df)} rows:\")\n",
    "    print(f\"Found {len(matches)} occurrences of 'other (please specify)' ({results['percentage']:.2f}%)\")\n",
    "    \n",
    "    if matches:\n",
    "        print(\"\\nSample matches:\")\n",
    "        for i, match in enumerate(matches[:5]):  # Show up to 5 examples\n",
    "            print(f\"{i+1}. Index {match['index']}, Encounter: {match['encounter_id']}, QID: {match['base_qid']}\")\n",
    "            print(f\"   Answer: {match['answer']}\")\n",
    "            print(f\"   All answers: {match['all_answers']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Check training data\n",
    "print(\"CHECKING TRAINING DATA:\")\n",
    "train_results = check_for_other_please_specify(train_df)\n",
    "\n",
    "# Check validation data\n",
    "print(\"\\nCHECKING VALIDATION DATA:\")\n",
    "val_results = check_for_other_please_specify(val_df)\n",
    "\n",
    "# Check specifically for CQID011 (where \"other (please specify)\" is likely)\n",
    "print(\"\\nCHECKING CQID011 IN TRAINING DATA:\")\n",
    "train_cqid011 = train_df[train_df['base_qid'] == 'CQID011']\n",
    "train_cqid011_results = check_for_other_please_specify(train_cqid011)\n",
    "\n",
    "print(\"\\nCHECKING CQID011 IN VALIDATION DATA:\")\n",
    "val_cqid011 = val_df[val_df['base_qid'] == 'CQID011']\n",
    "val_cqid011_results = check_for_other_please_specify(val_cqid011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e98618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCHECKING CQID013 IN VALIDATION DATA:\")\n",
    "val_cqid013 = val_df[val_df['base_qid'] == 'CQID013']\n",
    "val_cqid013_results = check_for_other_please_specify(val_cqid013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20972fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
