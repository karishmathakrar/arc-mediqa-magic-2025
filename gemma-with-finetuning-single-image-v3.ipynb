{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/hf_cache\n"
     ]
    }
   ],
   "source": [
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/coda1/p-dsgt_clef2025/0/kthakrar3/hf_cache'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.pop(\"TRANSFORMERS_CACHE\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\"\n",
    "\n",
    "# where the downloaded model goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE: None\n",
      "HF_HOME: /storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.hf_cache\n"
     ]
    }
   ],
   "source": [
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/bin/python\n",
      "Python version: 3.10.10 (main, Apr 15 2024, 11:52:16) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
      "Python version info: sys.version_info(major=3, minor=10, micro=10, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>answer_index</th>\n",
       "      <th>question_en</th>\n",
       "      <th>options_en</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>responses</th>\n",
       "      <th>query_title_en</th>\n",
       "      <th>query_content_en</th>\n",
       "      <th>image_paths</th>\n",
       "      <th>responses_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U04473</td>\n",
       "      <td>[IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]</td>\n",
       "      <td>[{'author_id': 'U00217', 'content_zh': '银屑病，似与...</td>\n",
       "      <td>Pleural effusion accompanied by rash</td>\n",
       "      <td>A patient with pleural effusion is accompanied...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Psoriasis seems to have no relation to pleura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00002</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U06063</td>\n",
       "      <td>[IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...</td>\n",
       "      <td>[{'author_id': 'U11305', 'content_zh': '脚气', '...</td>\n",
       "      <td>What is on the bottom of the right foot?</td>\n",
       "      <td>The patient is a 50-year-old male, who has bee...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Beriberi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00003</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U00780</td>\n",
       "      <td>[IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...</td>\n",
       "      <td>[{'author_id': 'U01131', 'content_zh': '瘙痒症，有无...</td>\n",
       "      <td>Interpreting Images - Is it magical skin?</td>\n",
       "      <td>Male, 65 years old, skin lesions as shown in t...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Pruritus, is there any other special medical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENC00004</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>2</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>widespread</td>\n",
       "      <td>U00209</td>\n",
       "      <td>[IMG_ENC00004_00001.jpg, IMG_ENC00004_00002.jpg]</td>\n",
       "      <td>[{'author_id': 'U06715', 'content_zh': '肢端角化病？...</td>\n",
       "      <td>Skin Disease</td>\n",
       "      <td>Male, 15 years old, keratosis on both palms, s...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Acrokeratosis?, Progressive Symmetrical Eryth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENC00005</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>1</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "      <td>limited area</td>\n",
       "      <td>U09050</td>\n",
       "      <td>[IMG_ENC00005_00001.jpg]</td>\n",
       "      <td>[{'author_id': 'U09402', 'content_zh': '是否神经性皮...</td>\n",
       "      <td>Perifollicular atrophy?</td>\n",
       "      <td>Young female, silver-gray dot-like atrophy spo...</td>\n",
       "      <td>['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...</td>\n",
       "      <td>[Is it neurodermatitis?, Impotence?, Lichen Sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id          qid  answer_index                        question_en  \\\n",
       "0     ENC00001  CQID010-001             1  How much of the body is affected?   \n",
       "1     ENC00002  CQID010-001             1  How much of the body is affected?   \n",
       "2     ENC00003  CQID010-001             1  How much of the body is affected?   \n",
       "3     ENC00004  CQID010-001             2  How much of the body is affected?   \n",
       "4     ENC00005  CQID010-001             1  How much of the body is affected?   \n",
       "\n",
       "                                          options_en question_type_en  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "1  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "2  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "3  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "4  ['single spot', 'limited area', 'widespread', ...             Site   \n",
       "\n",
       "  question_category_en   answer_text author_id  \\\n",
       "0              General  limited area    U04473   \n",
       "1              General  limited area    U06063   \n",
       "2              General  limited area    U00780   \n",
       "3              General    widespread    U00209   \n",
       "4              General  limited area    U09050   \n",
       "\n",
       "                                           image_ids  \\\n",
       "0   [IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]   \n",
       "1  [IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...   \n",
       "2  [IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...   \n",
       "3   [IMG_ENC00004_00001.jpg, IMG_ENC00004_00002.jpg]   \n",
       "4                           [IMG_ENC00005_00001.jpg]   \n",
       "\n",
       "                                           responses  \\\n",
       "0  [{'author_id': 'U00217', 'content_zh': '银屑病，似与...   \n",
       "1  [{'author_id': 'U11305', 'content_zh': '脚气', '...   \n",
       "2  [{'author_id': 'U01131', 'content_zh': '瘙痒症，有无...   \n",
       "3  [{'author_id': 'U06715', 'content_zh': '肢端角化病？...   \n",
       "4  [{'author_id': 'U09402', 'content_zh': '是否神经性皮...   \n",
       "\n",
       "                              query_title_en  \\\n",
       "0       Pleural effusion accompanied by rash   \n",
       "1   What is on the bottom of the right foot?   \n",
       "2  Interpreting Images - Is it magical skin?   \n",
       "3                               Skin Disease   \n",
       "4                    Perifollicular atrophy?   \n",
       "\n",
       "                                    query_content_en  \\\n",
       "0  A patient with pleural effusion is accompanied...   \n",
       "1  The patient is a 50-year-old male, who has bee...   \n",
       "2  Male, 65 years old, skin lesions as shown in t...   \n",
       "3  Male, 15 years old, keratosis on both palms, s...   \n",
       "4  Young female, silver-gray dot-like atrophy spo...   \n",
       "\n",
       "                                         image_paths  \\\n",
       "0  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "1  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "2  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "3  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "4  ['/storage/coda1/p-dsgt_clef2025/0/kthakrar3/m...   \n",
       "\n",
       "                                        responses_en  \n",
       "0  [Psoriasis seems to have no relation to pleura...  \n",
       "1                                         [Beriberi]  \n",
       "2  [Pruritus, is there any other special medical ...  \n",
       "3  [Acrokeratosis?, Progressive Symmetrical Eryth...  \n",
       "4  [Is it neurodermatitis?, Impotence?, Lichen Sc...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_file = os.path.join(\"2025_dataset\", \"train\", \"final_df_2.csv\")\n",
    "train_images_dir = os.path.join(\"2025_dataset\", \"train\", \"images_train\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "\n",
    "train_df['image_ids'] = train_df['image_ids'].apply(eval)\n",
    "train_df['responses_en'] = train_df['responses_en'].apply(eval)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 samples for training\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.head(10)  # Start with 10 samples for quick debugging\n",
    "print(f\"Using {len(train_df)} samples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe shape: (10, 8)\n",
      "Columns: ['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids', 'question_type_en', 'question_category_en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sample row:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>question_en</th>\n",
       "      <th>options_en</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>image_ids</th>\n",
       "      <th>question_type_en</th>\n",
       "      <th>question_category_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENC00001</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENC00002</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENC00003</td>\n",
       "      <td>CQID010-001</td>\n",
       "      <td>How much of the body is affected?</td>\n",
       "      <td>['single spot', 'limited area', 'widespread', ...</td>\n",
       "      <td>limited area</td>\n",
       "      <td>[IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...</td>\n",
       "      <td>Site</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encounter_id          qid                        question_en  \\\n",
       "0     ENC00001  CQID010-001  How much of the body is affected?   \n",
       "1     ENC00002  CQID010-001  How much of the body is affected?   \n",
       "2     ENC00003  CQID010-001  How much of the body is affected?   \n",
       "\n",
       "                                          options_en   answer_text  \\\n",
       "0  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "1  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "2  ['single spot', 'limited area', 'widespread', ...  limited area   \n",
       "\n",
       "                                           image_ids question_type_en  \\\n",
       "0   [IMG_ENC00001_00001.jpg, IMG_ENC00001_00002.jpg]             Site   \n",
       "1  [IMG_ENC00002_00001.jpg, IMG_ENC00002_00002.jp...             Site   \n",
       "2  [IMG_ENC00003_00001.jpg, IMG_ENC00003_00002.jp...             Site   \n",
       "\n",
       "  question_category_en  \n",
       "0              General  \n",
       "1              General  \n",
       "2              General  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = train_df[['encounter_id', 'qid', 'question_en', 'options_en', 'answer_text', 'image_ids', 'question_type_en', 'question_category_en']]\n",
    "\n",
    "print(f\"Filtered dataframe shape: {train_df.shape}\")\n",
    "print(\"Columns:\", train_df.columns.tolist())\n",
    "\n",
    "display(\"Sample row:\", train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_idx, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    batch_data = []\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_idx}\"):\n",
    "        try:\n",
    "            # Only take the first image from the list\n",
    "            if not row['image_ids'] or len(row['image_ids']) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get just the first image path\n",
    "            image_path = os.path.join(train_images_dir, row['image_ids'][0])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            # Verify the image is valid\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupt or unreadable image at {image_path} — {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Format options text\n",
    "            options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(eval(row['options_en']))])\n",
    "            \n",
    "            # Create metadata string\n",
    "            metadata = f\"Type: {row.get('question_type_en', '')}, Category: {row.get('question_category_en', '')}\"\n",
    "            \n",
    "            # Create the full query text with instructions\n",
    "            query_text = f\"Question: Based on the image, {row['question_en']}\\nQuestion Metadata: {metadata}\\nOptions: {options_text}\"\n",
    "#             query_text += \"\\n\\nCRITICAL INSTRUCTION: Only respond with an option if it is **clearly and unambiguously** supported by the image. If the image is unclear, incomplete, or could fit multiple answers, respond with: 'Not mentioned'. You must respond with the **exact text** of one option below. No numbers, no explanation. Given the medical context, err on the side of caution.\"\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"encounter_id\": row['encounter_id'],\n",
    "                \"qid\": row['qid'],\n",
    "                \"query_text\": query_text,\n",
    "                \"image_path\": image_path,\n",
    "                \"answer_text\": row['answer_text'],\n",
    "                \"question_type\": row.get('question_type_en', ''),\n",
    "                \"question_category\": row.get('question_category_en', '')\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    batch_file = os.path.join(save_dir, f\"batch_{batch_idx}.pkl\")\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batch_data, f)\n",
    "    \n",
    "    return len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.processor = processor\n",
    "        self.examples = []\n",
    "        \n",
    "        for batch_file in sorted(os.listdir(data_dir)):\n",
    "            if batch_file.startswith(\"batch_\") and batch_file.endswith(\".pkl\"):\n",
    "                with open(os.path.join(data_dir, batch_file), 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    self.examples.extend(batch_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Open just one image, convert to RGB\n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        # Use consistent system message\n",
    "        system_message = \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": example['query_text']},\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": example['answer_text']}],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return {\"messages\": messages}\n",
    "\n",
    "def preprocess_dataset(df, batch_size=50, save_dir=\"processed_data\"):\n",
    "    total_processed = 0\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{(len(df)-1)//batch_size + 1}\")\n",
    "        processed = process_batch(batch_df, batch_idx, save_dir)\n",
    "        total_processed += processed\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed {total_processed} examples so far\")\n",
    "    \n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"processed_data_debug\"\n",
    "\n",
    "if os.path.exists(processed_data_dir):\n",
    "    shutil.rmtree(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9bf6a09e35462381a1e536a48a702a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 examples so far\n",
      "Total processed examples: 10\n"
     ]
    }
   ],
   "source": [
    "total_examples = preprocess_dataset(train_df, batch_size=500, save_dir=processed_data_dir)\n",
    "print(f\"Total processed examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data (first example):\n",
      "Encounter ID: ENC00001\n",
      "Question ID: CQID010-001\n",
      "Query text: Question: Based on the image, How much of the body is affected?\n",
      "Question Metadata: Type: Site, Category: General\n",
      "Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned\n",
      "Image path: 2025_dataset/train/images_train/IMG_ENC00001_00001.jpg\n",
      "Answer text: limited area\n",
      "Question type: Site\n",
      "Question category: General\n"
     ]
    }
   ],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[0]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "# print(f\"Number of images: {len(sample_data['image_path'])}\")\n",
    "print(f\"Image path: {sample_data['image_path']}\")  # Changed from 'image_paths' to 'image_path'\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")\n",
    "print(f\"Question type: {sample_data['question_type']}\")\n",
    "print(f\"Question category: {sample_data['question_category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data (first example):\n",
      "Encounter ID: ENC00002\n",
      "Question ID: CQID010-001\n",
      "Query text: Question: Based on the image, How much of the body is affected?\n",
      "Question Metadata: Type: Site, Category: General\n",
      "Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned\n",
      "Image path: 2025_dataset/train/images_train/IMG_ENC00002_00001.jpg\n",
      "Answer text: limited area\n",
      "Question type: Site\n",
      "Question category: General\n"
     ]
    }
   ],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[1]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "# print(f\"Number of images: {len(sample_data['image_path'])}\")\n",
    "print(f\"Image path: {sample_data['image_path']}\")  # Changed from 'image_paths' to 'image_path'\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")\n",
    "print(f\"Question type: {sample_data['question_type']}\")\n",
    "print(f\"Question category: {sample_data['question_category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed data (first example):\n",
      "Encounter ID: ENC00003\n",
      "Question ID: CQID010-001\n",
      "Query text: Question: Based on the image, How much of the body is affected?\n",
      "Question Metadata: Type: Site, Category: General\n",
      "Options: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned\n",
      "Image path: 2025_dataset/train/images_train/IMG_ENC00003_00001.jpg\n",
      "Answer text: limited area\n",
      "Question type: Site\n",
      "Question category: General\n"
     ]
    }
   ],
   "source": [
    "batch_file = os.path.join(processed_data_dir, \"batch_0.pkl\")\n",
    "with open(batch_file, 'rb') as f:\n",
    "    batch_data = pickle.load(f)\n",
    "\n",
    "print(\"\\nSample of processed data (first example):\")\n",
    "sample_data = batch_data[2]\n",
    "print(f\"Encounter ID: {sample_data['encounter_id']}\")\n",
    "print(f\"Question ID: {sample_data['qid']}\")\n",
    "print(f\"Query text: {sample_data['query_text']}\")\n",
    "# print(f\"Number of images: {len(sample_data['image_path'])}\")\n",
    "print(f\"Image path: {sample_data['image_path']}\")  # Changed from 'image_paths' to 'image_path'\n",
    "print(f\"Answer text: {sample_data['answer_text']}\")\n",
    "print(f\"Question type: {sample_data['question_type']}\")\n",
    "print(f\"Question category: {sample_data['question_category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 10\n",
      "\n",
      "First example roles:\n",
      "System role: {'role': 'system', 'content': [{'type': 'text', 'text': \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"}]}\n",
      "User role: {'role': 'user', 'content': [{'type': 'text', 'text': 'Question: Based on the image, How much of the body is affected?\\nQuestion Metadata: Type: Site, Category: General\\nOptions: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=1944x2541 at 0x15541D84F850>}]}\n",
      "Assistant role: {'role': 'assistant', 'content': [{'type': 'text', 'text': 'limited area'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[0]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 10\n",
      "\n",
      "First example roles:\n",
      "System role: {'role': 'system', 'content': [{'type': 'text', 'text': \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"}]}\n",
      "User role: {'role': 'user', 'content': [{'type': 'text', 'text': 'Question: Based on the image, How much of the body is affected?\\nQuestion Metadata: Type: Site, Category: General\\nOptions: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=2560x1920 at 0x15541D962920>}]}\n",
      "Assistant role: {'role': 'assistant', 'content': [{'type': 'text', 'text': 'limited area'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[1]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 10\n",
      "\n",
      "First example roles:\n",
      "System role: {'role': 'system', 'content': [{'type': 'text', 'text': \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"}]}\n",
      "User role: {'role': 'user', 'content': [{'type': 'text', 'text': 'Question: Based on the image, How much of the body is affected?\\nQuestion Metadata: Type: Site, Category: General\\nOptions: 1. single spot, 2. limited area, 3. widespread, 4. Not mentioned'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=600x400 at 0x15541D3CB340>}]}\n",
      "Assistant role: {'role': 'assistant', 'content': [{'type': 'text', 'text': 'limited area'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "\n",
    "example = dataset[2]\n",
    "print(f\"\\nFirst example roles:\")\n",
    "print(f\"System role: {example['messages'][0]}\")\n",
    "print(f\"User role: {example['messages'][1]}\")\n",
    "print(f\"Assistant role: {example['messages'][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beed143ccd254a0abaf559bd49552c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538ec1322f914932813dfbb6c570ac31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default chat template: {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "Special tokens map: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16. Use a different GPU.\")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs, token=hf_token)\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "print(f\"Default chat template: {processor.tokenizer.chat_template}\")\n",
    "print(f\"Special tokens map: {processor.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_image():\n",
    "    \"\"\"Create a small black image as a placeholder.\"\"\"\n",
    "    return Image.new('RGB', (224, 224), color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Extract image from messages\n",
    "        image_input = None\n",
    "        for msg in example[\"messages\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                for content in msg[\"content\"]:\n",
    "                    if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                        image_input = content[\"image\"]\n",
    "                        break\n",
    "        \n",
    "        if image_input is None:\n",
    "            image_input = create_dummy_image()\n",
    "            \n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        texts.append(text.strip())\n",
    "        images.append([image_input])\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts, \n",
    "        images=images,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Get token IDs for start_of_image and end_of_image\n",
    "    boi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    eoi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"eoi_token\"]\n",
    "    )\n",
    "    \n",
    "    # Mask tokens that shouldn't contribute to the loss\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == boi_token_id] = -100\n",
    "    labels[labels == eoi_token_id] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10\n",
      "Sampling 3 examples from dataset\n",
      "Sample size: 3\n",
      "First example keys: ['messages']\n",
      "Collated batch contains: ['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values', 'labels']\n",
      "Input_ids shape: torch.Size([3, 422])\n",
      "Labels shape: torch.Size([3, 422])\n"
     ]
    }
   ],
   "source": [
    "dataset = MedicalImageDataset(processed_data_dir, processor)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: Dataset is empty! Check data loading process.\")\n",
    "    pass\n",
    "else:\n",
    "    sample_size = min(3, len(dataset))\n",
    "    print(f\"Sampling {sample_size} examples from dataset\")\n",
    "    \n",
    "    sample_examples = [dataset[i] for i in range(sample_size)]\n",
    "    \n",
    "    print(f\"Sample size: {len(sample_examples)}\")\n",
    "    print(\"First example keys:\", list(sample_examples[0].keys()))\n",
    "    \n",
    "    batch = collate_fn(sample_examples)\n",
    "    print(\"Collated batch contains:\", list(batch.keys()))\n",
    "    print(f\"Input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch with 8 examples\n",
      "Processed batch with 2 examples\n",
      "Processed all 10 examples\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "total_examples = 0\n",
    "for batch in dataloader:\n",
    "    # Process each batch. Note: in training, pass this to model.forward()\n",
    "    batch_size = len(batch[\"input_ids\"])\n",
    "    total_examples += batch_size\n",
    "    print(f\"Processed batch with {batch_size} examples\")\n",
    "\n",
    "print(f\"Processed all {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUGGING TEMPLATE APPLICATION ===\n",
      "\n",
      "Example 0:\n",
      "Message structure:\n",
      "  Message 0 role: system\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Message 1 role: user\n",
      "  Content types: ['text', 'image']\n",
      "  Image content count: 1\n",
      "  Message 2 role: assistant\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  <image> tokens found: 0\n",
      "  <start_of_image> tokens found: 1\n",
      "  <image_soft_token> tokens found: 0\n",
      "  Text preview: <bos><start_of_turn>user\n",
      "You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note t...\n",
      "  Special tokens in processor: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "  Sample token IDs (first 20): [2, 2, 105, 2364, 107, 3048, 659, 496, 5526, 2471, 3671, 16326, 236761, 5180, 1186, 4209, 563, 531, 17318, 506]\n",
      "\n",
      "Example 1:\n",
      "Message structure:\n",
      "  Message 0 role: system\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Message 1 role: user\n",
      "  Content types: ['text', 'image']\n",
      "  Image content count: 1\n",
      "  Message 2 role: assistant\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  <image> tokens found: 0\n",
      "  <start_of_image> tokens found: 1\n",
      "  <image_soft_token> tokens found: 0\n",
      "  Text preview: <bos><start_of_turn>user\n",
      "You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note t...\n",
      "  Special tokens in processor: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "  Sample token IDs (first 20): [2, 2, 105, 2364, 107, 3048, 659, 496, 5526, 2471, 3671, 16326, 236761, 5180, 1186, 4209, 563, 531, 17318, 506]\n",
      "\n",
      "Example 2:\n",
      "Message structure:\n",
      "  Message 0 role: system\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  Message 1 role: user\n",
      "  Content types: ['text', 'image']\n",
      "  Image content count: 1\n",
      "  Message 2 role: assistant\n",
      "  Content types: ['text']\n",
      "  Image content count: 0\n",
      "  <image> tokens found: 0\n",
      "  <start_of_image> tokens found: 1\n",
      "  <image_soft_token> tokens found: 0\n",
      "  Text preview: <bos><start_of_turn>user\n",
      "You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note t...\n",
      "  Special tokens in processor: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "  Sample token IDs (first 20): [2, 2, 105, 2364, 107, 3048, 659, 496, 5526, 2471, 3671, 16326, 236761, 5180, 1186, 4209, 563, 531, 17318, 506]\n"
     ]
    }
   ],
   "source": [
    "def debug_template_application(examples):\n",
    "    print(\"\\n=== DEBUGGING TEMPLATE APPLICATION ===\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        \n",
    "        # First, check message structure\n",
    "        print(\"Message structure:\")\n",
    "        for j, msg in enumerate(example[\"messages\"]):\n",
    "            role = msg.get(\"role\", \"unknown\")\n",
    "            print(f\"  Message {j} role: {role}\")\n",
    "            \n",
    "            content = msg.get(\"content\", [])\n",
    "            if not isinstance(content, list):\n",
    "                content = [content]\n",
    "                \n",
    "            print(f\"  Content types: {[c.get('type') if isinstance(c, dict) else type(c).__name__ for c in content]}\")\n",
    "            \n",
    "            # Check specifically for image content\n",
    "            image_count = sum(1 for c in content if isinstance(c, dict) and c.get('type') == 'image')\n",
    "            print(f\"  Image content count: {image_count}\")\n",
    "        \n",
    "        # Next, check template application\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Check for various possible image tokens\n",
    "        image_tokens = [\"<image>\", \"<start_of_image>\", \"<image_soft_token>\"]\n",
    "        for token in image_tokens:\n",
    "            count = text.count(token)\n",
    "            print(f\"  {token} tokens found: {count}\")\n",
    "        \n",
    "        # Print text snippet to see if tokens appear\n",
    "        print(f\"  Text preview: {text[:200]}...\")\n",
    "        \n",
    "        # Get all available tokens from special tokens map\n",
    "        special_tokens = processor.tokenizer.special_tokens_map\n",
    "        print(f\"  Special tokens in processor: {special_tokens}\")\n",
    "        \n",
    "        # Tokenize the text to see what token IDs are actually being used\n",
    "        token_ids = processor.tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "        # Get a few token IDs around where an image might be\n",
    "        if len(token_ids) > 20:\n",
    "            print(f\"  Sample token IDs (first 20): {token_ids[:20].tolist()}\")\n",
    "            \n",
    "debug_template_application(sample_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-product-description\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,  # Critical for custom datasets\n",
    "    label_names=[\"labels\"],  # Explicitly setting label_names\n",
    ")\n",
    "\n",
    "# args.remove_unused_columns = False # Not needed in our case but leaving it in in case the loaded data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test with a small batch\n",
    "# test_indices = list(range(min(3, len(dataset))))\n",
    "# test_batch = [dataset[i] for i in test_indices]\n",
    "\n",
    "# Now initialize your actual trainer with the regular collate_fn\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=60.474578857421875, metrics={'train_runtime': 43.9671, 'train_samples_per_second': 0.227, 'train_steps_per_second': 0.045, 'total_flos': 87786641260032.0, 'train_loss': 60.474578857421875})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/coda1/p-dsgt_clef2025/0/kthakrar3/mediqa-magic-v2/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you can test your model, make sure to free the memory.\n",
    "\n",
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f771a60757b4c80a331e134a9f7efbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f5de2fb4674f0d9f17de6b61de07ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['merged_model/processor_config.json']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b255cb195a41a9b0f1b9f90295ae46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model with PEFT adapter\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"merged_model\",  # Use merged model for inference\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_analysis(image_path, question, options, question_type=\"\", question_category=\"\", model=model, processor=processor):\n",
    "    \"\"\"\n",
    "    Generate medical image analysis based on a question and image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the medical image\n",
    "        question: The medical question to analyze\n",
    "        options: List of possible options to choose from\n",
    "        question_type: Optional metadata about question type\n",
    "        question_category: Optional metadata about question category\n",
    "        model: The model to use for inference\n",
    "        processor: The processor to use for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        The model's response (selected option)\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    else:\n",
    "        # Already a PIL Image\n",
    "        image = image_path.convert(\"RGB\") if hasattr(image_path, 'convert') else image_path\n",
    "    \n",
    "    # Format options text\n",
    "    options_text = \", \".join([f\"{i+1}. {opt}\" for i, opt in enumerate(options)])\n",
    "    \n",
    "    # Create metadata string\n",
    "    metadata = f\"Type: {question_type}, Category: {question_category}\"\n",
    "    \n",
    "    # Create the query text\n",
    "    query_text = f\"Question: Based on the image, {question}\\nQuestion Metadata: {metadata}\\nOptions: {options_text}\"\n",
    "    \n",
    "    # System message\n",
    "    system_message = \"You are a medical image analysis assistant. Your only task is to examine the provided clinical images and select the exact option text that best describes what you see. Note this is not the full context so if you are unsure or speculate other regions being affected, respond with 'Not mentioned'. You must respond with the full text of one of the provided options, exactly as written. Do not include any additional words or reasoning. Given the medical context, err on the side of caution when uncertain.\"\n",
    "    \n",
    "    # Convert to messages format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": query_text},\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "        ]},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Extract image from messages - needs to be in correct format for processor\n",
    "    image_inputs = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            for content in msg[\"content\"]:\n",
    "                if isinstance(content, dict) and content.get(\"type\") == \"image\" and \"image\" in content:\n",
    "                    image_inputs.append(content[\"image\"])\n",
    "                    break\n",
    "    \n",
    "    # Tokenize the text and process the images\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image_inputs],  # Nested list as processor expects\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Move the inputs to the device\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    stop_token_ids = [processor.tokenizer.eos_token_id, processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "    generated_ids = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=64,  # Shorter is fine for option selection\n",
    "        top_p=0.9,\n",
    "        do_sample=True, \n",
    "        temperature=0.5,  # Lower temp for more precise answers\n",
    "        eos_token_id=stop_token_ids, \n",
    "        disable_compile=True\n",
    "    )\n",
    "    \n",
    "    # Trim the generation and decode the output to text\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: 3. widespread\n",
      "\n",
      "=== More Test Cases ===\n",
      "\n",
      "Test Case 1:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: limited area\n",
      "\n",
      "Test Case 2:\n",
      "Question: How much of the body is affected?\n",
      "Options: ['single spot', 'limited area', 'widespread', 'Not mentioned']\n",
      "Model's answer: limited area\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with a sample image from your test set\n",
    "    image_path = \"2025_dataset/train/images_train/IMG_ENC00001_00001.jpg\"\n",
    "    question = \"How much of the body is affected?\"\n",
    "    options = [\"single spot\", \"limited area\", \"widespread\", \"Not mentioned\"]\n",
    "    question_type = \"Site\"\n",
    "    question_category = \"General\"\n",
    "    \n",
    "    result = generate_medical_analysis(\n",
    "        image_path=image_path,\n",
    "        question=question,\n",
    "        options=options,\n",
    "        question_type=question_type,\n",
    "        question_category=question_category\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Options: {options}\")\n",
    "    print(f\"Model's answer: {result}\")\n",
    "    \n",
    "    # You can test with multiple examples\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"image_path\": \"2025_dataset/train/images_train/IMG_ENC00002_00001.jpg\",\n",
    "            \"question\": \"How much of the body is affected?\",\n",
    "            \"options\": [\"single spot\", \"limited area\", \"widespread\", \"Not mentioned\"],\n",
    "            \"question_type\": \"Site\",\n",
    "            \"question_category\": \"General\"\n",
    "        },\n",
    "        {\n",
    "            \"image_path\": \"2025_dataset/train/images_train/IMG_ENC00003_00001.jpg\",\n",
    "            \"question\": \"How much of the body is affected?\",\n",
    "            \"options\": [\"single spot\", \"limited area\", \"widespread\", \"Not mentioned\"],\n",
    "            \"question_type\": \"Site\", \n",
    "            \"question_category\": \"General\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== More Test Cases ===\")\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nTest Case {i+1}:\")\n",
    "        result = generate_medical_analysis(**test_case)\n",
    "        print(f\"Question: {test_case['question']}\")\n",
    "        print(f\"Options: {test_case['options']}\")\n",
    "        print(f\"Model's answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Mediqa)",
   "language": "python",
   "name": "py310_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
